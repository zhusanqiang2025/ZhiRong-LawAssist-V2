"""
åˆ†æ storage/uploads ç›®å½•ä¸‹çš„æ–‡ä»¶ä½¿ç”¨æƒ…å†µ

ä½¿ç”¨æ–¹æ³•ï¼š
    docker-compose exec backend python scripts/analyze_uploads_files.py
"""
import os
import sys
from pathlib import Path

# æ·»åŠ  backend ç›®å½•åˆ° Python è·¯å¾„
CURRENT_DIR = Path(__file__).parent
BACKEND_DIR = CURRENT_DIR.parent
sys.path.insert(0, str(BACKEND_DIR))

from app.database import SessionLocal
from app.models.contract import ContractDoc

print("\n" + "="*100)
print("åˆ†æ storage/uploads ç›®å½•ä¸‹çš„æ–‡ä»¶ä½¿ç”¨æƒ…å†µ")
print("="*100 + "\n")

# è·å–æ•°æ®åº“ä¼šè¯
db = SessionLocal()

# æ‰«æ storage/uploads ç›®å½•
print("æ­¥éª¤ 1: æ‰«æ storage/uploads ç›®å½•")
storage_dir = "/app/storage/uploads"

if not os.path.exists(storage_dir):
    print(f"  âŒ ç›®å½•ä¸å­˜åœ¨: {storage_dir}")
    db.close()
    sys.exit(1)

all_files = []
for root, dirs, files in os.walk(storage_dir):
    for file in files:
        file_path = os.path.join(root, file)
        all_files.append(file_path)

print(f"  ç›®å½•ä¸‹å…±æœ‰ {len(all_files)} ä¸ªæ–‡ä»¶")

# æŸ¥è¯¢æ•°æ®åº“ä¸­å¼•ç”¨çš„æ–‡ä»¶è·¯å¾„
print("\næ­¥éª¤ 2: æŸ¥è¯¢æ•°æ®åº“ä¸­å¼•ç”¨çš„æ–‡ä»¶è·¯å¾„")

# ContractDoc è¡¨ä¸­çš„ä¸‰ä¸ªæ–‡ä»¶è·¯å¾„å­—æ®µ
contracts = db.query(ContractDoc).all()

db_files = set()

# æ”¶é›† original_file_path
for c in contracts:
    if c.original_file_path:
        # æå–æ–‡ä»¶å
        file_name = os.path.basename(c.original_file_path)
        if "uploads" in c.original_file_path or "storage" in c.original_file_path:
            db_files.add(file_name)

# æ”¶é›† pdf_converted_path
for c in contracts:
    if c.pdf_converted_path:
        file_name = os.path.basename(c.pdf_converted_path)
        if "uploads" in c.pdf_converted_path or "storage" in c.pdf_converted_path:
            db_files.add(file_name)

# æ”¶é›† final_docx_path
for c in contracts:
    if c.final_docx_path:
        file_name = os.path.basename(c.final_docx_path)
        if "uploads" in c.final_docx_path or "storage" in c.final_docx_path:
            db_files.add(file_name)

print(f"  æ•°æ®åº“ä¸­å¼•ç”¨çš„ uploads æ–‡ä»¶æ•°: {len(db_files)}")

# æ‰¾å‡ºå­¤ç«‹æ–‡ä»¶
print("\næ­¥éª¤ 3: è¯†åˆ«å­¤ç«‹æ–‡ä»¶")
all_file_names = set(os.path.basename(f) for f in all_files)
orphan_files = all_file_names - db_files

print(f"  å­¤ç«‹æ–‡ä»¶æ•°: {len(orphan_files)}")
print(f"  å·²å¼•ç”¨æ–‡ä»¶æ•°: {len(all_file_names) - len(orphan_files)}")

if len(orphan_files) == 0:
    print("  âœ… æ²¡æœ‰å‘ç°å­¤ç«‹æ–‡ä»¶ï¼Œæ‰€æœ‰æ–‡ä»¶éƒ½è¢«æ•°æ®åº“å¼•ç”¨")
    db.close()
    sys.exit(0)

# æ‰¾å‡ºå®Œæ•´çš„å­¤ç«‹æ–‡ä»¶è·¯å¾„
orphan_file_paths = []
for file_path in all_files:
    if os.path.basename(file_path) in orphan_files:
        orphan_file_paths.append(file_path)

print(f"  å®Œæ•´è·¯å¾„æ•°: {len(orphan_file_paths)}")

# è®¡ç®—æ€»å¤§å°
total_size = sum(os.path.getsize(f) for f in orphan_file_paths)
size_mb = total_size / (1024 * 1024)

print(f"  æ€»å¤§å°: {size_mb:.2f} MB")

# æ˜¾ç¤ºæ–‡ä»¶ç±»å‹ç»Ÿè®¡
print("\næ­¥éª¤ 4: æ–‡ä»¶ç±»å‹ç»Ÿè®¡")
file_types = {}
for file_path in orphan_file_paths:
    ext = os.path.splitext(file_path)[1].lower()
    file_types[ext] = file_types.get(ext, 0) + 1

print(f"  å­¤ç«‹æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
for ext, count in sorted(file_types.items(), key=lambda x: x[1], reverse=True):
    print(f"    {ext or '(æ— æ‰©å±•å)'}: {count} ä¸ª")

# æ˜¾ç¤ºå‰20ä¸ªå­¤ç«‹æ–‡ä»¶
print("\næ­¥éª¤ 5: å­¤ç«‹æ–‡ä»¶åˆ—è¡¨ï¼ˆå‰20ä¸ªï¼‰")
print("  " + "-"*98)
for f in orphan_file_paths[:20]:
    size_kb = os.path.getsize(f) / 1024
    print(f"  - {os.path.basename(f)} ({size_kb:.1f} KB)")

if len(orphan_file_paths) > 20:
    print(f"  ... è¿˜æœ‰ {len(orphan_file_paths) - 20} ä¸ªæ–‡ä»¶æœªæ˜¾ç¤º")

print("\n" + "="*100)
print(f"åˆ†æå®Œæˆ: å…± {len(orphan_file_paths)} ä¸ªå­¤ç«‹æ–‡ä»¶ ({size_mb:.2f} MB)")
print("="*100)
print("\nğŸ’¡ æç¤º: è¿è¡Œ cleanup_orphan_uploads.py æ¥åˆ é™¤è¿™äº›å­¤ç«‹æ–‡ä»¶")

db.close()
