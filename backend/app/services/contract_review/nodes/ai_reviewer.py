import json
import logging
import os
import httpx
from typing import Dict, Any, Optional, List, Tuple

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser

# å¼•å…¥å†…éƒ¨ä¾èµ– (æ³¨æ„ç›¸å¯¹è·¯å¾„ï¼Œå‡è®¾æœ¬æ–‡ä»¶åœ¨ nodes/ ç›®å½•ä¸‹)
from ..state import AgentState
from ..schemas import ReviewOutput, ContractProfile, LegalRelationshipAnalysis
from ..rule_assembler import rule_assembler

logger = logging.getLogger(__name__)

# ================= é…ç½®åŒº =================
API_KEY = os.getenv("LANGCHAIN_API_KEY", "your-api-key")
API_BASE_URL = os.getenv("LANGCHAIN_API_BASE_URL", "https://api.openai.com/v1")
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4o-mini")

def get_model(json_mode=False):
    """åˆå§‹åŒ– LLM æ¨¡å‹"""
    # ä½¿ç”¨ httpx å¿½ç•¥ SSL è¯ä¹¦é—®é¢˜ (æ²¿ç”¨ä½ ä¹‹å‰çš„é…ç½®)
    http_client = httpx.Client(verify=False, trust_env=False)
    
    return ChatOpenAI(
        model=MODEL_NAME,
        api_key=API_KEY,
        base_url=API_BASE_URL,
        temperature=0.1, # ä¿æŒä½æ¸©åº¦ä»¥ç¡®ä¿é€»è¾‘ä¸¥è°¨
        http_client=http_client,
        # åªæœ‰æ”¯æŒ json_object çš„æ¨¡å‹æ‰å¼€å¯æ­¤é€‰é¡¹
        model_kwargs={"response_format": {"type": "json_object"}} if json_mode else {}
    )

# ================= Stage 1: åˆåŒæ³•å¾‹ç”»åƒ (Profile) =================

def execute_stage_1(text: str, metadata: dict) -> dict:
    """
    ç¬¬ä¸€é˜¶æ®µï¼šè¯†åˆ«åˆåŒçš„åŸºç¡€æ³•å¾‹å±æ€§ï¼ˆéé£é™©åˆ¤æ–­ï¼‰
    """
    logger.info("--- [Stage 1] Start: Contract Profiling ---")
    try:
        llm = get_model(json_mode=True)
        parser = PydanticOutputParser(pydantic_object=ContractProfile)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€åèµ„æ·±åˆåŒå¾‹å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ã€è¯†åˆ«åˆåŒçš„æ³•å¾‹å±æ€§ã€‘ï¼Œä¸ºåç»­å®¡æŸ¥å»ºç«‹åŸºç¡€ã€‚
            
            è¯·åŸºäºåˆåŒå†…å®¹åŠå…ƒæ•°æ®ï¼Œé‡ç‚¹åˆ¤æ–­ï¼š
            1. **åˆåŒç±»å‹**ï¼šå…·ä½“çš„æ³•å¾‹å®šæ€§ï¼ˆå¦‚åŠ³åŠ¨åˆåŒã€æ‰¿æ½åˆåŒã€æŠ€æœ¯å¼€å‘åˆåŒï¼‰ã€‚
            2. **æŒç»­æ€§æœåŠ¡**ï¼šæ˜¯å¦æ¶‰åŠé•¿æœŸçš„ã€æŒç»­æ€§çš„æœåŠ¡æä¾›ï¼Ÿ
            3. **å¤åˆæ€§**ï¼šæ˜¯å¦ä¸ºåŒ…å«å¤šç§æ³•å¾‹å…³ç³»çš„æ··åˆåˆåŒï¼Ÿ
            4. **äººèº«ä¾é™„æ€§**ï¼šç”²æ–¹å¯¹ä¹™æ–¹æ˜¯å¦å­˜åœ¨å¼ºç®¡ç†ã€å¼ºæ§åˆ¶ç‰¹å¾ï¼ˆåŒºåˆ«åŠ³åŠ¨ä¸åŠ³åŠ¡çš„å…³é”®ï¼‰ï¼Ÿ

            å…ƒæ•°æ®å‚è€ƒï¼š{metadata}
            
            è¦æ±‚ï¼š
            - ä»…è¾“å‡ºå®¢è§‚å±æ€§ï¼Œä¸è¦è¿›è¡Œé£é™©åˆ¤æ–­ã€‚
            - ä¸å¾—åŒ…å«ä¿®æ”¹å»ºè®®ã€‚
            - å¿…é¡»ä¸¥æ ¼éµå¾ª JSON æ ¼å¼ã€‚
            
            {format_instructions}"""),
            ("user", "åˆåŒå†…å®¹æ‘˜è¦ï¼ˆå‰6000å­—ï¼‰ï¼š\n{text}")
        ])
        
        chain = prompt | llm | parser
        # æˆªå–å‰ 6000 å­—é€šå¸¸è¶³å¤Ÿåˆ¤æ–­å±æ€§ï¼ŒèŠ‚çœ Token
        result = chain.invoke({
            "text": text[:6000], 
            "metadata": json.dumps(metadata, ensure_ascii=False),
            "format_instructions": parser.get_format_instructions()
        })
        
        logger.info(f"--- [Stage 1] Completed. Type: {result.contract_type} ---")
        return result.dict()
        
    except Exception as e:
        logger.error(f"[Stage 1 Error] æ³•å¾‹ç”»åƒåˆ†æå¤±è´¥: {e}")
        # å®¹é”™ï¼šè¿”å›ç©ºå­—å…¸ï¼Œä¸é˜»æ–­æµç¨‹
        return {}

# ================= Stage 2: æ³•å¾‹å…³ç³»ä¸é€‚ç”¨æ³• (Relationships) =================

def execute_stage_2(text: str, profile: dict) -> dict:
    """
    ç¬¬äºŒé˜¶æ®µï¼šåŸºäºç”»åƒåˆ¤æ–­æ³•å¾‹å…³ç³»åŠæ ¸å¿ƒé£é™©æ–¹å‘
    """
    logger.info("--- [Stage 2] Start: Legal Relationship Analysis ---")
    try:
        llm = get_model(json_mode=True)
        parser = PydanticOutputParser(pydantic_object=LegalRelationshipAnalysis)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€åæ³•ç†å­¦ä¸“å®¶ã€‚è¯·åŸºäºã€åˆåŒæ³•å¾‹ç”»åƒã€‘ï¼Œåˆ¤æ–­æ³•å¾‹å…³ç³»ä¸é€‚ç”¨æ³•å¾‹é—®é¢˜ã€‚
            
            å½“å‰æ³•å¾‹ç”»åƒï¼š{profile}
            
            è¯·é‡ç‚¹åˆ†æï¼š
            1. **åŠ³åŠ¨å…³ç³»é£é™©**ï¼šæ˜¯å¦å­˜åœ¨è¢«å¸æ³•æœºå…³è®¤å®šä¸ºäº‹å®åŠ³åŠ¨å…³ç³»çš„é£é™©ï¼Ÿ(High/Medium/Low)
            2. **ä¾µæƒé£é™©**ï¼šæ˜¯å¦å­˜åœ¨ä¾µæƒè´£ä»»é«˜å‘é£é™©ï¼Ÿ
            3. **æ³•å¾‹é€‚ç”¨**ï¼šä¸»è¦é€‚ç”¨å“ªäº›æ³•å¾‹ï¼ˆå¦‚æ°‘æ³•å…¸ã€åŠ³åŠ¨æ³•ã€è‘—ä½œæƒæ³•ç­‰ï¼‰ï¼Ÿ
            
            è¦æ±‚ï¼š
            - ä¸¥ç¦è¾“å‡ºä¿®æ”¹å»ºè®®ã€‚
            - å¿…é¡»ä¸¥æ ¼éµå¾ª JSON æ ¼å¼ã€‚
            
            {format_instructions}"""),
            ("user", "åˆåŒå†…å®¹æ‘˜è¦ï¼š\n{text}")
        ])
        
        chain = prompt | llm | parser
        result = chain.invoke({
            "text": text[:6000],
            "profile": json.dumps(profile, ensure_ascii=False),
            "format_instructions": parser.get_format_instructions()
        })
        
        logger.info(f"--- [Stage 2] Completed. Labor Risk: {result.labor_relation_risk} ---")
        return result.dict()
        
    except Exception as e:
        logger.error(f"[Stage 2 Error] æ³•å¾‹å…³ç³»åˆ¤æ–­å¤±è´¥: {e}")
        return {}

# ================= Stage 3: é£é™©ä¸è´£ä»»å®¡æŸ¥ (Review) =================

def execute_stage_3(
    text: str,
    stance: str,
    profile: dict,
    relationships: dict,
    user_rules: list = None,
    transaction_structures: list = None  # â­ æ–°å¢: äº¤æ˜“ç»“æ„åˆ—è¡¨
) -> ReviewOutput:
    """
    ç¬¬ä¸‰é˜¶æ®µï¼šåŠ è½½è§„åˆ™åŒ…ï¼Œè¿›è¡Œæœ€ç»ˆå®¡æŸ¥

    Args:
        text: åˆåŒæ–‡æœ¬
        stance: å®¡æŸ¥ç«‹åœº
        profile: åˆåŒç”»åƒ
        relationships: æ³•å¾‹å…³ç³»åˆ†æ
        user_rules: ç”¨æˆ·è‡ªå®šä¹‰è§„åˆ™
        transaction_structures: ç”¨æˆ·é€‰æ‹©çš„äº¤æ˜“ç»“æ„åˆ—è¡¨ (æ–°å¢)
    """
    logger.info("--- [Stage 3] Start: Deep Risk Review ---")

    # 1. åŠ¨æ€ç»„è£…è§„åˆ™åŒ… (Rule Pack)
    # è¿™é‡Œçš„ rule_assembler ä¼šè¯»å– JSON é…ç½®å¹¶ç»“åˆ knowledge graph ç‰¹å¾(å¦‚æœ‰)
    # ç›®å‰ profile ä¸­æš‚æ—  legal_featuresï¼Œassembler ä¼šä½¿ç”¨ defaults æˆ–åŸºç¡€æ˜ å°„
    rule_pack_text = rule_assembler.assemble_prompt_context(
        legal_features=profile, # æš‚æ—¶å°† profile ä¼ è¿›å»ï¼Œæœªæ¥å¯¹æ¥ KnowledgeGraph åä¼  features
        stance=stance,
        user_custom_rules=user_rules,
        transaction_structures=transaction_structures  # â­ æ–°å¢: ä¼ é€’äº¤æ˜“ç»“æ„
    )
    
    # 2. æ„å»ºä¸Šä¸‹æ–‡
    context = f"""
    ã€å‰ç½®åˆ¤æ–­ä¾æ®ã€‘
    1. åˆåŒæ³•å¾‹ç”»åƒ: {json.dumps(profile, ensure_ascii=False)}
    2. æ³•å¾‹å…³ç³»å®šæ€§: {json.dumps(relationships, ensure_ascii=False)}
    """
    
    # 3. è°ƒç”¨ LLM - â­ ä½¿ç”¨é²æ£’çš„è§£æå™¨
    from ..json_parser import create_robust_parser

    # æ ¹æ®æ¨¡å‹èƒ½åŠ›å†³å®šæ˜¯å¦å¯ç”¨JSONæ¨¡å¼
    model_name = os.getenv("MODEL_NAME", "gpt-4o-mini")
    use_json_mode = not any(keyword in model_name.lower()
                           for keyword in ["deepseek", "qwen", "baichuan", "yi", "chatglm"])

    llm = get_model(json_mode=use_json_mode)
    parser = create_robust_parser(ReviewOutput)

    if use_json_mode:
        # JSONæ¨¡å¼ï¼šä¸¥æ ¼æ ¼å¼è¦æ±‚
        system_template = """ä½ æ˜¯ä¸€åèµ„æ·±å¾‹å¸ˆã€‚è¯·ä» {stance} ç«‹åœºï¼Œä¸¥æ ¼ä¾æ®ã€å®¡æŸ¥æ‰§è¡ŒæŒ‡ä»¤é›†ã€‘å¯¹åˆåŒè¿›è¡Œé£é™©å®¡æŸ¥ã€‚

        {context}

        ========================================
        ã€å®¡æŸ¥æ‰§è¡ŒæŒ‡ä»¤é›† (Rule Pack)ã€‘
        è¯·é€æ¡å¯¹ç…§ä»¥ä¸‹è§„åˆ™è¿›è¡Œæ£€æŸ¥ï¼š

        {rule_pack_text}
        ========================================

        å®¡æŸ¥è¾“å‡ºè¦æ±‚ï¼š
        1. **é€»è¾‘é—­ç¯**ï¼šé£é™©ç‚¹å¿…é¡»æœ‰æ¡æ¬¾ä¾æ®ï¼Œä¸å¾—å‡­ç©ºæé€ ã€‚
        2. **åŠ³åŠ¨é£é™©è”åŠ¨**ï¼šå¦‚æœå‰ç½®åˆ¤æ–­æç¤ºæœ‰åŠ³åŠ¨å…³ç³»é£é™©ï¼Œå¿…é¡»åœ¨å®¡æŸ¥ä¸­ä½œä¸ºé«˜é£é™©é¡¹åˆ—å‡ºã€‚
        3. **è½åœ°å»ºè®®**ï¼šé’ˆå¯¹æ¯ä¸ªé—®é¢˜ï¼Œå¿…é¡»ç»™å‡ºå…·ä½“çš„ä¿®æ”¹å»ºè®®ï¼ˆRevisionï¼‰æˆ–è­¦å‘Šï¼ˆAlertï¼‰ã€‚
        4. **æ ¼å¼ä¸¥æ ¼**ï¼šå¿…é¡»è¾“å‡ºç¬¦åˆ Schema çš„ JSON æ•°æ®ã€‚

        {format_instructions}"""
    else:
        # æ–‡æœ¬æ¨¡å¼ï¼šæ›´å®½æ¾çš„æ ¼å¼è¦æ±‚
        system_template = """ä½ æ˜¯ä¸€åèµ„æ·±å¾‹å¸ˆã€‚è¯·ä» {stance} ç«‹åœºï¼Œä¸¥æ ¼ä¾æ®ã€å®¡æŸ¥æ‰§è¡ŒæŒ‡ä»¤é›†ã€‘å¯¹åˆåŒè¿›è¡Œé£é™©å®¡æŸ¥ã€‚

        {context}

        ========================================
        ã€å®¡æŸ¥æ‰§è¡ŒæŒ‡ä»¤é›† (Rule Pack)ã€‘
        è¯·é€æ¡å¯¹ç…§ä»¥ä¸‹è§„åˆ™è¿›è¡Œæ£€æŸ¥ï¼š

        {rule_pack_text}
        ========================================

        å®¡æŸ¥è¾“å‡ºè¦æ±‚ï¼š
        1. **é€»è¾‘é—­ç¯**ï¼šé£é™©ç‚¹å¿…é¡»æœ‰æ¡æ¬¾ä¾æ®ï¼Œä¸å¾—å‡­ç©ºæé€ ã€‚
        2. **åŠ³åŠ¨é£é™©è”åŠ¨**ï¼šå¦‚æœå‰ç½®åˆ¤æ–­æç¤ºæœ‰åŠ³åŠ¨å…³ç³»é£é™©ï¼Œå¿…é¡»åœ¨å®¡æŸ¥ä¸­ä½œä¸ºé«˜é£é™©é¡¹åˆ—å‡ºã€‚
        3. **è½åœ°å»ºè®®**ï¼šé’ˆå¯¹æ¯ä¸ªé—®é¢˜ï¼Œå¿…é¡»ç»™å‡ºå…·ä½“çš„ä¿®æ”¹å»ºè®®ï¼ˆRevisionï¼‰æˆ–è­¦å‘Šï¼ˆAlertï¼‰ã€‚
        4. **è¾“å‡ºæ ¼å¼**ï¼šè¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å« summary å’Œ issues åˆ—è¡¨ã€‚

        è¾“å‡ºç»“æ„ç¤ºä¾‹ï¼š
        {{
          "summary": "å®¡æŸ¥æ€»ç»“",
          "issues": [
            {{
              "issue_type": "é£é™©ç±»å‹",
              "quote": "ç›¸å…³æ¡æ¬¾åŸæ–‡",
              "explanation": "é£é™©è¯´æ˜",
              "suggestion": "ä¿®æ”¹å»ºè®®",
              "legal_basis": "æ³•å¾‹ä¾æ®",
              "severity": "ä¸¥é‡ç¨‹åº¦(Critical/High/Medium/Low)",
              "action_type": "æ“ä½œç±»å‹(Revision/Alert)"
            }}
          ]
        }}
        """

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("user", "åˆåŒå…¨æ–‡ï¼š\n{text}")
    ])

    chain = prompt | llm | parser

    # Stage 3 éœ€è¦é˜…è¯»å…¨æ–‡ï¼ˆæˆ–å¤§éƒ¨åˆ†å†…å®¹ï¼‰ä»¥å‘ç°ç»†èŠ‚é—®é¢˜
    # è¿™é‡Œæ”¾å®½ token é™åˆ¶åˆ° 12000 å­—ç¬¦ (çº¦ 6k tokens)
    invoke_args = {
        "stance": stance,
        "context": context,
        "rule_pack_text": rule_pack_text,
        "text": text[:12000],
    }
    if use_json_mode:
        invoke_args["format_instructions"] = parser.get_format_instructions()

    result = chain.invoke(invoke_args)

    logger.info(f"--- [Stage 3] Completed. Issues Found: {len(result.issues)} ---")
    return result

# ================= ä¸»èŠ‚ç‚¹å…¥å£ (Node Entry) =================

def ai_reviewer_node(state: AgentState) -> AgentState:
    """
    LangGraph èŠ‚ç‚¹ï¼šä¸²è¡Œæ‰§è¡Œä¸‰é˜¶æ®µå®¡æŸ¥é€»è¾‘

    â­ æ”¯æŒé•¿æ–‡æœ¬åˆ†å—å¤„ç†
    - åˆ†å—æ¨¡å¼: ä½¿ç”¨æ»‘åŠ¨çª—å£é€å—å®¡æŸ¥
    - å…¨æ–‡æ¨¡å¼: ç›´æ¥å®¡æŸ¥å…¨æ–‡
    """
    print("\nğŸ¤– [AI] æ­£åœ¨è¿›è¡Œä¸‰é˜¶æ®µå®¡æŸ¥ (Profile -> Relation -> RuleBasedReview)...")

    contract_text = state.get("contract_text", "")
    metadata = state.get("metadata", {})
    stance = state.get("stance", "neutral")

    # â­ è·å–åˆ†å—ä¿¡æ¯
    chunks = state.get("chunks", [])
    chunk_mode = state.get("chunk_review_mode", False)

    if chunk_mode and chunks:
        logger.info(f"[AI Reviewer] å¯ç”¨åˆ†å—å®¡æŸ¥æ¨¡å¼ - åˆ†å—æ•°é‡: {len(chunks)}")

    # è·å–ç”¨æˆ·è‡ªå®šä¹‰è§„åˆ™ (ä» metadata ä¸­é€ä¼ ï¼Œå¦‚æœå‰ç«¯ä¼ äº†çš„è¯)
    user_rules = metadata.get("custom_rules", [])

    # â­ æ–°å¢: è·å–äº¤æ˜“ç»“æ„ (ä» metadata ä¸­é€ä¼ )
    transaction_structures = metadata.get("transaction_structures", [])
    if transaction_structures:
        logger.info(f"[AI Reviewer] ä½¿ç”¨äº¤æ˜“ç»“æ„: {transaction_structures}")

    # ========== Step 1: æ³•å¾‹ç”»åƒ ==========
    # â­ åˆ†å—æ¨¡å¼: ä½¿ç”¨ç¬¬ä¸€å— + åˆåŒæ‘˜è¦è¿›è¡Œå…¨å±€åˆ†æ
    if chunk_mode and chunks:
        # ä½¿ç”¨ç¬¬ä¸€å—è¿›è¡Œç”»åƒåˆ†æ
        first_chunk_text = chunks[0][0]

        # æ·»åŠ åˆåŒä¸Šä¸‹æ–‡æ‘˜è¦,ç¡®ä¿å…¨å±€ä¸€è‡´æ€§
        context_summary = _generate_contract_summary(metadata, chunks)

        # ç»“åˆç¬¬ä¸€å—å’Œæ‘˜è¦è¿›è¡Œåˆ†æ
        analysis_text = f"""
ã€åˆåŒä¸Šä¸‹æ–‡æ‘˜è¦ã€‘
{context_summary}

ã€ç¬¬ä¸€å—å†…å®¹ã€‘
{first_chunk_text}
"""

        profile = execute_stage_1(analysis_text, metadata)
        logger.info("[AI Reviewer] åˆ†å—æ¨¡å¼: åŸºäºç¬¬ä¸€å—+æ‘˜è¦å®Œæˆç”»åƒ")
    else:
        # å…¨æ–‡æ¨¡å¼
        profile = execute_stage_1(contract_text, metadata)

    # ========== Step 2: æ³•å¾‹å…³ç³» ==========
    # å³ä½¿ Step 1 å¤±è´¥è¿”å›ç©ºå­—å…¸ï¼ŒStep 2 ä¾ç„¶å°è¯•è¿è¡Œï¼ˆä¾é  LLM çš„é€šç”¨èƒ½åŠ›å…œåº•ï¼‰
    if chunk_mode and chunks:
        # åˆ†å—æ¨¡å¼: åŸºäºç¬¬ä¸€å—+æ‘˜è¦
        relationships = execute_stage_2(analysis_text, profile)
        logger.info("[AI Reviewer] åˆ†å—æ¨¡å¼: åŸºäºç¬¬ä¸€å—+æ‘˜è¦å®Œæˆæ³•å¾‹å…³ç³»åˆ†æ")
    else:
        relationships = execute_stage_2(contract_text, profile)

    # ========== Step 3: é£é™©å®¡æŸ¥ (æ ¸å¿ƒ) ==========
    review_result = None
    status = "processing"

    try:
        if chunk_mode and chunks:
            # â­ åˆ†å—å®¡æŸ¥æ¨¡å¼: ä½¿ç”¨æ»‘åŠ¨çª—å£
            review_result = execute_stage_3_chunked_with_sliding_window(
                chunks=chunks,
                stance=stance,
                profile=profile,
                relationships=relationships,
                user_rules=user_rules,
                transaction_structures=transaction_structures
            )
            logger.info("[AI Reviewer] åˆ†å—å®¡æŸ¥å®Œæˆ")
        else:
            # å…¨æ–‡å®¡æŸ¥æ¨¡å¼
            review_result = execute_stage_3(
                text=contract_text,
                stance=stance,
                profile=profile,
                relationships=relationships,
                user_rules=user_rules,
                transaction_structures=transaction_structures
            )
            logger.info("[AI Reviewer] å…¨æ–‡å®¡æŸ¥å®Œæˆ")

        status = "success"
    except Exception as e:
        logger.error(f"[Stage 3 Fatal Error] æœ€ç»ˆå®¡æŸ¥å¤±è´¥: {e}", exc_info=True)
        status = "error"
        # è¿™é‡Œå¯ä»¥é€‰æ‹©è¿”å›ä¸€ä¸ªç©ºçš„ ReviewOutput å¯¹è±¡ï¼Œé˜²æ­¢å‰ç«¯å´©æºƒ
        # review_result = ReviewOutput(summary="å®¡æŸ¥æœåŠ¡æš‚æ—¶ä¸å¯ç”¨", issues=[])

    # --- æ›´æ–°çŠ¶æ€ ---
    return {
        **state,
        "contract_profile": profile,
        "legal_relationships": relationships,
        "review_result": review_result,
        "status": status
    }


# ================= è¾…åŠ©å‡½æ•° =================

def _generate_contract_summary(metadata: Dict, chunks: list) -> str:
    """
    ç”ŸæˆåˆåŒä¸Šä¸‹æ–‡æ‘˜è¦

    ç”¨é€”: ä¸ºå…¨å±€åˆ†ææä¾›åˆåŒçš„æ•´ä½“ç»“æ„ä¿¡æ¯

    Args:
        metadata: åˆåŒå…ƒæ•°æ®
        chunks: åˆ†å—åˆ—è¡¨

    Returns:
        åˆåŒæ‘˜è¦ (åŒ…æ‹¬: å½“äº‹äººã€æ ‡çš„ã€æœŸé™ã€åˆ†å—ç»“æ„ç­‰)
    """
    from ..utils import extract_section_keywords

    summary_parts = []

    # 1. åŸºæœ¬å…ƒæ•°æ®
    if metadata.get('contract_name'):
        summary_parts.append(f"åˆåŒåç§°: {metadata['contract_name']}")

    if metadata.get('parties'):
        # ä¿®å¤ï¼šå¤„ç† parties å­—ç¬¦ä¸²æ ¼å¼
        parties = metadata['parties']
        parties_info = ""

        if isinstance(parties, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
            parties_info = parties
        elif isinstance(parties, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæ ¼å¼åŒ–æ˜¾ç¤º
            parties_info = "; ".join([
                f"{p.get('role', 'æœªçŸ¥')}: {p.get('name', 'æœªå‘½å')}"
                for p in parties
            ])
        else:
            parties_info = str(parties)

        summary_parts.append(f"å½“äº‹äºº: {parties_info}")

    if metadata.get('contract_amount'):
        summary_parts.append(f"åˆåŒé‡‘é¢: {metadata['contract_amount']}")

    # 2. åˆ†å—ç»“æ„ä¿¡æ¯
    summary_parts.append(f"åˆåŒåˆ†å—ä¿¡æ¯: å…± {len(chunks)} å—")

    # 3. æ¯å—çš„ç®€çŸ­æè¿° (ä½¿ç”¨å…³é”®è¯æå–)
    chunk_descriptions = []
    for idx, (chunk_text, _) in enumerate(chunks):
        # æå–æ¯å—çš„å…³é”®è¯ (ä¾‹å¦‚: ç¬¬Xæ¡, ç¬¬XèŠ‚ç­‰)
        keywords = extract_section_keywords(chunk_text)
        chunk_descriptions.append(f"ç¬¬{idx+1}å—: {keywords}")

    summary_parts.append("åˆ†å—å†…å®¹: " + " | ".join(chunk_descriptions))

    return "\n".join(summary_parts)


def execute_stage_3_chunked_with_sliding_window(
    chunks: list,
    stance: str,
    profile: dict,
    relationships: dict,
    user_rules: list = None,
    transaction_structures: list = None
) -> ReviewOutput:
    """
    â­ æ ¸å¿ƒå‡½æ•°: æ»‘åŠ¨çª—å£åˆ†å—å®¡æŸ¥

    è®¾è®¡ç†å¿µ:
    - ä½¿ç”¨æ»‘åŠ¨çª—å£ç¡®ä¿ç›¸é‚»å—ä¹‹é—´çš„ä¸Šä¸‹æ–‡è¿ç»­æ€§
    - æ¯ä¸ªå—åŒ…å«: å‰ä¸€å—çš„å°¾éƒ¨ + å½“å‰å— + ä¸‹ä¸€å—çš„å¤´éƒ¨
    - è¿™æ ·å¯ä»¥è¯†åˆ«è·¨å—çš„é£é™©ç‚¹ (ä¾‹å¦‚: æ¡æ¬¾å¼•ç”¨ã€é€»è¾‘å†²çª)

    Args:
        chunks: åˆ†å—åˆ—è¡¨ [(chunk_text, (start, end)), ...]
        stance: å®¡æŸ¥ç«‹åœº
        profile: åˆåŒç”»åƒ
        relationships: æ³•å¾‹å…³ç³»
        user_rules: ç”¨æˆ·è‡ªå®šä¹‰è§„åˆ™
        transaction_structures: äº¤æ˜“ç»“æ„

    Returns:
        ReviewOutput: åˆå¹¶åçš„å®¡æŸ¥ç»“æœ
    """
    from typing import List, Tuple

    all_items = []

    # è§„åˆ™ç»„è£…
    rule_pack_text = rule_assembler.assemble_prompt_context(
        legal_features=profile,
        stance=stance,
        user_custom_rules=user_rules,
        transaction_structures=transaction_structures
    )

    # â­ æ»‘åŠ¨çª—å£å‚æ•°
    window_overlap = 300  # çª—å£é‡å å­—ç¬¦æ•°

    logger.info(f"[Sliding Window] å¼€å§‹åˆ†å—å®¡æŸ¥ - åˆ†å—æ•°: {len(chunks)}, çª—å£é‡å : {window_overlap}")

    # é€å—å®¡æŸ¥ (ä½¿ç”¨æ»‘åŠ¨çª—å£)
    for idx, (chunk_text, (start, end)) in enumerate(chunks):
        logger.info(f"[Sliding Window] å®¡æŸ¥ç¬¬ {idx+1}/{len(chunks)} å— (ä½ç½® {start}-{end})")

        # ========== æ„å»ºæ»‘åŠ¨çª—å£ä¸Šä¸‹æ–‡ ==========
        window_context = chunk_text

        # æ·»åŠ å‰ä¸€å—çš„å°¾éƒ¨ (å¦‚æœæœ‰)
        if idx > 0:
            prev_chunk_text = chunks[idx - 1][0]
            prev_tail = prev_chunk_text[-window_overlap:]
            window_context = f"[å‰å—å°¾éƒ¨]\n{prev_tail}\n\n[å½“å‰å—]\n{window_context}"

        # æ·»åŠ ä¸‹ä¸€å—çš„å¤´éƒ¨ (å¦‚æœæœ‰)
        if idx < len(chunks) - 1:
            next_chunk_text = chunks[idx + 1][0]
            next_head = next_chunk_text[:window_overlap]
            window_context = f"{window_context}\n\n[ä¸‹å—å¤´éƒ¨]\n{next_head}"

        # â­ å®¡æŸ¥å½“å‰çª—å£
        chunk_review = _review_single_window(
            window_text=window_context,
            chunk_position=(start, end),
            stance=stance,
            profile=profile,
            relationships=relationships,
            rule_pack_text=rule_pack_text,
            window_type="middle" if 0 < idx < len(chunks) - 1 else ("first" if idx == 0 else "last")
        )

        # åˆå¹¶å®¡æŸ¥é¡¹
        if chunk_review and hasattr(chunk_review, 'issues'):
            all_items.extend(chunk_review.issues)
            logger.info(f"[Sliding Window] ç¬¬{idx+1}å—å‘ç° {len(chunk_review.issues)} ä¸ªé£é™©ç‚¹")

    # ========== åˆå¹¶å’Œå»é‡ ==========
    merged_items = _merge_and_deduplicate_items(all_items)

    logger.info(f"[Sliding Window] åˆ†å—å®¡æŸ¥å®Œæˆ - åŸå§‹é¡¹: {len(all_items)}, åˆå¹¶å: {len(merged_items)}")

    return ReviewOutput(
        summary=f"å…±å‘ç° {len(merged_items)} ä¸ªé£é™©ç‚¹ (åˆ†å—å®¡æŸ¥: {len(chunks)} å—)",
        issues=merged_items
    )


def _review_single_window(
    window_text: str,
    chunk_position: Tuple[int, int],
    stance: str,
    profile: Dict,
    relationships: Dict,
    rule_pack_text: str,
    window_type: str  # "first" | "middle" | "last"
) -> Optional[ReviewOutput]:
    """
    å®¡æŸ¥å•ä¸ªæ»‘åŠ¨çª—å£

    Args:
        window_text: çª—å£æ–‡æœ¬ (åŒ…å«é‡å ä¸Šä¸‹æ–‡)
        chunk_position: å½“å‰å—åœ¨åŸæ–‡ä¸­çš„ä½ç½®
        stance: å®¡æŸ¥ç«‹åœº
        profile: åˆåŒç”»åƒ
        relationships: æ³•å¾‹å…³ç³»
        rule_pack_text: è§„åˆ™åŒ…
        window_type: çª—å£ç±»å‹

    Returns:
        ReviewOutput
    """
    # æ„å»ºä¸Šä¸‹æ–‡
    context = f"""
ã€å‰ç½®åˆ¤æ–­ä¾æ®ã€‘
1. åˆåŒæ³•å¾‹ç”»åƒ: {json.dumps(profile, ensure_ascii=False)}
2. æ³•å¾‹å…³ç³»å®šæ€§: {json.dumps(relationships, ensure_ascii=False)}

ã€å®¡æŸ¥è§„åˆ™åŒ…ã€‘
{rule_pack_text}

ã€å¾…å®¡æŸ¥æ–‡æœ¬ã€‘
âš ï¸ æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸Šä¸‹æ–‡çš„æ»‘åŠ¨çª—å£
- å½“å‰å—ä½ç½®: {chunk_position}
- çª—å£ç±»å‹: {window_type}
- è¯·é‡ç‚¹å…³æ³¨å½“å‰å—å†…çš„é£é™©,åŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡å…³è”

{window_text}
"""

    # æ„å»ºPrompt - â­ ä½¿ç”¨é²æ£’çš„è§£æå™¨ï¼Œæ”¯æŒéJSONæ¨¡å¼
    from ..json_parser import create_robust_parser

    # æ ¹æ®æ¨¡å‹èƒ½åŠ›å†³å®šæ˜¯å¦å¯ç”¨JSONæ¨¡å¼
    # æŸäº›æ¨¡å‹ï¼ˆå¦‚DeepSeekã€Qwenï¼‰å¯¹JSONæ¨¡å¼æ”¯æŒä¸ä½³ï¼Œä½¿ç”¨æ–‡æœ¬æ¨¡å¼æ›´ç¨³å®š
    model_name = os.getenv("MODEL_NAME", "gpt-4o-mini")

    # å¯¹äºå›½äº§æ¨¡å‹ï¼Œç¦ç”¨JSONæ¨¡å¼ä»¥æé«˜ç¨³å®šæ€§
    use_json_mode = not any(keyword in model_name.lower()
                           for keyword in ["deepseek", "qwen", "baichuan", "yi", "chatglm"])

    llm = get_model(json_mode=use_json_mode)

    # ä½¿ç”¨é²æ£’çš„è§£æå™¨
    parser = create_robust_parser(ReviewOutput)

    if use_json_mode:
        # JSONæ¨¡å¼ï¼šä¸¥æ ¼æ ¼å¼è¦æ±‚
        system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±æ³•å¾‹é¡¾é—®ã€‚è¯·å®¡æŸ¥ä»¥ä¸‹åˆåŒçª—å£å¹¶è¾“å‡ºJSONæ ¼å¼çš„å®¡æŸ¥ç»“æœã€‚

å…³é”®è¦æ±‚:
1. è¯†åˆ«å½“å‰å—ä¸­çš„é£é™©ç‚¹
2. æ³¨æ„ä¸Šä¸‹æ–‡å…³è” (ä¾‹å¦‚: æ¡æ¬¾å¼•ç”¨ã€é€»è¾‘å†²çª)
3. æ ‡æ³¨é£é™©åœ¨å½“å‰å—ä¸­çš„ä½ç½® (ç›¸å¯¹ä½ç½®)
4. é£é™©ç±»å‹ä½¿ç”¨æ ‡å‡†åˆ†ç±»
5. å¿…é¡»è¾“å‡ºç¬¦åˆJSONæ ¼å¼çš„ç»“æœ

{format_instructions}
"""
    else:
        # æ–‡æœ¬æ¨¡å¼ï¼šæ›´å®½æ¾çš„æ ¼å¼è¦æ±‚ï¼Œé€šè¿‡è§£æå™¨å¤„ç†
        system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±æ³•å¾‹é¡¾é—®ã€‚è¯·å®¡æŸ¥ä»¥ä¸‹åˆåŒçª—å£ã€‚

å…³é”®è¦æ±‚:
1. è¯†åˆ«å½“å‰å—ä¸­çš„é£é™©ç‚¹
2. æ³¨æ„ä¸Šä¸‹æ–‡å…³è” (ä¾‹å¦‚: æ¡æ¬¾å¼•ç”¨ã€é€»è¾‘å†²çª)
3. æ ‡æ³¨é£é™©åœ¨å½“å‰å—ä¸­çš„ä½ç½® (ç›¸å¯¹ä½ç½®)
4. é£é™©ç±»å‹ä½¿ç”¨æ ‡å‡†åˆ†ç±»

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœï¼ŒåŒ…å«ä»¥ä¸‹ç»“æ„ï¼š
{{
  "summary": "å®¡æŸ¥æ€»ç»“",
  "issues": [
    {{
      "issue_type": "é£é™©ç±»å‹",
      "quote": "ç›¸å…³æ¡æ¬¾åŸæ–‡",
      "explanation": "é£é™©è¯´æ˜",
      "suggestion": "ä¿®æ”¹å»ºè®®",
      "legal_basis": "æ³•å¾‹ä¾æ®",
      "severity": "ä¸¥é‡ç¨‹åº¦(Critical/High/Medium/Low)",
      "action_type": "æ“ä½œç±»å‹(Revision/Alert)"
    }}
  ]
}}
"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", "{context}")
    ])

    chain = prompt | llm | parser

    try:
        invoke_args = {"context": context}
        if use_json_mode:
            invoke_args["format_instructions"] = parser.get_format_instructions()

        result = chain.invoke(invoke_args)
        return result
    except Exception as e:
        logger.error(f"[Window Review Error] çª—å£å®¡æŸ¥å¤±è´¥: {e}")
        return ReviewOutput(summary=f"çª—å£å®¡æŸ¥å¤±è´¥: {str(e)}", issues=[])


def _merge_and_deduplicate_items(items: list) -> list:
    """
    â­ å…³é”®å‡½æ•°: åˆå¹¶å’Œå»é‡å®¡æŸ¥é¡¹

    ç­–ç•¥:
    1. æŒ‰ä½ç½®åˆ†ç»„ (ç›¸é‚»çš„ç›¸ä¼¼é¡¹å¯èƒ½æ˜¯åŒä¸€é£é™©)
    2. æŒ‰issue_typeåˆ†ç»„ (ç›¸åŒç±»å‹çš„é£é™©åˆå¹¶)
    3. å»é™¤å®Œå…¨é‡å¤é¡¹
    4. ä¼˜å…ˆçº§æ’åº (Critical > High > Medium > Low)

    Args:
        items: å®¡æŸ¥é¡¹åˆ—è¡¨

    Returns:
        åˆå¹¶åçš„å®¡æŸ¥é¡¹åˆ—è¡¨
    """
    if not items:
        return []

    # 1. æŒ‰ç±»å‹å»é‡ (å®Œå…¨ç›¸åŒå†…å®¹çš„è§†ä¸ºé‡å¤)
    seen_signatures = set()
    unique_items = []

    for item in items:
        # ç”Ÿæˆé¡¹çš„å”¯ä¸€ç­¾å (åŸºäº issue_type + quote çš„å‰50ä¸ªå­—ç¬¦)
        signature = f"{item.issue_type}_{item.quote[:50] if hasattr(item, 'quote') and item.quote else ''}"

        if signature not in seen_signatures:
            seen_signatures.add(signature)
            unique_items.append(item)

    logger.info(f"[Merge] å»é‡å‰: {len(items)}, å»é‡å: {len(unique_items)}")

    # 2. æŒ‰ç±»å‹åˆ†ç»„ (ç›¸ä¼¼ä½ç½®ã€ç›¸åŒç±»å‹çš„é¡¹)
    from collections import defaultdict
    item_groups = defaultdict(list)

    for item in unique_items:
        issue_type = item.issue_type if hasattr(item, 'issue_type') else "æœªçŸ¥"
        item_groups[issue_type].append(item)

    # 3. å¯¹æ¯ç»„è¿›è¡Œåˆå¹¶
    merged_items = []

    for issue_type, group_items in item_groups.items():
        if len(group_items) == 1:
            merged_items.append(group_items[0])
        else:
            # åˆå¹¶å¤šä¸ªç›¸ä¼¼é¡¹
            merged_item = _merge_similar_items(group_items)
            merged_items.append(merged_item)

    # 4. æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
    severity_order = {"Critical": 4, "High": 3, "Medium": 2, "Low": 1}

    merged_items.sort(
        key=lambda x: severity_order.get(
            x.severity if hasattr(x, 'severity') else "Medium",
            0
        ),
        reverse=True
    )

    logger.info(f"[Merge] æœ€ç»ˆåˆå¹¶å: {len(merged_items)} ä¸ªå®¡æŸ¥é¡¹")

    return merged_items


def _merge_similar_items(items: list) -> Any:
    """
    åˆå¹¶ç›¸ä¼¼çš„å®¡æŸ¥é¡¹

    ç­–ç•¥:
    - ä¿ç•™æœ€ä¸¥é‡ç¨‹åº¦çš„severity
    - åˆå¹¶explanation (ä½¿ç”¨åˆ†éš”ç¬¦)
    - åˆå¹¶suggestion (é€‰æ‹©æœ€å…¨é¢çš„)

    Args:
        items: ç›¸ä¼¼çš„å®¡æŸ¥é¡¹åˆ—è¡¨

    Returns:
        åˆå¹¶åçš„å®¡æŸ¥é¡¹
    """
    # é€‰æ‹©æœ€ä¸¥é‡çš„é¡¹ä½œä¸ºåŸºç¡€
    severity_order = {"Critical": 4, "High": 3, "Medium": 2, "Low": 1}

    base_item = max(
        items,
        key=lambda x: severity_order.get(
            x.severity if hasattr(x, 'severity') else "Medium",
            0
        )
    )

    # å¦‚æœæ‰€æœ‰é¡¹éƒ½ç›¸åŒ,ç›´æ¥è¿”å›
    if len(items) == 1:
        return base_item

    # åˆå¹¶explanation
    explanations = []
    for item in items:
        if hasattr(item, 'explanation') and item.explanation:
            explanations.append(item.explanation)

    merged_explanation = " | ".join(explanations) if explanations else ""

    # åˆå¹¶suggestion (é€‰æ‹©æœ€é•¿çš„)
    suggestions = []
    for item in items:
        if hasattr(item, 'suggestion') and item.suggestion:
            suggestions.append(item.suggestion)

    merged_suggestion = max(suggestions, key=len) if suggestions else ""

    # åˆ›å»ºåˆå¹¶åçš„é¡¹ (ä½¿ç”¨ ReviewOutput çš„å†…éƒ¨ç±»)
    # è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä¿®æ”¹ base_item çš„å±æ€§
    if hasattr(base_item, 'explanation'):
        base_item.explanation = merged_explanation
    if hasattr(base_item, 'suggestion'):
        base_item.suggestion = merged_suggestion

    logger.debug(f"[Merge] åˆå¹¶äº† {len(items)} ä¸ªç›¸ä¼¼é¡¹ (ç±»å‹: {base_item.issue_type if hasattr(base_item, 'issue_type') else 'æœªçŸ¥'})")

    return base_item