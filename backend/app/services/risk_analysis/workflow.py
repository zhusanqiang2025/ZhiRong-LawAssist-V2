# backend/app/services/risk_analysis/workflow.py

"""
é£é™©è¯„ä¼° LangGraph å·¥ä½œæµ

åŸºäº LangGraph çš„é£é™©è¯„ä¼°æµç¨‹ç¼–æ’
"""

import logging
import uuid
import asyncio
import json
from typing import TypedDict, Optional, List, Dict, Any
from datetime import datetime
from dataclasses import asdict

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from sqlalchemy.orm import Session

from app.database import SessionLocal
from app.models.risk_analysis import RiskAnalysisSession, RiskAnalysisStatus, RiskItem
from app.services.unified_document_service import get_unified_document_service, StructuredDocumentResult
from app.services.risk_analysis.document_preorganization import get_document_preorganization_service, PreorganizedDocuments
from app.services.risk_analysis.rule_assembler import get_risk_rule_assembler
from app.services.risk_analysis.multi_model_analyzer import get_multi_model_analyzer
from app.core.config import settings

logger = logging.getLogger(__name__)


# ==================== è¾…åŠ©å‡½æ•° ====================

async def send_ws_progress(session_id: str, node: str, status: str, message: str = "", progress: float = 0.0):
    """
    å‘é€ WebSocket è¿›åº¦æ¶ˆæ¯ï¼ˆå¼‚æ­¥ï¼‰

    é‡è¦ï¼šä¼˜é›…å¤„ç† WebSocket æ–­è¿ï¼Œé¿å… Broken Pipe é”™è¯¯å¯¼è‡´ä»»åŠ¡å´©æºƒ
    Args:
        session_id: ä¼šè¯ ID
        node: èŠ‚ç‚¹åç§° (documentPreorganization, multiModelAnalysis, reportGeneration)
        status: çŠ¶æ€ (pending, processing, completed, failed)
        message: è¿›åº¦æ¶ˆæ¯
        progress: è¿›åº¦ç™¾åˆ†æ¯” 0-1
    """
    try:
        from app.api.websocket import manager
        # æ£€æŸ¥è¿æ¥æ˜¯å¦å­˜åœ¨
        if not manager.is_connected(session_id):
            # è¿æ¥ä¸å­˜åœ¨ï¼Œé™é»˜è·³è¿‡ï¼ˆåå°æ¨¡å¼ï¼‰
            logger.debug(f"[WS] ä¼šè¯ {session_id} æ— æ´»è·ƒè¿æ¥ï¼Œè·³è¿‡è¿›åº¦æ¨é€ï¼ˆåå°æ¨¡å¼ï¼‰")
            return

        # å‘é€è¿›åº¦
        await manager.send_progress(session_id, {
            "type": "node_progress",
            "node": node,
            "status": status,
            "message": message,
            "progress": progress
        })
    except Exception as e:
        # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œé¿å…å¯¼è‡´ä»»åŠ¡å¤±è´¥
        # å¸¸è§å¼‚å¸¸ï¼šBrokenPipeError, ConnectionResetError, RuntimeError
        logger.debug(f"[WS] è¿›åº¦æ¨é€å¤±è´¥ï¼ˆä¼šè¯å¯èƒ½å·²æ–­å¼€ï¼‰: {type(e).__name__}")
        # ä¸è¦æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä»»åŠ¡ç»§ç»­æ‰§è¡Œ
        pass


# ==================== çŠ¶æ€å®šä¹‰ ====================

class RiskAnalysisState(TypedDict):
    """é£é™©è¯„ä¼°çŠ¶æ€"""
    # è¾“å…¥
    session_id: str
    user_input: str
    document_paths: List[str]
    package_id: Optional[str]  # å¯é€‰ï¼šNone è¡¨ç¤ºé€šç”¨è¯„ä¼°æ¨¡å¼
    stop_after_preorganization: Optional[bool]  # æ˜¯å¦åœ¨é¢„æ•´ç†ååœæ­¢

    # ç¬¬ä¸€é˜¶æ®µï¼šæ–‡æ¡£å¤„ç†
    raw_documents: Optional[List[StructuredDocumentResult]]  # UnifiedDocumentService è¾“å‡º
    preorganized_docs: Optional[PreorganizedDocuments]  # é¢„æ•´ç†è¾“å‡º

    # ç¬¬äºŒé˜¶æ®µï¼šè§„åˆ™ç»„è£…
    assembled_rules: Optional[List[Dict[str, Any]]]
    analysis_mode: Optional[str]  # "general" æˆ– "package_based"
    selected_model: Optional[str]  # å•æ¨¡å‹åˆ†ææ—¶æŒ‡å®šçš„æ¨¡å‹åç§°ï¼ˆå¦‚ "deepseek"ï¼‰

    # --- æ–°å¢ï¼šç”¨äºä¼ é€’å¢å¼ºåˆ†æç»“æœ ---
    enhanced_analysis_data: Optional[Dict[str, Any]]  # æ¥è‡ª enhanced_document_analysis
    # ---------------------------------------

    context: Optional[Dict[str, Any]]  # åˆ†æä¸Šä¸‹æ–‡

    # ç¬¬ä¸‰é˜¶æ®µï¼šAIåˆ†æ
    analysis_results: Optional[Dict[str, Any]]  # é‡å‘½åä»¥é¿å…ä¸ model_ å†²çª

    # è¾“å‡º
    final_report: Optional[str]
    final_result: Optional[Dict[str, Any]]  # æœ€ç»ˆç»¼åˆç»“æœ
    aggregated: Optional[Dict[str, Any]]  # ä¸­é—´æ±‡æ€»ç»“æœ
    status: str
    error: Optional[str]


# ==================== èŠ‚ç‚¹å‡½æ•° ====================

async def process_documents_node(state: RiskAnalysisState) -> Dict[str, Any]:
    """èŠ‚ç‚¹1ï¼šå¤„ç†æ–‡æ¡£ï¼ˆåŸºç¡€æå–ï¼‰"""
    session_id = state['session_id']
    # å‘é€ WebSocket è¿›åº¦ï¼šå¼€å§‹å¤„ç†æ–‡æ¡£
    await send_ws_progress(
        session_id, "documentPreorganization", "processing",
        "æ­£åœ¨å¤„ç†æ–‡æ¡£ï¼Œæå–å†…å®¹...", 0.05
    )
    logger.info(f"[{state['session_id']}] å¼€å§‹å¤„ç†æ–‡æ¡£...")
    doc_service = get_unified_document_service()

    try:
        results = await doc_service.batch_process_async(
            state["document_paths"],
            extract_content=True,
            extract_metadata=True,
            max_concurrent=3
        )

        # è¿‡æ»¤æˆåŠŸçš„æ–‡æ¡£
        successful_docs = [r for r in results if r.status == "success"]

        if not successful_docs:
            logger.error(f"[{state['session_id']}] æ–‡æ¡£å¤„ç†å…¨éƒ¨å¤±è´¥")
            return {
                "raw_documents": [],
                "status": "failed",
                "error": "æ–‡æ¡£å¤„ç†å…¨éƒ¨å¤±è´¥"
            }

        logger.info(f"[{state['session_id']}] æ–‡æ¡£å¤„ç†å®Œæˆ: {len(successful_docs)}/{len(results)} æˆåŠŸ")

        return {
            "raw_documents": successful_docs,
            "status": "documents_processed"
        }

    except Exception as e:
        logger.error(f"[{state['session_id']}] æ–‡æ¡£å¤„ç†å¼‚å¸¸: {e}", exc_info=True)
        return {
            "status": "failed",
            "error": f"æ–‡æ¡£å¤„ç†å¼‚å¸¸: {str(e)}"
        }


async def preorganize_documents_node(state: RiskAnalysisState) -> Dict[str, Any]:
    """èŠ‚ç‚¹2ï¼šé¢„æ•´ç†æ–‡æ¡£ï¼ˆä¸“ä¸šåŠ©ç†èŠ‚ç‚¹ï¼‰- å¢å¼ºç‰ˆ"""
    session_id = state['session_id']

    # å‘é€ WebSocket è¿›åº¦ï¼šå¼€å§‹å¤„ç†
    await send_ws_progress(
        session_id, "documentPreorganization", "processing",
        "æ­£åœ¨è¿›è¡Œæ–‡æ¡£é¢„æ•´ç†...", 0.05
    )

    logger.info(f"[{session_id}] å¼€å§‹é¢„æ•´ç†æ–‡æ¡£...")

    if not state.get("raw_documents"):
        await send_ws_progress(session_id, "documentPreorganization", "failed", "æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£", 0)
        return {"status": "no_documents", "error": "æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£"}

    llm = ChatOpenAI(
        model=settings.MODEL_NAME or "Qwen3-235B-A22B-Thinking-2507",
        api_key=settings.LANGCHAIN_API_KEY,
        base_url=settings.LANGCHAIN_API_BASE_URL,
        temperature=0
    )

    # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
    async def progress_callback(step: str, progress: float, message: str):
        """è¿›åº¦å›è°ƒï¼Œå®æ—¶å‘é€è¿›åº¦æ›´æ–°"""
        await send_ws_progress(session_id, "documentPreorganization", "processing", message, progress)

    try:
        # ========================================================
        # æ­¥éª¤1ï¼šåŸºç¡€é¢„æ•´ç†ï¼ˆåˆ†ç±»ã€è´¨é‡è¯„ä¼°ã€åŸºç¡€æ‘˜è¦ï¼‰
        # ========================================================
        # è¿›åº¦èŒƒå›´ï¼š0.1 - 0.45
        await send_ws_progress(session_id, "documentPreorganization", "processing",
                               "æ­£åœ¨è¿›è¡Œæ–‡æ¡£åˆ†ç±»å’Œè´¨é‡è¯„ä¼°...", 0.1)

        preorg_service = get_document_preorganization_service(llm)
        preorganized = await preorg_service.preorganize(
            documents=state["raw_documents"],
            user_context=state.get("user_input"),
            progress_callback=progress_callback
        )

        # ========================================================
        # æ­¥éª¤2ï¼šå¢å¼ºåˆ†æï¼ˆäº¤æ˜“å…¨æ™¯ã€ä¸»ä½“ç”»åƒã€æ—¶é—´çº¿ï¼‰
        # ========================================================
        # è¿›åº¦èŒƒå›´ï¼š0.45 - 0.75
        await send_ws_progress(session_id, "documentPreorganization", "processing",
                               "æ­£åœ¨æ„å»ºäº¤æ˜“å…¨æ™¯å›¾...", 0.45)

        from app.services.risk_analysis.enhanced_document_analysis import get_enhanced_document_analysis_service
        enhanced_analysis_service = get_enhanced_document_analysis_service(llm)

        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ï¼šä¼ å…¥ preorganized_data é¿å…é‡å¤è®¡ç®—
        enhanced_analysis = await enhanced_analysis_service.analyze_documents(
            documents=state["raw_documents"],
            preorganized_data=preorganized,
            user_context=state.get("user_input"),
            progress_callback=progress_callback
        )

        # ========================================================
        # æ­¥éª¤3ï¼šæ³•å¾‹ç‰¹å¾å¢å¼ºï¼ˆæ¶æ„å›¾ç­‰ï¼‰
        # ========================================================
        # è¿›åº¦èŒƒå›´ï¼š0.75 - 0.95
        await send_ws_progress(session_id, "documentPreorganization", "processing",
                               "æ­£åœ¨ç»˜åˆ¶æ³•å¾‹æ¶æ„å›¾...", 0.75)

        extra_features = await preorg_service.enhance_preorganization_result(
            documents=state["raw_documents"],
            summaries=preorganized.document_summaries,
            classification=preorganized.document_classification
        )

        await send_ws_progress(session_id, "documentPreorganization", "completed",
                               "æ–‡æ¡£é¢„æ•´ç†å®Œæˆ", 1.0)

        logger.info(f"[{session_id}] æ–‡æ¡£é¢„æ•´ç†å®Œæˆï¼ˆå«å¢å¼ºåˆ†æï¼‰")

        # ========================================================
        # æ•°æ®åºåˆ—åŒ–ä¸å‘é€ (æ›´æ–°é€‚é…æ–°çš„æ•°æ®ç»“æ„)
        # ========================================================
        try:
            from app.api.websocket import manager
            # åºåˆ—åŒ–åŸºç¡€é¢„æ•´ç†æ•°æ® - ä½¿ç”¨ Pydantic model_dump() ä¿ç•™æ‰€æœ‰å­—æ®µ
            preorganized_data = {
                "document_summaries": {
                    path: summary.model_dump(mode='json', exclude_none=False)
                    for path, summary in preorganized.document_summaries.items()
                },
                "document_classification": preorganized.document_classification,
                "ranked_documents": preorganized.ranked_documents
            }

            # åºåˆ—åŒ–å¢å¼ºåˆ†ææ•°æ® (é€‚é… EnhancedAnalysisResult ç»“æ„)
            enhanced_analysis_data_for_ws = {
                "transaction_summary": enhanced_analysis.transaction_summary,
                "contract_status": enhanced_analysis.contract_status,
                "dispute_focus": enhanced_analysis.dispute_focus,
                # ä½¿ç”¨ asdict è½¬æ¢ dataclass å¯¹è±¡
                "parties": [asdict(p) for p in enhanced_analysis.parties],
                "timeline": [asdict(t) for t in enhanced_analysis.timeline],
                "relationships": enhanced_analysis.doc_relationships,
                # æ–°å¢ï¼šå°†æ¶æ„å›¾æ•°æ®åŒ…å«åœ¨ enhanced_analysis ä¸­
                "architecture_diagram": extra_features.get("architecture_diagram") if extra_features else None
            }

            await manager.send_progress(session_id, {
                "type": "preorganization_completed",
                "has_result": True,
                "can_proceed": True,
                "preorganized_data": preorganized_data,
                "enhanced_analysis": enhanced_analysis_data_for_ws,
                "enhanced_data": extra_features  # æ¶æ„å›¾ã€æ³•å¾‹ç‰¹å¾ç­‰
            })
            logger.info(f"[WS] å‘é€preorganization_completedæ¶ˆæ¯æˆåŠŸ: {session_id}")
        except Exception as e:
            logger.error(f"[WS] å‘é€preorganization_completedæ¶ˆæ¯å¤±è´¥: {e}")

        # ========================================================
        # ä¿å­˜é¢„æ•´ç†ç»“æœåˆ°æ•°æ®åº“
        # ========================================================
        try:
            from app.database import SessionLocal
            from app.models.risk_analysis_preorganization import RiskAnalysisPreorganization
            db = SessionLocal()
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è®°å½•
                existing_record = db.query(RiskAnalysisPreorganization).filter(
                    RiskAnalysisPreorganization.session_id == session_id
                ).first()

                if not existing_record:
                    # å°†æ–°çš„æ•°æ®ç»“æ„æ˜ å°„åˆ° DB å­—æ®µ
                    preorg_record = RiskAnalysisPreorganization(
                        session_id=session_id,
                        # æ˜ å°„ï¼šäº¤æ˜“ç»¼è¿° -> ç”¨æˆ·éœ€æ±‚æ‘˜è¦(æˆ–åˆ›å»ºæ–°å­—æ®µ)
                        user_requirement_summary=enhanced_analysis.transaction_summary,
                        # æ˜ å°„ï¼šåŸºç¡€æ‘˜è¦ä¿¡æ¯ - preorganized_data["document_summaries"] å·²ç»æ˜¯å­—å…¸
                        documents_info=json.dumps(preorganized_data.get("document_summaries", {}), ensure_ascii=False),
                        # æ˜ å°„ï¼šæ—¶é—´çº¿ -> äº‹å®æ‘˜è¦
                        fact_summary=json.dumps([asdict(t) for t in enhanced_analysis.timeline], ensure_ascii=False),
                        # æ˜ å°„ï¼šæ³•å¾‹ç‰¹å¾
                        contract_legal_features=extra_features.get("contract_legal_features"),
                        # æ˜ å°„ï¼šæ–‡æ¡£å…³ç³»
                        contract_relationships=enhanced_analysis.doc_relationships,
                        # æ˜ å°„ï¼šæ¶æ„å›¾
                        architecture_diagram=extra_features.get("architecture_diagram"),
                        # æ–°å¢ï¼šä¿å­˜å®Œæ•´çš„ enhanced_analysis æ•°æ®
                        enhanced_analysis_json=json.dumps({
                            "transaction_summary": enhanced_analysis.transaction_summary,
                            "contract_status": enhanced_analysis.contract_status,
                            "dispute_focus": enhanced_analysis.dispute_focus,
                            "parties": [asdict(p) for p in enhanced_analysis.parties],
                            "timeline": [asdict(t) for t in enhanced_analysis.timeline],
                            "doc_relationships": enhanced_analysis.doc_relationships,
                            # æ–°å¢ï¼šåŒ…å«æ¶æ„å›¾æ•°æ®
                            "architecture_diagram": extra_features.get("architecture_diagram") if extra_features else None
                        }, ensure_ascii=False),
                        is_confirmed=False
                    )
                    db.add(preorg_record)
                    db.commit()
                    logger.info(f"[RiskAnalysisWorkflow] é¢„æ•´ç†ç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“: {session_id}")
            finally:
                db.close()
        except Exception as e:
            logger.error(f"[RiskAnalysisWorkflow] ä¿å­˜é¢„æ•´ç†ç»“æœå¤±è´¥: {e}", exc_info=True)

        # ========================================================
        # --- ä¿®æ”¹ç‚¹ï¼šè¿”å›æ—¶åŒ…å« enhanced_analysis_data ---
        # ========================================================
        # å°† enhanced_analysis å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ï¼Œå¹¶åˆå¹¶ extra_features
        # ä½¿ç”¨ asdict è½¬æ¢ dataclass
        enhanced_data_dict = {
            "transaction_summary": enhanced_analysis.transaction_summary,
            "contract_status": enhanced_analysis.contract_status,
            "dispute_focus": enhanced_analysis.dispute_focus,
            "parties": [asdict(p) for p in enhanced_analysis.parties],
            "timeline": [asdict(t) for t in enhanced_analysis.timeline],
            "relationships": enhanced_analysis.doc_relationships,
            # åˆå¹¶æ¶æ„å›¾ç­‰é¢å¤–ç‰¹å¾
            "architecture_diagram": extra_features.get("architecture_diagram") if extra_features else None
        }

        return {
            "preorganized_docs": preorganized,
            "enhanced_analysis_data": enhanced_data_dict,  # <--- æ–°å¢è¿”å›å­—æ®µ
            "status": "preorganization_completed"
        }
        # ----------------------------------------------

    except Exception as e:
        logger.error(f"[{session_id}] æ–‡æ¡£é¢„æ•´ç†å¤±è´¥: {e}", exc_info=True)
        await send_ws_progress(session_id, "documentPreorganization", "failed",
                               f"é¢„æ•´ç†å¤±è´¥: {str(e)}", 0)
        return {
            "preorganized_docs": None,
            "status": "preorganization_failed"
        }


async def assemble_rules_node(state: RiskAnalysisState) -> Dict[str, Any]:
    """èŠ‚ç‚¹3ï¼šç»„è£…è§„åˆ™ï¼ˆåŸºäºé¢„æ•´ç†ç»“æœï¼‰"""
    logger.info(f"[{state['session_id']}] å¼€å§‹ç»„è£…è§„åˆ™...")
    db = SessionLocal()

    try:
        package_id = state.get("package_id")

        # é€šç”¨è¯„ä¼°æ¨¡å¼ï¼šæ²¡æœ‰è§„åˆ™åŒ…
        if not package_id:
            logger.info(f"[{state['session_id']}] é€šç”¨è¯„ä¼°æ¨¡å¼ï¼Œä¸ä½¿ç”¨è§„åˆ™åŒ…")
            # æ„å»ºåŸºç¡€ä¸Šä¸‹æ–‡
            raw_docs = state.get("raw_documents") or []
            context = {
                "user_input": state["user_input"],
                "classification": {},
                "cross_doc_info": {},
                "document_count": len(raw_docs) if raw_docs else 0
            }
            # å¦‚æœæœ‰é¢„æ•´ç†ç»“æœï¼Œä½¿ç”¨é¢„æ•´ç†æ•°æ®
            preorganized = state.get("preorganized_docs")
            if preorganized:
                if hasattr(preorganized, 'document_classification'):
                    context["classification"] = preorganized.document_classification
                if hasattr(preorganized, 'cross_doc_info'):
                    context["cross_doc_info"] = preorganized.cross_doc_info

            return {
                "assembled_rules": [],
                "context": context,
                "analysis_mode": "general",
                "status": "rules_assembled"
            }

        # è§„åˆ™åŒ…è¯„ä¼°æ¨¡å¼ï¼šä½¿ç”¨è§„åˆ™åŒ…
        # æ„å»ºä¸Šä¸‹æ–‡
        context = {
            "user_input": state["user_input"],
            "classification": {},
            "cross_doc_info": {}
        }

        # å¦‚æœæœ‰é¢„æ•´ç†ç»“æœï¼Œä½¿ç”¨é¢„æ•´ç†æ•°æ®
        preorganized = state.get("preorganized_docs")
        if preorganized:
            if hasattr(preorganized, 'document_classification'):
                context["classification"] = preorganized.document_classification
            if hasattr(preorganized, 'cross_doc_info'):
                context["cross_doc_info"] = preorganized.cross_doc_info
        else:
            # é™çº§ï¼šä½¿ç”¨åŸå§‹æ–‡æ¡£
            raw_docs = state.get("raw_documents") or []
            context["document_count"] = len(raw_docs) if raw_docs else 0

        assembler = get_risk_rule_assembler(db)
        rules = assembler.assemble_rules(
            package_id=package_id,
            context=context
        )

        logger.info(f"[{state['session_id']}] è§„åˆ™ç»„è£…å®Œæˆ: {len(rules)} æ¡è§„åˆ™")

        return {
            "assembled_rules": rules,
            "context": context,
            "analysis_mode": "package_based",
            "status": "rules_assembled"
        }

    except Exception as e:
        logger.error(f"[{state['session_id']}] è§„åˆ™ç»„è£…å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "failed",
            "error": f"è§„åˆ™ç»„è£…å¤±è´¥: {str(e)}"
        }
    finally:
        db.close()


async def multi_model_analyze_node(state: RiskAnalysisState) -> Dict[str, Any]:
    """èŠ‚ç‚¹4ï¼šå¤šæ¨¡å‹å¹¶è¡Œåˆ†æï¼ˆæ”¯æŒå•/å¤šæ¨¡å‹åˆ‡æ¢ï¼‰"""
    session_id = state['session_id']

    # ä» state è·å–é…ç½®
    rules = state.get("assembled_rules")
    analysis_mode = state.get("analysis_mode", "multi")  # é»˜è®¤ä¸º multi
    selected_model = state.get("selected_model")  # å•æ¨¡å‹æ—¶æŒ‡å®šçš„æ¨¡å‹å (å¦‚ "deepseek")

    mode_text = f"å•æ¨¡å‹ ({selected_model})" if analysis_mode == "single" else "å¤šæ¨¡å‹ç»¼åˆ"

    # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°ï¼Œç”¨äº MultiModelAnalyzer
    async def progress_callback(step: str, progress: float, message: str):
        """è¿›åº¦å›è°ƒï¼Œå°†è¿›åº¦è½¬æ¢ä¸º multiModelAnalysis èŠ‚ç‚¹çš„è¿›åº¦"""
        # MultiModelAnalyzer çš„è¿›åº¦èŒƒå›´æ˜¯ 75%-95%ï¼Œæ˜ å°„åˆ° multiModelAnalysis èŠ‚ç‚¹çš„ 40%-95%
        # progress 0.76-0.95 â†’ èŠ‚ç‚¹è¿›åº¦ 0.40-0.95
        node_progress = 0.40 + (progress - 0.75) * (0.95 - 0.40) / (0.95 - 0.75)
        node_progress = max(0.40, min(0.95, node_progress))  # é™åˆ¶åœ¨ 0.40-0.95 èŒƒå›´å†…
        await send_ws_progress(
            session_id, "multiModelAnalysis", "processing",
            message, node_progress
        )

    # å¼‚æ­¥èŠ‚ç‚¹ä¸­å¯ä»¥ç›´æ¥ await
    await send_ws_progress(
        session_id, "multiModelAnalysis", "processing",
        f"æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†æ [{mode_text}]...", 0.40
    )

    logger.info(f"[{session_id}] å¼€å§‹åˆ†æ: {mode_text}")

    # æ„å»ºä¸Šä¸‹æ–‡
    preorganized = state.get("preorganized_docs")
    raw_docs = state.get("raw_documents", [])
    evaluation_stance = state.get("evaluation_stance")  # âœ… æ–°å¢ï¼šè·å–è¯„ä¼°ç«‹åœº

    # âœ… æ–°å¢ï¼šè¿½è¸ªæ—¥å¿— - ç¡®è®¤ç«‹åœºæ˜¯å¦è¢«ä¼ é€’
    if evaluation_stance:
        logger.info(f"[{session_id}] âœ… æ£€æµ‹åˆ°è¯„ä¼°ç«‹åœº: {evaluation_stance[:100]}...")
    else:
        logger.warning(f"[{session_id}] âš ï¸ æœªæä¾›è¯„ä¼°ç«‹åœºï¼ˆevaluation_stance ä¸ºç©ºï¼‰")

    if preorganized and hasattr(preorganized, 'document_summaries'):
        summaries = []
        for path, summary in preorganized.document_summaries.items():
            summaries.append(f"æ–‡æ¡£: {path}\næ‘˜è¦: {summary.summary}")
        context = "\n\n".join(summaries)
    else:
        context = "\n\n".join([doc.content for doc in raw_docs])

    # âœ… æ–°å¢ï¼šæ³¨å…¥è¯„ä¼°ç«‹åœºåˆ°ä¸Šä¸‹æ–‡
    if evaluation_stance:
        logger.info(f"[{session_id}] ğŸ“ æ­£åœ¨æ³¨å…¥è¯„ä¼°ç«‹åœºåˆ° LLM ä¸Šä¸‹æ–‡...")
        context = f"""**é‡è¦æç¤ºï¼šæœ¬æ¬¡é£é™©è¯„ä¼°åº”é‡‡ç”¨ä»¥ä¸‹ç«‹åœºè¿›è¡Œåˆ†æ**
{evaluation_stance}

è¯·åœ¨åˆ†æè¿‡ç¨‹ä¸­å§‹ç»ˆåŸºäºä¸Šè¿°ç«‹åœºï¼Œé‡ç‚¹å…³æ³¨è¯¥ç«‹åœºä¸‹çš„é£é™©ç‚¹å’Œå»ºè®®ã€‚

---

{context}
"""
        logger.info(f"[{session_id}] âœ… è¯„ä¼°ç«‹åœºå·²æ³¨å…¥ï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
    else:
        if state.get("user_input"):
            context = f"## ç”¨æˆ·éœ€æ±‚\n{state['user_input']}\n\n## æ–‡æ¡£å†…å®¹\n{context}"

    # ========================================================
    # --- ä¿®æ”¹ç‚¹ï¼šä¼˜å…ˆè·å– enhanced_analysis_data ---
    # ========================================================
    # è¿™æ˜¯ preorganize_documents_node æ–°å¢çš„å­—æ®µï¼ŒåŒ…å«äº†å®Œæ•´çš„å¢å¼ºåˆ†æç»“æœ
    enhanced_data = state.get("enhanced_analysis_data")

    # å¦‚æœæ²¡æœ‰æ–°çš„æ•°æ®ï¼Œé™çº§ä½¿ç”¨æ—§çš„ cross_doc_info (å…¼å®¹æ€§å¤„ç†)
    if not enhanced_data and preorganized and hasattr(preorganized, 'cross_doc_info'):
        enhanced_data = preorganized.cross_doc_info
    # ---------------------------------------

    try:
        analyzer = get_multi_model_analyzer()
        # è°ƒç”¨åˆ†æå™¨ (ä¼ å…¥æ–°çš„å‚æ•°)
        result = await analyzer.analyze_parallel(
            context=context,
            rules=rules or [],
            session_id=state["session_id"],
            analysis_mode=analysis_mode,  # <--- ä¼ å…¥æ¨¡å¼
            selected_model=selected_model,  # <--- ä¼ å…¥é€‰å®šçš„æ¨¡å‹
            enhanced_data=enhanced_data,
            progress_callback=progress_callback  # <--- ä¼ å…¥è¿›åº¦å›è°ƒ
        )

        if result.get("error"):
            await send_ws_progress(session_id, "multiModelAnalysis", "failed",
                                   result.get("error", "åˆ†æå¤±è´¥"), 0.6)
            return {
                "status": "failed",
                "error": result["error"]
            }

        final_result = result.get("final_result", {})
        aggregated = result.get("aggregated", {})

        await send_ws_progress(session_id, "multiModelAnalysis", "completed",
                               "æ™ºèƒ½åˆ†æå®Œæˆ", 0.7)

        return {
            "analysis_results": result,
            "final_result": final_result,
            "aggregated": aggregated,
            "status": "analysis_completed"
        }

    except Exception as e:
        logger.error(f"[{session_id}] åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {"status": "failed", "error": str(e)}


async def generate_report_node(state: RiskAnalysisState) -> Dict[str, Any]:
    """èŠ‚ç‚¹5ï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆå¢å¼ºç‰ˆï¼šæ•´ä½“è¯„ä¼° + ä¼˜å…ˆçº§æ’åº + æ¥æºè§†è§’ï¼‰"""
    session_id = state['session_id']

    # å¼‚æ­¥èŠ‚ç‚¹ä¸­å¯ä»¥ç›´æ¥ await
    await send_ws_progress(
        session_id, "reportGeneration", "processing",
        "æ­£åœ¨ç”Ÿæˆé£é™©è¯„ä¼°æŠ¥å‘Š...", 0.8
    )

    logger.info(f"[{session_id}] å¼€å§‹ç”ŸæˆæŠ¥å‘Š...")

    # è·å–æœ€ç»ˆç»¼åˆç»“æœ
    final_result = state.get("final_result")
    if not final_result:
        await send_ws_progress(session_id, "reportGeneration", "failed",
                               "æ²¡æœ‰å¯ç”¨çš„åˆ†æç»“æœ", 0.8)
        return {
            "status": "failed",
            "error": "æ²¡æœ‰å¯ç”¨çš„åˆ†æç»“æœ"
        }

    # è·å–æ•´ä½“è¯„ä¼°
    overall_assessment = final_result.get("overall_assessment", {})
    risk_level = overall_assessment.get("risk_level", "medium")
    core_risks = overall_assessment.get("core_risks", [])
    handling_recommendation = overall_assessment.get("recommendation", "å»ºè®®ä¼˜å…ˆå¤„ç†é«˜é£é™©é¡¹ï¼Œé€æ­¥é™ä½æ•´ä½“é£é™©æ°´å¹³")

    # è·å–é£é™©é¡¹
    risk_items = final_result.get("risk_items", [])

    # æŒ‰ä¼˜å…ˆçº§æ’åºé£é™©é¡¹
    sorted_risks = _sort_risks_by_priority(risk_items)

    await send_ws_progress(session_id, "reportGeneration", "processing",
                           "æ­£åœ¨æ•´ç†é£é™©æ¸…å•...", 0.9)

    # âœ… æ–°å¢ï¼šè·å–è¯„ä¼°ç«‹åœºå¹¶æ³¨å…¥åˆ°æŠ¥å‘Š
    evaluation_stance = state.get("evaluation_stance")

    # æ„å»º Markdown æŠ¥å‘Š
    report_lines = [
        "# é£é™©è¯„ä¼°æŠ¥å‘Š\n",
        f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**æ€»ä½“ç½®ä¿¡åº¦**: {final_result.get('total_confidence', 0):.1%}\n"
    ]

    # âœ… å¢å¼ºç‰ˆï¼šå¦‚æœæä¾›äº†è¯„ä¼°ç«‹åœºï¼Œåœ¨æŠ¥å‘Šå¼€å¤´æ˜¾è‘—æ˜¾ç¤º
    if evaluation_stance:
        report_lines.extend([
            "## ğŸ¯ è¯„ä¼°ç«‹åœº\n",
            f"\n{evaluation_stance}\n",
            "---\n",
            "### âš ï¸ é‡è¦è¯´æ˜",
            f"\n**æœ¬æŠ¥å‘Šå®Œå…¨åŸºäºä¸Šè¿°ã€Œ{evaluation_stance[:30]}...ã€ç«‹åœºè¿›è¡Œåˆ†æ**ã€‚æ‰€æœ‰é£é™©è¯†åˆ«ã€è¯„çº§å’Œå»ºè®®å‡ä»è¯¥è§†è§’å‡ºå‘ï¼Œå¯èƒ½ä¸å…¶ä»–ç«‹åœºä¸‹çš„è¯„ä¼°ç»“æœå­˜åœ¨å·®å¼‚ã€‚",
            "å»ºè®®è¯»è€…ï¼š",
            f"1. ç»“åˆè‡ªèº«å®é™…æƒ…å†µç†è§£æœ¬æŠ¥å‘Šçš„é£é™©åˆ¤æ–­",
            f"2. å¦‚éœ€ä¸åŒç«‹åœºä¸‹çš„é£é™©è¯„ä¼°ï¼ˆå¦‚å¯¹æ–¹è§†è§’ã€ä¸­ç«‹ç¬¬ä¸‰æ–¹è§†è§’ï¼‰ï¼Œè¯·é‡æ–°æŒ‡å®šç«‹åœºå¹¶åˆ†æ",
            f"3. é‡ç‚¹å…³æ³¨ä¸è¯¥ç«‹åœºç›¸å…³çš„æ ¸å¿ƒé£é™©ç‚¹\n",
            "---\n"
        ])

    report_lines.extend([
        "## ä¸€ã€æ•´ä½“è¯„ä¼°\n",
        f"**é£é™©ç­‰çº§**: {_format_risk_level(risk_level)}",
        f"**åº”å¯¹å»ºè®®**: {handling_recommendation}\n"
    ])

    # æ ¸å¿ƒé£é™©æ¦‚è¿°
    if core_risks:
        report_lines.append("**æ ¸å¿ƒé£é™©**:")
        for risk in core_risks:
            report_lines.append(f"- {risk}")
        report_lines.append("")

    # æ€»ä½“æ‘˜è¦
    summary = final_result.get("summary", "")
    if summary:
        report_lines.extend([
            "**æ€»ä½“æ‘˜è¦**:",
            summary,
            "\n---\n"
        ])

    # é£é™©åˆ†å¸ƒç»Ÿè®¡
    report_lines.append("## äºŒã€é£é™©åˆ†å¸ƒç»Ÿè®¡\n")

    # è®¡ç®—åˆ†å¸ƒ
    distribution = {"critical": 0, "high": 0, "medium": 0, "low": 0}
    for risk in risk_items:
        level = risk.get("risk_level", "low").lower()
        if level in distribution:
            distribution[level] += 1

    report_lines.extend([
        f"- ğŸ”´ é«˜é£é™©ï¼ˆå«ä¸¥é‡ï¼‰ï¼š{distribution.get('critical', 0) + distribution.get('high', 0)} ä¸ª",
        f"- ğŸŸ¡ ä¸­é£é™©ï¼š{distribution.get('medium', 0)} ä¸ª",
        f"- ğŸŸ¢ ä½é£é™©ï¼š{distribution.get('low', 0)} ä¸ª",
        "\n---\n",
        "## ä¸‰ã€ä¼˜å…ˆçº§æ’åºé£é™©æ¸…å•\n"
    ])

    # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„å±•ç¤º
    high_risks = [r for r in sorted_risks if r.get("risk_level") in ["critical", "high"]]
    medium_risks = [r for r in sorted_risks if r.get("risk_level") == "medium"]
    low_risks = [r for r in sorted_risks if r.get("risk_level") == "low"]

    # é«˜é£é™©
    if high_risks:
        report_lines.extend([
            "### ğŸ”´ ä¼˜å…ˆçº§1 - é«˜é£é™©ï¼ˆéœ€ç«‹å³å¤„ç†ï¼‰\n"
        ])
        for i, risk in enumerate(high_risks, 1):
            report_lines.extend(_format_risk_item(risk, i))

    # ä¸­é£é™©
    if medium_risks:
        report_lines.extend([
            "### ğŸŸ¡ ä¼˜å…ˆçº§2 - ä¸­é£é™©ï¼ˆå»ºè®®å¤„ç†ï¼‰\n"
        ])
        for i, risk in enumerate(medium_risks, 1):
            report_lines.extend(_format_risk_item(risk, i))

    # ä½é£é™©
    if low_risks:
        report_lines.extend([
            "### ğŸŸ¢ ä¼˜å…ˆçº§3 - ä½é£é™©ï¼ˆå¯å…³æ³¨ï¼‰\n"
        ])
        for i, risk in enumerate(low_risks, 1):
            report_lines.extend(_format_risk_item(risk, i))

    # ç»¼åˆå»ºè®®
    report_lines.extend([
        "\n---\n",
        "## å››ã€ç»¼åˆå»ºè®®\n",
        _generate_comprehensive_suggestions(sorted_risks),
        "\n---\n",
        "## å…è´£å£°æ˜\n",
        "æœ¬è¯„ä¼°ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šå¾‹å¸ˆæ„è§ã€‚",
        "å»ºè®®åœ¨åšå‡ºä»»ä½•å•†ä¸šå†³ç­–å‰ï¼Œå’¨è¯¢å…·æœ‰èµ„è´¨çš„ä¸“ä¸šäººå£«ã€‚"
    ])

    final_report = "\n".join(report_lines)

    # å‘é€å®Œæˆè¿›åº¦
    await send_ws_progress(session_id, "reportGeneration", "completed",
                           "é£é™©è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå®Œæˆ", 1.0)
    logger.info(f"[{session_id}] æŠ¥å‘Šç”Ÿæˆå®Œæˆ")

    return {
        "final_report": final_report,
        "final_result": final_result,
        "status": "completed"
    }


# ==================== æŠ¥å‘Šç”Ÿæˆè¾…åŠ©å‡½æ•° ====================

def _format_risk_level(level: str) -> str:
    """æ ¼å¼åŒ–é£é™©ç­‰çº§"""
    emoji_map = {
        "high": "ğŸ”´ é«˜",
        "medium": "ğŸŸ¡ ä¸­",
        "low": "ğŸŸ¢ ä½"
    }
    return emoji_map.get(level, f"{level}")


def _sort_risks_by_priority(risks: List[Dict]) -> List[Dict]:
    """æŒ‰ä¼˜å…ˆçº§å¯¹é£é™©è¿›è¡Œæ’åº"""

    def priority_score(risk):
        score = 0.0
        # é£é™©ç­‰çº§æƒé‡
        level_weights = {"critical": 100, "high": 80, "medium": 60, "low": 40}
        level = risk.get("risk_level", "low").lower()
        score += level_weights.get(level, 40)

        # ç½®ä¿¡åº¦æƒé‡
        score += risk.get("confidence", 0) * 20

        # å¤šæ¨¡å‹å…±è¯†åŠ åˆ†
        source_count = risk.get("source_models_count", 1)
        if source_count > 1:
            score += 10 * (source_count - 1)

        return score

    return sorted(risks, key=priority_score, reverse=True)


def _format_risk_item(risk: Dict, index: int) -> List[str]:
    """æ ¼å¼åŒ–å•ä¸ªé£é™©é¡¹"""
    level = risk.get("risk_level", "unknown").upper()
    emoji = {
        "CRITICAL": "ğŸ”´",
        "HIGH": "ğŸŸ ",
        "MEDIUM": "ğŸŸ¡",
        "LOW": "ğŸŸ¢"
    }.get(level, "âšª")

    # æ¥æºè§†è§’æ˜ å°„
    source_models = risk.get("source_models", [])
    perspectives_map = {
        "deepseek": "è§„åˆ™æ‰§è¡Œ",
        "gpt_oss": "ç»“æ„åˆ†æ",
        "qwen3_235b": "æ·±åº¦åˆ†æ"
    }
    perspectives = [perspectives_map.get(m, m) for m in source_models if m in perspectives_map]
    source_text = ", ".join(perspectives) if perspectives else "AI åˆ†æ"

    lines = [
        f"#### {emoji} {index}. {risk.get('title', 'æœªå‘½å')}",
        "",
        f"**æè¿°ï¼š** {risk.get('description', 'æ— æè¿°')}",
        f"**é£é™©ç­‰çº§ï¼š** {level}",
        f"**ç½®ä¿¡åº¦ï¼š** {risk.get('confidence', 0):.1%}",
        f"**æ¥æºè§†è§’ï¼š** {source_text}",
        "",
        "**ç†ç”±ï¼š**"
    ]

    for reason in risk.get("reasons", []):
        lines.append(f"- {reason}")

    lines.extend([
        "",
        "**å»ºè®®ï¼š**"
    ])

    for suggestion in risk.get("suggestions", []):
        lines.append(f"- {suggestion}")

    lines.append("")
    return lines


def _generate_comprehensive_suggestions(risks: List[Dict]) -> str:
    """ç”Ÿæˆç»¼åˆå»ºè®®"""
    high_count = sum(1 for r in risks if r.get("risk_level") in ["critical", "high"])
    medium_count = sum(1 for r in risks if r.get("risk_level") == "medium")

    suggestions = []

    # ä¼˜å…ˆå¤„ç†å»ºè®®
    if high_count > 0:
        suggestions.append(f"1. **ä¼˜å…ˆå¤„ç†**ï¼šç«‹å³å¤„ç† {high_count} ä¸ªé«˜é£é™©é¡¹ï¼Œè¿™äº›é£é™©å¯èƒ½å¯¹äº¤æ˜“é€ æˆé‡å¤§å½±å“ã€‚")

    if medium_count > 0:
        suggestions.append(f"2. **åç»­å¤„ç†**ï¼šå»ºè®®åœ¨ç­¾ç½²å‰å¤„ç† {medium_count} ä¸ªä¸­é£é™©é¡¹ï¼Œé™ä½æ½œåœ¨é£é™©ã€‚")

    # ä¸“ä¸šå»ºè®®
    suggestions.append("3. **ä¸“ä¸šå’¨è¯¢**ï¼šå¯¹äºå¤æ‚æ³•å¾‹æ¡æ¬¾ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆè¿›è¡Œç¡®è®¤ã€‚")
    suggestions.append("4. **æŒç»­è·Ÿè¸ª**ï¼šåœ¨åˆåŒå±¥è¡Œè¿‡ç¨‹ä¸­ï¼ŒæŒç»­å…³æ³¨å·²è¯†åˆ«é£é™©çš„æ¼”å˜ã€‚")

    return "\n".join(suggestions)


# ==================== æ„å»ºå·¥ä½œæµ ====================

def should_continue_after_preorganization(state: RiskAnalysisState) -> str:
    """
    å†³å®šé¢„æ•´ç†åæ˜¯å¦ç»§ç»­åˆ†æ
    Returns:
        "continue" æˆ– "stop"
    """
    # æ£€æŸ¥æ˜¯å¦è®¾ç½®äº†åœæ­¢æ ‡å¿—
    if state.get("stop_after_preorganization"):
        logger.info(f"[{state['session_id']}] é¢„æ•´ç†å®Œæˆï¼Œåœæ­¢å·¥ä½œæµç­‰å¾…ç”¨æˆ·ç¡®è®¤")
        return "stop"
    else:
        logger.info(f"[{state['session_id']}] é¢„æ•´ç†å®Œæˆï¼Œç»§ç»­åˆ†ææµç¨‹")
        return "continue"


def build_risk_analysis_graph():
    """æ„å»ºé£é™©è¯„ä¼°å·¥ä½œæµ"""
    # åˆå§‹åŒ–å›¾
    builder = StateGraph(RiskAnalysisState)

    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("process_documents", process_documents_node)
    builder.add_node("preorganize_documents", preorganize_documents_node)
    builder.add_node("assemble_rules", assemble_rules_node)
    builder.add_node("multi_model_analyze", multi_model_analyze_node)
    builder.add_node("generate_report", generate_report_node)

    # å®šä¹‰æµç¨‹
    builder.set_entry_point("process_documents")
    builder.add_edge("process_documents", "preorganize_documents")

    # é¢„æ•´ç†åçš„æ¡ä»¶è·¯ç”±ï¼šæ ¹æ® stop_after_preorganization å†³å®šæ˜¯å¦ç»§ç»­
    builder.add_conditional_edges(
        "preorganize_documents",
        should_continue_after_preorganization,
        {
            "continue": "assemble_rules",  # ç»§ç»­åˆ†æ
            "stop": END  # åœæ­¢ç­‰å¾…ç”¨æˆ·ç¡®è®¤
        }
    )

    builder.add_edge("assemble_rules", "multi_model_analyze")
    builder.add_edge("multi_model_analyze", "generate_report")
    builder.add_edge("generate_report", END)

    # ç¼–è¯‘
    memory = MemorySaver()
    app = builder.compile(checkpointer=memory)

    return app


# ==================== æœåŠ¡ç±» ====================

class RiskAnalysisWorkflowService:
    """é£é™©è¯„ä¼°å·¥ä½œæµæœåŠ¡"""

    def __init__(self, db: Session):
        self.db = db
        self.graph = build_risk_analysis_graph()

    async def run_analysis(
            self,
            session_id: str,
            user_input: str,
            document_paths: List[str],
            package_id: str,
            stop_after_preorganization: bool = False,
            analysis_mode: str = "multi",
            selected_model: Optional[str] = None
    ) -> bool:
        """
        è¿è¡Œé£é™©è¯„ä¼°å·¥ä½œæµï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
        Args:
            session_id: ä¼šè¯ ID
            user_input: ç”¨æˆ·è¾“å…¥
            document_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
            package_id: è§„åˆ™åŒ… ID
            stop_after_preorganization: æ˜¯å¦åœ¨é¢„æ•´ç†ååœæ­¢ï¼ˆç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼‰
            analysis_mode: åˆ†ææ¨¡å¼ï¼Œ"multi"ï¼ˆå¤šæ¨¡å‹ç»¼åˆï¼‰æˆ– "single"ï¼ˆå•æ¨¡å‹ï¼‰
            selected_model: å•æ¨¡å‹æ¨¡å¼ä¸‹æŒ‡å®šçš„æ¨¡å‹åç§°ï¼ˆå¦‚ "deepseek"ï¼‰
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info(
            f"[RiskAnalysisWorkflow] å¼€å§‹è¿è¡Œä¼šè¯: {session_id}, stop_after_preorganization: {stop_after_preorganization}")

        # æ›´æ–°ä¼šè¯çŠ¶æ€
        session = self.db.query(RiskAnalysisSession).filter(
            RiskAnalysisSession.session_id == session_id
        ).first()

        if not session:
            logger.error(f"[RiskAnalysisWorkflow] ä¼šè¯ä¸å­˜åœ¨: {session_id}")
            return False

        try:
            session.status = RiskAnalysisStatus.ANALYZING.value
            self.db.commit()

            # å‡†å¤‡åˆå§‹çŠ¶æ€
            initial_state: RiskAnalysisState = {
                "session_id": session_id,
                "user_input": user_input,
                "document_paths": document_paths,
                "package_id": package_id,  # å¯ä»¥æ˜¯ None
                "stop_after_preorganization": stop_after_preorganization,  # è®¾ç½®åœæ­¢æ ‡å¿—
                "raw_documents": None,
                "preorganized_docs": None,
                "assembled_rules": None,
                "analysis_mode": analysis_mode,  # ä¼ é€’åˆ†ææ¨¡å¼
                "selected_model": selected_model,  # ä¼ é€’é€‰å®šçš„æ¨¡å‹
                "enhanced_analysis_data": None,  # åˆå§‹åŒ–æ–°å­—æ®µ
                "context": None,
                "analysis_results": None,
                "final_report": None,
                "final_result": None,
                "aggregated": None,
                "status": "processing",
                "error": None
            }

            # è¿è¡Œå·¥ä½œæµï¼ˆæ¡ä»¶è·¯ç”±ä¼šè‡ªåŠ¨å¤„ç†åœæ­¢é€»è¾‘ï¼‰
            config = {"configurable": {"thread_id": session_id}}
            result = await self.graph.ainvoke(initial_state, config)

            if stop_after_preorganization:
                # é¢„æ•´ç†å®Œæˆï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤
                logger.info(f"[RiskAnalysisWorkflow] ä¼šè¯ {session_id} é¢„æ•´ç†å®Œæˆï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤")
                return True
            else:
                # å®Œæ•´å·¥ä½œæµè¿è¡Œå®Œæˆï¼Œä¿å­˜ç»“æœ
                final_status = result.get("status", "failed")

                if final_status == "completed":
                    # ğŸšª å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç‹¬ç«‹çš„ã€å¹²å‡€çš„æ•°æ®åº“è¿æ¥ï¼ˆStale Session é—®é¢˜ï¼‰
                    try:
                        with SessionLocal() as db:
                            # é‡æ–°æŸ¥è¯¢ä¼šè¯ï¼ˆè·å–æœ€æ–°çŠ¶æ€å’Œä¸»é”®IDï¼‰
                            session_ref = db.query(RiskAnalysisSession).filter(
                                RiskAnalysisSession.session_id == session_id
                            ).first()

                            if not session_ref:
                                logger.error(f"ğŸ‘‰ [TRACK] ä¸¥é‡é”™è¯¯: ç‹¬ç«‹ä¼šè¯ä¸­æ‰¾ä¸åˆ°ä¼šè¯ {session_id}")
                                raise Exception(f"ä¼šè¯ {session_id} ä¸å­˜åœ¨")

                            logger.error(f"ğŸ‘‰ [TRACK] run_analysis ç‹¬ç«‹ä¼šè¯å·²è·å–ä¼šè¯ï¼Œä¸»é”®ID={session_ref.id}, session_id={session_id}")

                            # æ›´æ–°çŠ¶æ€å­—æ®µ
                            session_ref.status = RiskAnalysisStatus.COMPLETED.value
                            session_ref.report_md = result.get("final_report")

                            # ä» final_result è·å–æ‘˜è¦å’Œä¿¡å¿ƒ
                            final_result_data = result.get("final_result", {})
                            session_ref.summary = final_result_data.get("summary", "")

                            # è®¡ç®—é£é™©åˆ†å¸ƒ
                            risk_items = final_result_data.get("risk_items", [])
                            distribution = {"critical": 0, "high": 0, "medium": 0, "low": 0}
                            for item in risk_items:
                                level = item.get("risk_level", "low").lower()
                                if level in distribution:
                                    distribution[level] += 1
                            session_ref.risk_distribution = distribution
                            session_ref.total_confidence = final_result_data.get("total_confidence", 0.7)
                            session_ref.completed_at = datetime.utcnow()

                            # ä¿å­˜é£é™©é¡¹ï¼ˆåœ¨ç‹¬ç«‹ä¼šè¯ä¸­ï¼‰
                            # æ¸…ç©ºæ—§è®°å½•
                            db.query(RiskItem).filter(
                                RiskItem.session_id == session_ref.id  # ä½¿ç”¨ä¸»é”®ID
                            ).delete()

                            # æ’å…¥æ–°é£é™©é¡¹
                            for item in risk_items:
                                # è·å–æ¥æºæ¨¡å‹ä¿¡æ¯
                                source_models = item.get("source_models", [])
                                source_type = "multi_model"
                                if len(source_models) == 1:
                                    source_type = f"multi_model_{source_models[0]}"

                                risk_item = RiskItem(
                                    session_id=session_ref.id,  # ä½¿ç”¨ä¸»é”®ID
                                    title=item.get("title", "æœªå‘½å"),
                                    description=item.get("description", ""),
                                    risk_level=item.get("risk_level", "medium"),
                                    confidence=item.get("confidence", 0.0),
                                    reasons=item.get("reasons", []),
                                    suggestions=item.get("suggestions", []),
                                    source_type=source_type,
                                    source_rules=None
                                )
                                db.add(risk_item)

                            # âœ… æäº¤ç‹¬ç«‹ä¼šè¯ï¼ˆç»å¯¹å¯é ï¼‰
                            db.commit()
                            logger.error(f"ğŸ‘‰ [TRACK] run_analysis ç‹¬ç«‹ä¼šè¯æäº¤æˆåŠŸ! æ•°æ®å·²è½ç›˜, session_id={session_id}, ä¸»é”®ID={session_ref.id}, é£é™©é¡¹æ•°={len(risk_items)}")

                        # åªæœ‰åœ¨ç¡®ä¿¡æ•°æ®è½ç›˜åï¼Œæ‰é€šçŸ¥å‰ç«¯
                        try:
                            from app.api.websocket import manager
                            await manager.send_progress(session_id, {"type": "complete"})
                            logger.info(f"[WS] å‘é€completeæ¶ˆæ¯æˆåŠŸ: {session_id}")
                        except Exception as e:
                            logger.error(f"[WS] å‘é€completeæ¶ˆæ¯å¤±è´¥: {e}")

                        return True

                    except Exception as e:
                        logger.error(f"ğŸ‘‰ [TRACK] run_analysis ç‹¬ç«‹ä¼šè¯æäº¤å¼‚å¸¸: {e}, session_id={session_id}", exc_info=True)
                        # å›é€€åˆ°é™ˆæ—§ä¼šè¯ï¼ˆæœ€åçš„å°è¯•ï¼‰
                        session.status = RiskAnalysisStatus.FAILED.value
                        session.report_md = f"çŠ¶æ€æ›´æ–°å¤±è´¥: {str(e)}"
                        self.db.commit()
                        return False
                else:
                    session.status = RiskAnalysisStatus.FAILED.value
                    session.report_md = result.get("error", "åˆ†æå¤±è´¥")
                    self.db.commit()
                    logger.error(f"[RiskAnalysisWorkflow] ä¼šè¯ {session_id} å¤±è´¥: {result.get('error')}")
                    return False

        except Exception as e:
            logger.error(f"[RiskAnalysisWorkflow] ä¼šè¯ {session_id} å¼‚å¸¸: {e}", exc_info=True)
            session.status = RiskAnalysisStatus.FAILED.value
            self.db.commit()
            return False

    async def continue_analysis_after_confirmation(
            self,
            session_id: str,
            analysis_mode: str = "multi",
            selected_model: Optional[str] = None,
            evaluation_stance: Optional[str] = None  # âœ… æ–°å¢ï¼šè¯„ä¼°ç«‹åœºå‚æ•°
    ) -> bool:
        """
        åœ¨ç”¨æˆ·ç¡®è®¤é¢„æ•´ç†ç»“æœåç»§ç»­åˆ†æ
        Args:
            session_id: ä¼šè¯ ID
            analysis_mode: åˆ†ææ¨¡å¼ ("single" æˆ– "multi")
            selected_model: å•æ¨¡å‹æ¨¡å¼ä¸‹é€‰æ‹©çš„æ¨¡å‹åç§°
            evaluation_stance: é£é™©è¯„ä¼°ç«‹åœºï¼ˆæŠ•èµ„äººè§†è§’/å–æ–¹è§†è§’ç­‰ï¼‰
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"[RiskAnalysisWorkflow] ç»§ç»­åˆ†æä¼šè¯: {session_id}, mode: {analysis_mode}, stance: {evaluation_stance}")

        # æ›´æ–°ä¼šè¯çŠ¶æ€
        session = self.db.query(RiskAnalysisSession).filter(
            RiskAnalysisSession.session_id == session_id
        ).first()

        if not session:
            logger.error(f"[RiskAnalysisWorkflow] ä¼šè¯ä¸å­˜åœ¨: {session_id}")
            return False

        try:
            # å‡†å¤‡ç»§ç»­åˆ†æ
            # æ–¹æ¡ˆ:ä»æ•°æ®åº“é‡æ–°åŠ è½½é¢„æ•´ç†ç»“æœ,ç„¶åæ‰‹åŠ¨æ‰§è¡Œå‰©ä½™èŠ‚ç‚¹

            # ä»æ•°æ®åº“è·å–é¢„æ•´ç†ç»“æœ
            from app.models.risk_analysis_preorganization import RiskAnalysisPreorganization
            preorg_record = self.db.query(RiskAnalysisPreorganization).filter(
                RiskAnalysisPreorganization.session_id == session_id
            ).first()

            if not preorg_record:
                logger.error(f"[RiskAnalysisWorkflow] æœªæ‰¾åˆ°é¢„æ•´ç†è®°å½•: {session_id}")
                # é™çº§æ–¹æ¡ˆ:å°è¯•ä»checkpointè·å–çŠ¶æ€
                config = {"configurable": {"thread_id": session_id}}
                try:
                    current_snapshot = await self.graph.aget_state(config)
                    if not current_snapshot or not current_snapshot.values:
                        logger.error(f"[RiskAnalysisWorkflow] æ— æ³•è·å–ä¼šè¯ {session_id} çš„çŠ¶æ€")
                        return False
                    continue_state = dict(current_snapshot.values)
                except Exception as checkpoint_error:
                    logger.error(f"[RiskAnalysisWorkflow] checkpoint è·å–å¤±è´¥: {checkpoint_error}")
                    return False
            else:
                # ä»é¢„æ•´ç†è®°å½•é‡å»ºçŠ¶æ€
                logger.info(f"[RiskAnalysisWorkflow] ä»é¢„æ•´ç†è®°å½•é‡å»ºçŠ¶æ€: {session_id}")

                # å°†é¢„æ•´ç†è®°å½•çš„å„ä¸ªå­—æ®µç»„åˆæˆpreorganized_docsæ ¼å¼
                preorganized_docs = {
                    "user_requirement_summary": preorg_record.user_requirement_summary,
                    "documents_info": preorg_record.documents_info,
                    "fact_summary": preorg_record.fact_summary,
                    "contract_legal_features": preorg_record.contract_legal_features,
                    "contract_relationships": preorg_record.contract_relationships,
                    "architecture_diagram": preorg_record.architecture_diagram
                }

                # å°è¯•è§£æ enhanced_analysis_json
                enhanced_data_from_db = None
                if preorg_record.enhanced_analysis_json:
                    try:
                        enhanced_data_from_db = json.loads(preorg_record.enhanced_analysis_json)
                    except:
                        pass

                # ä»æ•°æ®åº“æ¢å¤ raw_documentsï¼ˆä»…æ¢å¤å…ƒæ•°æ®ï¼Œç¡®ä¿ document_count æ­£ç¡®ï¼‰
                raw_documents_from_db = []
                if session.document_processing_results:
                    if isinstance(session.document_processing_results, dict):
                        # æ•°æ®åº“æ ¼å¼ï¼š{filename: {file_path, status, metadata, ...}}
                        # è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œç¡®ä¿ len() è¿”å›çœŸå®æ–‡æ¡£æ•°é‡
                        raw_documents_from_db = list(session.document_processing_results.values())
                    elif isinstance(session.document_processing_results, list):
                        # å…¼å®¹æ—§æ•°æ®æ ¼å¼
                        raw_documents_from_db = session.document_processing_results

                continue_state: RiskAnalysisState = {
                    "session_id": session_id,
                    "user_input": session.user_description or "",
                    "document_paths": session.document_ids or [],
                    "package_id": None,  # ç”¨æˆ·ç¡®è®¤æ—¶æ‰é€‰æ‹©è§„åˆ™åŒ…
                    "stop_after_preorganization": False,  # ç§»é™¤åœæ­¢æ ‡å¿—
                    "raw_documents": raw_documents_from_db,  # æ¢å¤æ–‡æ¡£åˆ—è¡¨ï¼Œç¡®ä¿ len() æ­£ç¡®
                    "preorganized_docs": preorganized_docs,
                    "enhanced_analysis_data": enhanced_data_from_db,  # æ¢å¤å¢å¼ºæ•°æ®
                    "assembled_rules": None,
                    "analysis_mode": preorg_record.analysis_mode,
                    "selected_model": preorg_record.selected_model,
                    "evaluation_stance": evaluation_stance,  # âœ… æ–°å¢ï¼šä¼ é€’è¯„ä¼°ç«‹åœº
                    "context": None,
                    "analysis_results": None,
                    "final_report": None,
                    "final_result": None,
                    "aggregated": None,
                    "status": "processing",
                    "error": None
                }

            # ç›´æ¥ä» assemble_rules èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œï¼ˆè·³è¿‡å·²å®Œæˆçš„èŠ‚ç‚¹ï¼‰
            # æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è°ƒç”¨å‰©ä½™çš„èŠ‚ç‚¹
            logger.info(f"[RiskAnalysisWorkflow] ä¼šè¯ {session_id} ä»è§„åˆ™ç»„è£…èŠ‚ç‚¹ç»§ç»­åˆ†æ")

            # å‘é€åˆå§‹è¿›åº¦ï¼Œé‡ç½®è¿›åº¦æ¡
            await send_ws_progress(session_id, "multiModelAnalysis", "pending", "å‡†å¤‡ç»§ç»­åˆ†æ...", 0.0)

            # 1. è§„åˆ™ç»„è£…
            result1 = await assemble_rules_node(continue_state)
            continue_state.update(result1)
            if continue_state.get("status") == "failed":
                raise Exception(continue_state.get("error", "è§„åˆ™ç»„è£…å¤±è´¥"))

            # 2. å¤šæ¨¡å‹åˆ†æ
            analysis_mode_display = continue_state.get("analysis_mode", "multi")
            mode_text = "å•æ¨¡å‹" if analysis_mode_display == "single" else "å¤šæ¨¡å‹"
            logger.error(f"ğŸ‘‰ [TRACK] å¼€å§‹æ‰§è¡Œ {mode_text}åˆ†æèŠ‚ç‚¹, session_id={session_id}, mode={analysis_mode_display}")
            result2 = await multi_model_analyze_node(continue_state)
            logger.error(f"ğŸ‘‰ [TRACK] {mode_text}åˆ†æèŠ‚ç‚¹è¿”å›, status={result2.get('status')}, session_id={session_id}")
            continue_state.update(result2)
            if continue_state.get("status") == "failed":
                logger.error(f"ğŸ‘‰ [TRACK] {mode_text}åˆ†æå¤±è´¥, error={continue_state.get('error')}, session_id={session_id}")
                raise Exception(continue_state.get("error", f"{mode_text}åˆ†æå¤±è´¥"))

            # 3. æŠ¥å‘Šç”Ÿæˆ
            logger.error(f"ğŸ‘‰ [TRACK] å¼€å§‹æ‰§è¡Œ generate_report_node, session_id={session_id}")
            result3 = await generate_report_node(continue_state)
            logger.error(f"ğŸ‘‰ [TRACK] generate_report_node è¿”å›, status={result3.get('status')}, session_id={session_id}")
            continue_state.update(result3)

            # æœ€ç»ˆç»“æœ
            result = continue_state

            # ä¿å­˜ç»“æœ
            final_status = result.get("status", "failed")
            logger.error(f"ğŸ‘‰ [TRACK] final_status={final_status}, session_id={session_id}")

            if final_status == "completed":
                logger.error(f"ğŸ‘‰ [TRACK] è¿›å…¥ completed åˆ†æ”¯ï¼Œä½¿ç”¨ç‹¬ç«‹ä¼šè¯æ›´æ–°æ•°æ®åº“, session_id={session_id}")

                # ğŸšª å…³é”®ä¿®å¤ï¼šä½¿ç”¨å…¨æ–°çš„ã€å¹²å‡€çš„æ•°æ®åº“è¿æ¥ï¼ˆStale Session é—®é¢˜ï¼‰
                # self.db å¯èƒ½æ˜¯é™ˆæ—§ä¼šè¯ï¼Œcommit çœ‹ä¼¼æˆåŠŸä½†æ•°æ®æœªæŒä¹…åŒ–
                try:
                    with SessionLocal() as db:
                        # é‡æ–°æŸ¥è¯¢ä¼šè¯ï¼ˆè·å–æœ€æ–°çŠ¶æ€å’Œä¸»é”®IDï¼‰
                        session_ref = db.query(RiskAnalysisSession).filter(
                            RiskAnalysisSession.session_id == session_id
                        ).first()

                        if not session_ref:
                            logger.error(f"ğŸ‘‰ [TRACK] ä¸¥é‡é”™è¯¯: ç‹¬ç«‹ä¼šè¯ä¸­æ‰¾ä¸åˆ°ä¼šè¯ {session_id}")
                            raise Exception(f"ä¼šè¯ {session_id} ä¸å­˜åœ¨")

                        logger.error(f"ğŸ‘‰ [TRACK] ç‹¬ç«‹ä¼šè¯å·²è·å–ä¼šè¯ï¼Œä¸»é”®ID={session_ref.id}, session_id={session_id}")

                        # æ›´æ–°çŠ¶æ€å­—æ®µ
                        session_ref.status = RiskAnalysisStatus.COMPLETED.value
                        session_ref.report_md = result.get("final_report")

                        # ä» final_result è·å–æ‘˜è¦å’Œä¿¡å¿ƒ
                        final_result_data = result.get("final_result", {})
                        session_ref.summary = final_result_data.get("summary", "")

                        # è®¡ç®—é£é™©åˆ†å¸ƒ
                        risk_items = final_result_data.get("risk_items", [])
                        distribution = {"critical": 0, "high": 0, "medium": 0, "low": 0}
                        for item in risk_items:
                            level = item.get("risk_level", "low").lower()
                            if level in distribution:
                                distribution[level] += 1
                        session_ref.risk_distribution = distribution
                        session_ref.total_confidence = final_result_data.get("total_confidence", 0.7)
                        session_ref.completed_at = datetime.utcnow()

                        # ä¿å­˜é£é™©é¡¹ï¼ˆåœ¨ç‹¬ç«‹ä¼šè¯ä¸­ï¼‰
                        # æ¸…ç©ºæ—§è®°å½•
                        db.query(RiskItem).filter(
                            RiskItem.session_id == session_ref.id  # ä½¿ç”¨ä¸»é”®ID
                        ).delete()

                        # æ’å…¥æ–°é£é™©é¡¹
                        for item in risk_items:
                            # è·å–æ¥æºæ¨¡å‹ä¿¡æ¯
                            source_models = item.get("source_models", [])
                            source_type = "multi_model"
                            if len(source_models) == 1:
                                source_type = f"multi_model_{source_models[0]}"

                            risk_item = RiskItem(
                                session_id=session_ref.id,  # ä½¿ç”¨ä¸»é”®ID
                                title=item.get("title", "æœªå‘½å"),
                                description=item.get("description", ""),
                                risk_level=item.get("risk_level", "medium"),
                                confidence=item.get("confidence", 0.0),
                                reasons=item.get("reasons", []),
                                suggestions=item.get("suggestions", []),
                                source_type=source_type,
                                source_rules=None
                            )
                            db.add(risk_item)

                        # âœ… æäº¤ç‹¬ç«‹ä¼šè¯ï¼ˆç»å¯¹å¯é ï¼‰
                        db.commit()
                        logger.error(f"ğŸ‘‰ [TRACK] ç‹¬ç«‹ä¼šè¯æäº¤æˆåŠŸ! æ•°æ®å·²è½ç›˜, session_id={session_id}, ä¸»é”®ID={session_ref.id}, é£é™©é¡¹æ•°={len(risk_items)}")

                    # åªæœ‰åœ¨ç¡®ä¿¡æ•°æ®è½ç›˜åï¼Œæ‰é€šçŸ¥å‰ç«¯
                    try:
                        from app.api.websocket import manager
                        await manager.send_progress(session_id, {"type": "complete"})
                        logger.info(f"[WS] å‘é€completeæ¶ˆæ¯æˆåŠŸ: {session_id}")
                    except Exception as e:
                        logger.error(f"[WS] å‘é€completeæ¶ˆæ¯å¤±è´¥: {e}")

                    return True

                except Exception as e:
                    logger.error(f"ğŸ‘‰ [TRACK] ç‹¬ç«‹ä¼šè¯æäº¤å¼‚å¸¸: {e}, session_id={session_id}", exc_info=True)
                    # å›é€€åˆ°é™ˆæ—§ä¼šè¯ï¼ˆæœ€åçš„å°è¯•ï¼‰
                    session.status = RiskAnalysisStatus.FAILED.value
                    session.report_md = f"çŠ¶æ€æ›´æ–°å¤±è´¥: {str(e)}"
                    self.db.commit()
                    return False
            else:
                logger.error(f"ğŸ‘‰ [TRACK] final_status ä¸æ˜¯ completedï¼Œè·³è¿‡æäº¤: final_status={final_status}, session_id={session_id}")
                # ä¿å­˜é”™è¯¯ä¿¡æ¯
                session.status = RiskAnalysisStatus.FAILED.value
                session.report_md = result.get("error", "åˆ†æå¤±è´¥")
                self.db.commit()
                logger.error(f"[RiskAnalysisWorkflow] ä¼šè¯ {session_id} ç»§ç»­åˆ†æå¤±è´¥: status={final_status}")
                return False

        except Exception as e:
            logger.error(f"[RiskAnalysisWorkflow] ä¼šè¯ {session_id} ç»§ç»­åˆ†æå¼‚å¸¸: {e}", exc_info=True)
            session.status = RiskAnalysisStatus.FAILED.value
            self.db.commit()
            return False

    def _save_risk_items(self, session_id: int, final_result: Dict[str, Any]):
        """ä¿å­˜é£é™©é¡¹åˆ°æ•°æ®åº“ï¼ˆé€‚é…æ–°çš„ final_result æ ¼å¼ï¼‰"""
        # æ¸…ç©ºæ—§è®°å½•
        self.db.query(RiskItem).filter(
            RiskItem.session_id == session_id
        ).delete()

        # ä» final_result è·å–é£é™©é¡¹
        risk_items = final_result.get("risk_items", [])

        for item in risk_items:
            # è·å–æ¥æºæ¨¡å‹ä¿¡æ¯
            source_models = item.get("source_models", [])
            source_type = "multi_model"
            if len(source_models) == 1:
                source_type = f"multi_model_{source_models[0]}"

            risk_item = RiskItem(
                session_id=session_id,
                title=item.get("title", "æœªå‘½å"),
                description=item.get("description", ""),
                risk_level=item.get("risk_level", "medium"),
                confidence=item.get("confidence", 0.0),
                reasons=item.get("reasons", []),
                suggestions=item.get("suggestions", []),
                source_type=source_type,
                source_rules=None
            )
            self.db.add(risk_item)

        logger.info(f"[RiskAnalysisWorkflow] ä¿å­˜ {len(risk_items)} ä¸ªé£é™©é¡¹")


# ==================== ä¾¿æ·å‡½æ•° ====================

async def run_risk_analysis_workflow(
        session_id: str,
        user_input: str,
        document_paths: List[str],
        package_id: str,
        analysis_mode: str = "multi",  # <--- æ–°å¢
        selected_model: Optional[str] = None  # <--- æ–°å¢
) -> bool:
    """
    è¿è¡Œé£é™©è¯„ä¼°å·¥ä½œæµçš„ä¾¿æ·å‡½æ•°
    """
    db = SessionLocal()
    try:
        service = RiskAnalysisWorkflowService(db)
        return await service.run_analysis(
            session_id=session_id,
            user_input=user_input,
            document_paths=document_paths,
            package_id=package_id,
            analysis_mode=analysis_mode,  # ä¼ é€’
            selected_model=selected_model  # ä¼ é€’
        )
    finally:
        db.close()