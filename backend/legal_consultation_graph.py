"""
æ³•å¾‹å’¨è¯¢æ¨¡å— - LangGraph å·¥ä½œæµå®ç°ï¼ˆé‡æ„ç‰ˆï¼‰

å®ç°ä¸¤é˜¶æ®µå’¨è¯¢æµç¨‹ï¼š
1. å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹ï¼šä½¿ç”¨ LLM è¿›è¡Œé—®é¢˜åˆ†ç±»å’Œæ„å›¾è¯†åˆ«
2. ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹ï¼šæ ¹æ®åˆ†ç±»ç»“æœæä¾›ä¸“ä¸šæ³•å¾‹å»ºè®®ï¼ˆå†…éƒ¨è‡ªä¸»åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢ï¼‰

æ¶æ„è®¾è®¡ï¼š
    ç”¨æˆ·é—®é¢˜ â†’ å¾‹å¸ˆåŠ©ç†ï¼ˆåˆ†ç±»ï¼‰â†’ ä¸“ä¸šå¾‹å¸ˆï¼ˆå’¨è¯¢ï¼ŒæŒ‰éœ€æ£€ç´¢ï¼‰â†’ ç»“æ„åŒ–è¾“å‡º
"""

from typing import Dict, Any, List, Optional, Tuple, TypedDict, Annotated
from dataclasses import dataclass, field
from enum import Enum
import json
import logging
from operator import add

# LangGraph å’Œ LangChain ç›¸å…³å¯¼å…¥
from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI

logger = logging.getLogger(__name__)


# ==================== è¾…åŠ©å‡½æ•° ====================

async def get_legal_search_results(question: str, domain: str = None) -> dict:
    """è·å–æ³•å¾‹æ£€ç´¢ç»“æœï¼ˆå¼‚æ­¥ï¼‰"""
    try:
        from app.services.legal_search_skill import get_legal_search_skill, format_search_results_for_llm

        skill = get_legal_search_skill()

        # å¹¶è¡Œæ£€ç´¢æ³•è§„å’Œæ¡ˆä¾‹
        import asyncio
        laws_task = skill.search_laws(question, max_results=3)
        cases_task = skill.search_cases(question, max_results=3)

        laws, cases = await asyncio.gather(laws_task, cases_task)

        return {
            "laws": laws,
            "cases": cases,
            "formatted": format_search_results_for_llm({"laws": laws, "cases": cases})
        }
    except Exception as e:
        logger.warning(f"[æ³•å¾‹æ£€ç´¢] æ£€ç´¢å¤±è´¥: {e}")
        return {"laws": [], "cases": [], "formatted": ""}


# ==================== æ³•å¾‹æ£€ç´¢é…ç½® ====================

# éœ€è¦å¼ºåˆ¶æ£€ç´¢çš„æ³•å¾‹é¢†åŸŸï¼ˆç¡®ä¿å¼•ç”¨ç°è¡Œæ³•å¾‹ï¼‰
LEGAL_DOMAINS_REQUIRING_SEARCH = [
    # æ°‘æ³•å…¸ç›¸å…³
    "åˆåŒæ³•", "åˆåŒçº çº·", "æ°‘æ³•å…¸Â·åˆåŒç¼–",
    "ç‰©æƒæ³•", "ç‰©æƒçº çº·", "æ°‘æ³•å…¸Â·ç‰©æƒç¼–",
    "ä¾µæƒè´£ä»»æ³•", "ä¾µæƒçº çº·", "æ°‘æ³•å…¸Â·ä¾µæƒè´£ä»»ç¼–",
    "å©šå§»æ³•", "å©šå§»å®¶åº­", "æ°‘æ³•å…¸Â·å©šå§»å®¶åº­ç¼–",
    "ç»§æ‰¿æ³•", "ç»§æ‰¿çº çº·", "æ°‘æ³•å…¸Â·ç»§æ‰¿ç¼–",

    # å…¶ä»–æ°‘å•†äº‹æ³•å¾‹
    "å…¬å¸æ³•", "å…¬å¸çº çº·",
    "åˆä¼™ä¼ä¸šæ³•", "åˆä¼™çº çº·",
    "ç ´äº§æ³•", "ç ´äº§æ¸…ç®—",
    "åŠ³åŠ¨æ³•", "åŠ³åŠ¨äº‰è®®",
]

# å·²åºŸæ­¢æ³•å¾‹æ˜ å°„ï¼ˆç”¨äºåå¤„ç†æ£€æŸ¥ï¼‰
ABOLISHED_LAWS_MAPPING = {
    "ã€ŠåˆåŒæ³•ã€‹": "ã€Šæ°‘æ³•å…¸ã€‹åˆåŒç¼–",
    "ã€Šä¸­åäººæ°‘å…±å’Œå›½åˆåŒæ³•ã€‹": "ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹",
    "ã€Šç‰©æƒæ³•ã€‹": "ã€Šæ°‘æ³•å…¸ã€‹ç‰©æƒç¼–",
    "ã€Šä¸­åäººæ°‘å…±å’Œå›½ç‰©æƒæ³•ã€‹": "ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹",
    "ã€Šä¾µæƒè´£ä»»æ³•ã€‹": "ã€Šæ°‘æ³•å…¸ã€‹ä¾µæƒè´£ä»»ç¼–",
    "ã€Šä¸­åäººæ°‘å…±å’Œå›½ä¾µæƒè´£ä»»æ³•ã€‹": "ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹",
    "ã€Šå©šå§»æ³•ã€‹": "ã€Šæ°‘æ³•å…¸ã€‹å©šå§»å®¶åº­ç¼–",
    "ã€Šä¸­åäººæ°‘å…±å’Œå›½å©šå§»æ³•ã€‹": "ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹",
    "ã€Šç»§æ‰¿æ³•ã€‹": "ã€Šæ°‘æ³•å…¸ã€‹ç»§æ‰¿ç¼–",
    "ã€Šä¸­åäººæ°‘å…±å’Œå›½ç»§æ‰¿æ³•ã€‹": "ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹",
}

# æ³•å¾‹é¢†åŸŸè§„èŒƒåŒ–æ˜ å°„ï¼ˆå°†ä¸è§„èŒƒæˆ–ä¸å‡†ç¡®çš„åˆ†ç±»æ˜ å°„åˆ°æ ‡å‡†åˆ†ç±»ï¼‰
LEGAL_DOMAIN_NORMALIZATION = {
    # ä¸è§„èŒƒæˆ–è¿‡æ—¶çš„åˆ†ç±»
    "å»ºå·¥æ³•": "å»ºè®¾å·¥ç¨‹",
    "å»ºç­‘æ³•": "å»ºè®¾å·¥ç¨‹",
    "äº¤é€šæ³•": "ä¾µæƒè´£ä»»æ³•",
    "äº¤é€šäº‹æ•…": "ä¾µæƒè´£ä»»æ³•",
    "äº¤é€šè‚‡äº‹": "ä¾µæƒè´£ä»»æ³•",

    # æ›´å…·ä½“çš„åˆ†ç±»ï¼ˆæ˜ å°„åˆ°æ›´é€šç”¨çš„åˆ†ç±»ï¼‰
    "æˆ¿äº§çº çº·": "ç‰©æƒæ³•",
    "æˆ¿å±‹ä¹°å–": "åˆåŒæ³•",
    "æˆ¿å±‹ç§Ÿèµ": "åˆåŒæ³•",
    "ç‰©ä¸šç®¡ç†": "åˆåŒæ³•",
    "å€Ÿè´·çº çº·": "åˆåŒæ³•",
    "å€Ÿæ¬¾çº çº·": "åˆåŒæ³•",
    "æ°‘é—´å€Ÿè´·": "åˆåŒæ³•",
    "å€ºåŠ¡çº çº·": "åˆåŒæ³•",
    "è‚¡æƒçº çº·": "å…¬å¸æ³•",
    "è‚¡ä¸œçº çº·": "å…¬å¸æ³•",
    "æŠ•èµ„çº çº·": "å…¬å¸æ³•",
    "çŸ¥è¯†äº§æƒçº çº·": "çŸ¥è¯†äº§æƒ",
    "ä¸“åˆ©çº çº·": "çŸ¥è¯†äº§æƒ",
    "å•†æ ‡çº çº·": "çŸ¥è¯†äº§æƒ",
    "è‘—ä½œæƒçº çº·": "çŸ¥è¯†äº§æƒ",
    "å·¥ä¼¤èµ”å¿": "åŠ³åŠ¨æ³•",
    "å·¥ä¼¤è®¤å®š": "åŠ³åŠ¨æ³•",
    "ç¤¾ä¼šä¿é™©": "åŠ³åŠ¨æ³•",
    "è¿æ³•è§£é™¤": "åŠ³åŠ¨æ³•",
    "è¿æ³•è¾é€€": "åŠ³åŠ¨æ³•",
    "ç¦»å©šè¯‰è®¼": "å©šå§»å®¶åº­æ³•",
    "æŠšå…»æƒ": "å©šå§»å®¶åº­æ³•",
    "èµ¡å…»è´¹": "å©šå§»å®¶åº­æ³•",
    "æŠšå…»è´¹": "å©šå§»å®¶åº­æ³•",
    "è´¢äº§åˆ†å‰²": "å©šå§»å®¶åº­æ³•",
    "é—å˜±ç»§æ‰¿": "å©šå§»å®¶åº­æ³•",
    "æ³•å®šç»§æ‰¿": "å©šå§»å®¶åº­æ³•",

    # è‹±æ–‡æˆ–ä¸­è‹±æ–‡æ··åˆ
    "Contract Law": "åˆåŒæ³•",
    "Labor Law": "åŠ³åŠ¨æ³•",
    "Company Law": "å…¬å¸æ³•",
    "Criminal Law": "åˆ‘æ³•",

    # å…¶ä»–å¸¸è§ä¸è§„èŒƒè¡¨è¿°
    "æ³•å¾‹å’¨è¯¢": "æ°‘æ³•",
    "æ³•å¾‹é—®é¢˜": "æ°‘æ³•",
    "å…¶ä»–": "æ°‘æ³•",
    "æœªçŸ¥": "æ°‘æ³•",
}


def _normalize_legal_domain(domain: str) -> tuple[str, bool]:
    """
    è§„èŒƒåŒ–æ³•å¾‹é¢†åŸŸåˆ†ç±»

    Args:
        domain: LLM è¾“å‡ºçš„æ³•å¾‹é¢†åŸŸ

    Returns:
        (è§„èŒƒåŒ–åçš„é¢†åŸŸ, æ˜¯å¦è¿›è¡Œäº†ä¿®æ­£)
    """
    if not domain:
        return "æ°‘æ³•", True

    # å»é™¤ç©ºæ ¼å’Œå¼•å·
    domain_clean = domain.strip().strip('"').strip("'")

    # æŸ¥æ‰¾æ˜ å°„
    normalized_domain = LEGAL_DOMAIN_NORMALIZATION.get(domain_clean)

    if normalized_domain:
        logger.info(f"[é¢†åŸŸè§„èŒƒåŒ–] '{domain_clean}' â†’ '{normalized_domain}'")
        return normalized_domain, True

    # å¦‚æœæ²¡æœ‰æ˜ å°„ï¼Œæ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«æ ‡å‡†å…³é”®è¯
    standard_keywords = ["æ°‘æ³•", "åŠ³åŠ¨æ³•", "åˆåŒæ³•", "å…¬å¸æ³•", "ä¾µæƒè´£ä»»æ³•",
                       "å©šå§»å®¶åº­æ³•", "å»ºè®¾å·¥ç¨‹", "åˆ‘æ³•", "è¡Œæ”¿æ³•", "çŸ¥è¯†äº§æƒ",
                       "ç‰©æƒæ³•", "ç ´äº§æ³•", "ç¥¨æ®æ³•", "è¯åˆ¸æ³•", "ä¿é™©æ³•", "æµ·å•†æ³•"]

    for keyword in standard_keywords:
        if keyword in domain_clean:
            # å¦‚æœå·²ç»æ˜¯æ ‡å‡†åˆ†ç±»ï¼Œç›´æ¥è¿”å›
            if domain_clean == keyword or domain_clean.startswith(keyword) and domain_clean not in standard_keywords:
                logger.info(f"[é¢†åŸŸè§„èŒƒåŒ–] '{domain_clean}' å·²æ˜¯æ ‡å‡†åˆ†ç±»")
                return domain_clean, False

    # æœªæ‰¾åˆ°æ˜ å°„ï¼Œè¿”å›åŸå€¼å¹¶è®°å½•
    logger.warning(f"[é¢†åŸŸè§„èŒƒåŒ–] æœªæ‰¾åˆ° '{domain_clean}' çš„æ ‡å‡†æ˜ å°„ï¼Œä¿æŒåŸå€¼")
    return domain_clean, False


def _check_and_fix_legal_references(text: str) -> tuple[str, bool]:
    """
    æ£€æŸ¥å¹¶ä¿®æ­£è¿‡æ—¶çš„æ³•å¾‹å¼•ç”¨

    Args:
        text: LLM è¾“å‡ºçš„æ–‡æœ¬

    Returns:
        (ä¿®æ­£åçš„æ–‡æœ¬, æ˜¯å¦æœ‰ä¿®æ­£)
    """
    modified = False
    modifications = []

    for old_law, new_law in ABOLISHED_LAWS_MAPPING.items():
        if old_law in text:
            text = text.replace(old_law, new_law)
            modified = True
            modifications.append(f"{old_law} â†’ {new_law}")

    if modified:
        logger.warning(f"[æ³•å¾‹å¼•ç”¨ä¿®æ­£] æ£€æµ‹åˆ°è¿‡æ—¶å¼•ç”¨ï¼Œå·²è‡ªåŠ¨ä¿®æ­£: {', '.join(modifications)}")

    return text, modified


def _detect_user_intent(question: str) -> str:
    """
    æ£€æµ‹ç”¨æˆ·åœ¨å¤šè½®å¯¹è¯ä¸­çš„æ„å›¾

    è¿”å›ï¼šconciseï¼ˆç®€æ´ï¼‰, detailedï¼ˆè¯¦ç»†ï¼‰, specificï¼ˆå…·ä½“é—®é¢˜ï¼‰, normalï¼ˆæ­£å¸¸ï¼‰
    """
    question_lower = question.lower()

    # è¦æ±‚ç®€æ´çš„å…³é”®è¯
    concise_keywords = ["ç®€è¦", "ç®€çŸ­", "ç®€å•", "æ¦‚æ‹¬", "æ€»ç»“", "ä¸€å¥è¯", "å¤ªé•¿"]
    if any(kw in question for kw in concise_keywords):
        return "concise"

    # è¦æ±‚è¯¦ç»†çš„å…³é”®è¯
    detailed_keywords = ["è¯¦ç»†", "å±•å¼€", "å…·ä½“è¯´æ˜", "æ·±å…¥", "æ›´å¤š"]
    if any(kw in question for kw in detailed_keywords):
        return "detailed"

    # ç‰¹å®šé—®é¢˜
    specific_keywords = ["æ€ä¹ˆ", "å¦‚ä½•", "ä»€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "æ˜¯å¦", "å¯ä»¥"]
    if any(kw in question for kw in specific_keywords):
        return "specific"

    return "normal"


# ==================== æ•°æ®æ¨¡å‹å®šä¹‰ ====================

class ConsultationType(str, Enum):
    """æ³•å¾‹å’¨è¯¢ç±»å‹æšä¸¾"""
    CONTRACT_LAW = "åˆåŒæ³•"
    LABOR_LAW = "åŠ³åŠ¨æ³•"
    CORPORATE_LAW = "å…¬å¸æ³•"
    CIVIL_LAW = "æ°‘æ³•"
    CRIMINAL_LAW = "åˆ‘æ³•"
    CONSTRUCTION_LAW = "å»ºå·¥æ³•"
    BANKRUPTCY_LAW = "ç ´äº§æ³•"
    TRAFFIC_LAW = "äº¤é€šè‚‡äº‹"
    FAMILY_LAW = "å©šå§»å®¶åº­æ³•"
    INTELLECTUAL_PROPERTY = "çŸ¥è¯†äº§æƒ"
    OTHER = "å…¶ä»–"


class ConsultationState(TypedDict):
    """
    å’¨è¯¢å·¥ä½œæµçš„çŠ¶æ€

    è¿™ä¸ªçŠ¶æ€ä¼šåœ¨å„ä¸ªèŠ‚ç‚¹ä¹‹é—´ä¼ é€’å’Œæ›´æ–°
    """
    # è¾“å…¥
    question: str                          # ç”¨æˆ·çš„é—®é¢˜
    context: Dict[str, Any]                # é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    conversation_history: List[BaseMessage]  # å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰

    # ä¸¤é˜¶æ®µæ‰§è¡Œæ§åˆ¶
    user_confirmed: bool                   # ç”¨æˆ·æ˜¯å¦å·²ç¡®è®¤ï¼ˆç¬¬äºŒé˜¶æ®µæ ‡å¿—ï¼‰
    selected_suggested_questions: Optional[List[str]]  # ç”¨æˆ·é€‰æ‹©çš„å»ºè®®é—®é¢˜ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰

    # å¤šè½®å¯¹è¯æ”¯æŒï¼ˆæ–°å¢ï¼‰
    is_follow_up: bool                      # æ˜¯å¦ä¸ºåç»­é—®é¢˜ï¼ˆå¤šè½®å¯¹è¯ï¼‰
    session_id: Optional[str]               # ä¼šè¯IDï¼ˆç”¨äºæŒä¹…åŒ–ï¼‰
    previous_specialist_output: Optional[Dict[str, Any]]  # ä¸Šä¸€è½®ä¸“ä¸šå¾‹å¸ˆè¾“å‡º

    # å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹è¾“å‡º
    classification_result: Optional[Dict[str, Any]]  # åˆ†ç±»ç»“æœ
    specialist_role: Optional[str]         # ä¸“ä¸šå¾‹å¸ˆè§’è‰²
    confidence: Optional[float]            # åˆ†ç±»ç½®ä¿¡åº¦

    # èµ„æ–™åˆ†æèŠ‚ç‚¹è¾“å‡º (æ–°å¢)
    document_analysis: Optional[Dict[str, Any]]  # æ–‡æ¡£åˆ†æç»“æœ

    # ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹è¾“å‡º
    legal_analysis: Optional[str]          # æ³•å¾‹åˆ†æ
    legal_advice: Optional[str]            # æ³•å¾‹å»ºè®®
    risk_warning: Optional[str]            # é£é™©æé†’
    action_steps: Optional[List[str]]      # è¡ŒåŠ¨æ­¥éª¤
    relevant_laws: Optional[List[str]]     # ç›¸å…³æ³•å¾‹

    # æœ€ç»ˆè¾“å‡º
    final_report: Optional[str]            # æœ€ç»ˆæŠ¥å‘Š
    need_follow_up: bool                   # æ˜¯å¦éœ€è¦åç»­å’¨è¯¢
    follow_up_questions: List[str]         # åç»­é—®é¢˜

    # é”™è¯¯å¤„ç†
    error: Optional[str]                   # é”™è¯¯ä¿¡æ¯
    current_step: str                      # å½“å‰æ­¥éª¤ï¼ˆç”¨äºè°ƒè¯•ï¼‰


@dataclass
class ConsultationOutput:
    """å’¨è¯¢è¾“å‡ºï¼ˆç”¨äº API å“åº”ï¼‰"""
    question: str
    legal_basis: str
    analysis: str
    advice: str
    risk_warning: str
    action_steps: List[str]
    classification_result: Optional[Dict[str, Any]] = None
    need_follow_up: bool = False
    follow_up_questions: List[str] = field(default_factory=list)


# ==================== LLM åˆå§‹åŒ– ====================

def get_consultation_llm() -> ChatOpenAI:
    """
    è·å–ç”¨äºæ³•å¾‹å’¨è¯¢çš„ LLM å®ä¾‹

    æ¨¡å‹é€‰æ‹©ï¼šQwen3-235B-A22B-Thinking-2507

    æ¨¡å‹ç‰¹ç‚¹ï¼š
    - 235B å‚æ•°ï¼Œæ·±åº¦æ€è€ƒèƒ½åŠ›å¼º
    - è¶…æ—¶æ—¶é—´ 120sï¼Œmax_tokens 16000
    - é€‚åˆæ³•å¾‹åˆ†ç±»ã€åˆ†æå’Œæ¨ç†ä»»åŠ¡
    - æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼ˆJSONï¼‰å’Œé•¿æ–‡æœ¬ç”Ÿæˆ

    å¤‡ç”¨æ–¹æ¡ˆï¼š
    1. DeepSeek-R1-0528ï¼ˆæ€§ä»·æ¯”é«˜ï¼‰
    2. é»˜è®¤ OpenAI é…ç½®
    """
    try:
        from app.core.llm_config import get_qwen3_thinking_llm
        llm = get_qwen3_thinking_llm()
        logger.info("[æ³•å¾‹å’¨è¯¢] ä½¿ç”¨ Qwen3-235B-Thinking æ¨¡å‹")
        return llm
    except Exception as e:
        logger.warning(f"Qwen3-Thinking ä¸å¯ç”¨: {e}ï¼Œå°è¯•ä½¿ç”¨ DeepSeek")
        try:
            from app.core.llm_config import get_deepseek_llm
            llm = get_deepseek_llm()
            logger.info("[æ³•å¾‹å’¨è¯¢] ä½¿ç”¨ DeepSeek-R1 æ¨¡å‹")
            return llm
        except Exception as e2:
            logger.warning(f"DeepSeek ä¸å¯ç”¨: {e2}ï¼Œä½¿ç”¨é»˜è®¤ LLM")
            from app.core.llm_config import get_default_llm
            return get_default_llm()


# å…¼å®¹æ—§æ¥å£
def get_assistant_llm() -> ChatOpenAI:
    """è·å–å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹çš„ LLMï¼ˆä¸å’¨è¯¢ä½¿ç”¨åŒä¸€æ¨¡å‹ï¼‰"""
    return get_consultation_llm()


def get_specialist_llm() -> ChatOpenAI:
    """è·å–ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹çš„ LLMï¼ˆä¸å’¨è¯¢ä½¿ç”¨åŒä¸€æ¨¡å‹ï¼‰"""
    return get_consultation_llm()


# ==================== èŠ‚ç‚¹1ï¼šå¾‹å¸ˆåŠ©ç†ï¼ˆé—®é¢˜åˆ†ç±»ï¼‰====================

ASSISTANT_SYSTEM_PROMPT = """ä½ æ˜¯å¾‹å¸ˆäº‹åŠ¡æ‰€å‰å°åŠ©ç†ï¼Œè´Ÿè´£è¯†åˆ«å®¢æˆ·å’¨è¯¢çš„æ³•å¾‹é¢†åŸŸã€‚

**ä»»åŠ¡**ï¼šåˆ†æå®¢æˆ·é—®é¢˜ï¼Œè¯†åˆ«æ³•å¾‹é¢†åŸŸå¹¶æä¾›å¼•å¯¼æ€§é—®é¢˜

**æ³•å¾‹é¢†åŸŸå¿«é€Ÿå‚è€ƒè¡¨**ï¼ˆç”¨äºå‡†ç¡®åˆ†ç±»ï¼‰ï¼š
| ç”¨æˆ·é—®é¢˜å…³é”®è¯ | æ³•å¾‹é¢†åŸŸ |
|--------------|----------|
| äº¤é€šäº‹æ•…ã€äººèº«æŸå®³ã€è´¢äº§æŸå®³ã€åŒ»ç–—çº çº·ã€äº§å“è´£ä»»ã€ç¯å¢ƒæ±¡æŸ“ã€æ‰“æ¶æ–—æ®´ | ä¾µæƒè´£ä»»æ³• |
| å·¥èµ„æ‹–æ¬ ã€è¿æ³•è§£é™¤ã€å·¥ä¼¤èµ”å¿ã€ç¤¾ä¼šä¿é™©ã€åŠ³åŠ¨åˆåŒã€åŠ³åŠ¡æ´¾é£ | åŠ³åŠ¨æ³• |
| è¿çº¦ã€æ¬ æ¬¾ã€å€Ÿæ¬¾ã€ç§Ÿèµã€ä¹°å–ã€æœåŠ¡åˆåŒã€å»ºè®¾å·¥ç¨‹æ¬¾ | åˆåŒæ³• |
| è‚¡æƒã€è‚¡ä¸œã€å…¬å¸æ²»ç†ã€è‚¡æƒè½¬è®©ã€å…¬å¸æ¸…ç®—ã€æ³•äººä»£è¡¨ | å…¬å¸æ³• |
| ç¦»å©šã€æŠšå…»æƒã€èµ¡å…»è´¹ã€è´¢äº§åˆ†å‰²ã€é—å˜±ç»§æ‰¿ã€æ”¶å…»ã€ç™»è®°ç»“å©š | å©šå§»å®¶åº­æ³• |
| å·¥ç¨‹æ¬¾ã€æ–½å·¥ã€è´¨é‡çº çº·ã€å·¥æœŸå»¶è¯¯ã€è¿æ³•åˆ†åŒ…ã€å®é™…æ–½å·¥äºº | å»ºè®¾å·¥ç¨‹ |
| åˆ‘äº‹æ¡ˆä»¶ã€è¾©æŠ¤ã€å–ä¿å€™å®¡ã€å‡åˆ‘å‡é‡Šã€ç¼“åˆ‘ã€ä¸äºˆèµ·è¯‰ | åˆ‘æ³• |
| ç½šæ¬¾ã€æ‹˜ç•™ã€åŠé”€æ‰§ç…§ã€è¡Œæ”¿å¤è®®ã€è¡Œæ”¿è¯‰è®¼ | è¡Œæ”¿æ³• |

**è¾“å‡ºæ ¼å¼**ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š

**ç¤ºä¾‹1ï¼ˆåŠ³åŠ¨æ³•ï¼‰**ï¼š
```json
{
    "primary_type": "åŠ³åŠ¨æ³•",
    "specialist_role": "åŠ³åŠ¨æ³•ä¸“ä¸šå¾‹å¸ˆ",
    "confidence": 0.85,
    "urgency": "high",
    "complexity": "medium",
    "key_entities": ["å…¬å¸åç§°", "å‘˜å·¥"],
    "key_facts": ["å…¬å¸æ‹–æ¬ å·¥èµ„3ä¸ªæœˆ", "å‘˜å·¥è¢«è¿«è¾èŒ"],
    "relevant_laws": ["ã€ŠåŠ³åŠ¨åˆåŒæ³•ã€‹", "ã€Šå·¥èµ„æ”¯ä»˜æš‚è¡Œè§„å®šã€‹"],
    "preliminary_assessment": "ç”¨äººå•ä½æ‹–æ¬ å·¥èµ„ï¼ŒåŠ³åŠ¨è€…å¯ä¾æ³•ç»´æƒ",
    "need_confirmation": true,
    "basic_summary": "å‘˜å·¥å’¨è¯¢å…¬å¸æ‹–æ¬ å·¥èµ„é—®é¢˜ï¼Œæ¶‰åŠåŠ³åŠ¨æ³•é¢†åŸŸ",
    "direct_questions": ["å…¬å¸æ‹–æ¬ å·¥èµ„è¯¥å¦‚ä½•ç»´æƒï¼Ÿ", "åº”è¯¥é€šè¿‡ä»€ä¹ˆé€”å¾„è¿½è®¨å·¥èµ„ï¼Ÿ"],
    "suggested_questions": ["å¦‚ä½•æ”¶é›†æ‹–æ¬ å·¥èµ„çš„è¯æ®ï¼Ÿ", "åŠ³åŠ¨ä»²è£çš„ç”³è¯·æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ", "å¯ä»¥ä¸»å¼ å“ªäº›ç»æµè¡¥å¿é‡‘ï¼Ÿ", "ç”¨äººå•ä½æ‹–æ¬ å·¥èµ„çš„æ³•å¾‹è´£ä»»æœ‰å“ªäº›ï¼Ÿ"],
    "recommended_approach": "è½¬äº¤åŠ³åŠ¨æ³•ä¸“ä¸šå¾‹å¸ˆæ·±åº¦åˆ†æ"
}
```

**ç¤ºä¾‹2ï¼ˆä¾µæƒè´£ä»»æ³•/äº¤é€šäº‹æ•…ï¼‰**ï¼š
```json
{
    "primary_type": "ä¾µæƒè´£ä»»æ³•",
    "specialist_role": "ä¾µæƒè´£ä»»æ³•ä¸“ä¸šå¾‹å¸ˆ",
    "confidence": 0.90,
    "urgency": "high",
    "complexity": "medium",
    "key_entities": ["å—å®³äºº", "è‚‡äº‹æ–¹", "ä¿é™©å…¬å¸"],
    "key_facts": ["å‘ç”Ÿäº¤é€šäº‹æ•…é€ æˆæŸå®³", "éœ€è¦ç¡®å®šèµ”å¿è´£ä»»å’Œé‡‘é¢"],
    "relevant_laws": ["ã€Šé“è·¯äº¤é€šå®‰å…¨æ³•ã€‹", "ã€Šæ°‘æ³•å…¸ã€‹ä¾µæƒè´£ä»»ç¼–"],
    "preliminary_assessment": "äº¤é€šäº‹æ•…å±äºä¾µæƒè´£ä»»çº çº·ï¼Œåº”æ ¹æ®äº‹æ•…è´£ä»»è®¤å®šå’Œç›¸å…³æ³•å¾‹ç¡®å®šèµ”å¿è´£ä»»",
    "need_confirmation": true,
    "basic_summary": "ç”¨æˆ·å’¨è¯¢äº¤é€šäº‹æ•…å¤„ç†é—®é¢˜ï¼Œæ¶‰åŠè´£ä»»è®¤å®šã€èµ”å¿æ ‡å‡†å’Œæ³•å¾‹ç¨‹åº",
    "direct_questions": ["äº¤é€šäº‹æ•…è´£ä»»å¦‚ä½•è®¤å®šï¼Ÿ", "å¯ä»¥ä¸»å¼ å“ªäº›èµ”å¿é¡¹ç›®ï¼Ÿ", "èµ”å¿æ ‡å‡†å¦‚ä½•è®¡ç®—ï¼Ÿ"],
    "suggested_questions": ["äº¤é€šäº‹æ•…è´£ä»»è®¤å®šä¸æœæ€ä¹ˆåŠï¼Ÿ", "å¦‚ä½•ç”³è¯·äº¤é€šäº‹æ•…ä¼¤æ®‹é‰´å®šï¼Ÿ", "äº¤é€šäº‹æ•…èµ”å¿çš„è¯‰è®¼æ—¶æ•ˆæ˜¯å¤šä¹…ï¼Ÿ", "å¯¹æ–¹å…¨è´£ä½†æ— ä¿é™©è¯¥å¦‚ä½•ç´¢èµ”ï¼Ÿ"],
    "recommended_approach": "è½¬äº¤ä¾µæƒè´£ä»»æ³•ä¸“ä¸šå¾‹å¸ˆæ·±åº¦åˆ†æ"
}
```

**å­—æ®µè¯´æ˜**ï¼š
- primary_type: æ³•å¾‹é¢†åŸŸï¼ˆå¿…é¡»ä»ä¸Šè¿°å‚è€ƒè¡¨ä¸­é€‰æ‹©ï¼šä¾µæƒè´£ä»»æ³•ã€åŠ³åŠ¨æ³•ã€åˆåŒæ³•ã€å…¬å¸æ³•ã€å©šå§»å®¶åº­æ³•ã€å»ºè®¾å·¥ç¨‹ã€åˆ‘æ³•ã€è¡Œæ”¿æ³•ã€æ°‘æ³•ï¼‰
- specialist_role: ä¸“ä¸šå¾‹å¸ˆè§’è‰²
- confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
- urgency: ç´§æ€¥ç¨‹åº¦ï¼ˆhigh/medium/lowï¼‰
- complexity: å¤æ‚ç¨‹åº¦ï¼ˆsimple/medium/complexï¼‰
- key_entities: å…³é”®å½“äº‹äººæˆ–æœºæ„
- key_facts: å…³é”®äº‹å®ï¼ˆ3-5æ¡ï¼‰
- relevant_laws: ç›¸å…³æ³•å¾‹ï¼ˆ1-3ä¸ªï¼‰
- preliminary_assessment: åˆæ­¥è¯„ä¼°ï¼ˆ1-2å¥è¯ï¼‰
- need_confirmation: æ˜¯å¦éœ€è¦ç¡®è®¤ï¼ˆtrueï¼‰
- basic_summary: æ¡ˆä»¶æ€»ç»“ï¼ˆ2-3å¥è¯ï¼‰
- direct_questions: ä»ç”¨æˆ·è¾“å…¥æç‚¼çš„æ ¸å¿ƒé—®é¢˜ï¼ˆ1-3ä¸ªï¼‰
- suggested_questions: æ¨æµ‹ç”¨æˆ·å¯èƒ½å…³å¿ƒçš„é—®é¢˜ï¼ˆ2-5ä¸ªï¼‰

**suggested_questions ç”Ÿæˆè§„åˆ™**ï¼ˆé¢å‘ä¸“ä¸šæ³•å¾‹å’¨è¯¢ï¼‰ï¼š
- é—®æ³•å¾‹ç¨‹åºé—®é¢˜ï¼ˆå¦‚ä½•ç”³è¯·ä»²è£ï¼Ÿè¯‰è®¼æ—¶æ•ˆå¤šä¹…ï¼Ÿé‰´å®šç¨‹åºå¦‚ä½•è¿›è¡Œï¼Ÿï¼‰
- é—®æƒåˆ©æ•‘æµé—®é¢˜ï¼ˆä¸æœè®¤å®šæ€ä¹ˆåŠï¼Ÿå¦‚ä½•ç”³è¯·å¼‚è®®ï¼Ÿæœ‰å“ªäº›æ•‘æµé€”å¾„ï¼Ÿï¼‰
- é—®æ³•å¾‹åæœé—®é¢˜ï¼ˆå¯¹æ–¹åº”æ‰¿æ‹…ä»€ä¹ˆè´£ä»»ï¼Ÿå¯ä»¥ä¸»å¼ å“ªäº›èµ”å¿ï¼Ÿæ ‡å‡†å¦‚ä½•è®¡ç®—ï¼Ÿï¼‰
- é—®è¯æ®æ”¶é›†é—®é¢˜ï¼ˆå¦‚ä½•æ”¶é›†å’Œä¿å…¨è¯æ®ï¼Ÿå“ªäº›è¯æ®æœ‰æ•ˆï¼Ÿï¼‰
- ç¦æ­¢äº‹é¡¹ï¼šä¸è¦é—®"æ˜¯å¦..."ã€"æœ‰æ²¡æœ‰..."ã€"æ˜¯å¦å·²..."ç­‰å‘ç”¨æˆ·æ ¸å®æƒ…å†µçš„é—®é¢˜

**é‡è¦æé†’**ï¼š
1. é‡åˆ°"äº¤é€šäº‹æ•…"ã€"äººèº«æŸå®³"ã€"åŒ»ç–—çº çº·"ç­‰é—®é¢˜ï¼Œå¿…é¡»åˆ†ç±»ä¸º"ä¾µæƒè´£ä»»æ³•"
2. é‡åˆ°"å·¥èµ„"ã€"ç¦»èŒ"ã€"åŠ³åŠ¨åˆåŒ"ç­‰é—®é¢˜ï¼Œå¿…é¡»åˆ†ç±»ä¸º"åŠ³åŠ¨æ³•"
3. é‡åˆ°"æ¬ æ¬¾"ã€"è¿çº¦"ã€"å€Ÿæ¬¾"ã€"ç§Ÿèµ"ç­‰é—®é¢˜ï¼Œå¿…é¡»åˆ†ç±»ä¸º"åˆåŒæ³•"
4. åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šæ–‡å­—ï¼
"""


async def assistant_node(state: ConsultationState) -> ConsultationState:
    """
    å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹ï¼šå¯¹ç”¨æˆ·é—®é¢˜è¿›è¡Œåˆ†ç±»å’Œåˆæ­¥åˆ†æï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰

    å¤šè½®å¯¹è¯é€»è¾‘ï¼š
    - å¦‚æœ is_follow_up=Trueï¼Œè·³è¿‡åˆ†ç±»ï¼Œç›´æ¥è·¯ç”±åˆ°ä¸“ä¸šå¾‹å¸ˆ
    - å¦‚æœ is_follow_up=Falseï¼Œè¿›è¡Œå®Œæ•´çš„é—®é¢˜åˆ†ç±»
    - ã€æ–°å¢ã€‘å¦‚æœå·²æœ‰ classification_resultï¼ˆç”¨æˆ·ç¡®è®¤é˜¶æ®µï¼‰ï¼Œè·³è¿‡åˆ†ç±»ï¼Œç›´æ¥ä½¿ç”¨

    è¾“å…¥ï¼šstate["question"]
    è¾“å‡ºï¼šæ›´æ–° state["classification_result"], state["specialist_role"], state["confidence"]
    """
    logger.info("[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] å¼€å§‹åˆ†æç”¨æˆ·é—®é¢˜...")

    question = state["question"]
    context = state.get("context", {})

    # æ£€æŸ¥æ˜¯å¦ä¸ºåç»­é—®é¢˜ï¼ˆå¤šè½®å¯¹è¯ï¼‰
    is_follow_up = state.get("is_follow_up", False)

    # ã€æ–°å¢ã€‘æ£€æŸ¥æ˜¯å¦å·²æœ‰åˆ†ç±»ç»“æœï¼ˆç”¨æˆ·ç¡®è®¤é˜¶æ®µï¼‰
    existing_classification = state.get("classification_result")

    if existing_classification:
        # ç”¨æˆ·ç¡®è®¤é˜¶æ®µï¼šç›´æ¥ä½¿ç”¨æ¢å¤çš„åˆ†ç±»ç»“æœ
        logger.info("[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] ç”¨æˆ·ç¡®è®¤é˜¶æ®µï¼šä½¿ç”¨æ¢å¤çš„åˆ†ç±»ç»“æœï¼Œè·³è¿‡ LLM é‡æ–°åˆ†ç±»")
        logger.info(f"[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹]   direct_questions: {existing_classification.get('direct_questions')}")
        logger.info(f"[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹]   suggested_questions: {existing_classification.get('suggested_questions')}")

        # è®¾ç½®çŠ¶æ€
        state["specialist_role"] = existing_classification.get("specialist_role", "ä¸“ä¸šå¾‹å¸ˆ")
        state["confidence"] = existing_classification.get("confidence", 0.8)
        state["current_step"] = "assistant_node_completed"
        state["user_confirmed"] = True  # ç¡®ä¿ç»§ç»­åˆ°ä¸“ä¸šå¾‹å¸ˆ
        state["relevant_laws"] = existing_classification.get("relevant_laws", [])

        return state

    if is_follow_up:
        # å¤šè½®å¯¹è¯æ¨¡å¼ï¼šè·³è¿‡åˆ†ç±»ï¼Œç›´æ¥ä½¿ç”¨ä¸Šä¸€è½®çš„åˆ†ç±»ç»“æœ
        logger.info("[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] å¤šè½®å¯¹è¯æ¨¡å¼ï¼Œè·³è¿‡åˆ†ç±»ï¼Œç›´æ¥è¿›å…¥ä¸“ä¸šå¾‹å¸ˆ")

        # ä» previous_specialist_output ä¸­æå–æˆ–ä½¿ç”¨é»˜è®¤åˆ†ç±»
        previous_output = state.get("previous_specialist_output", {})

        # è®¾ç½®é»˜è®¤åˆ†ç±»ç»“æœï¼ˆåŸºäºä¸Šä¸€è½®çš„ä¸Šä¸‹æ–‡ï¼‰
        state["classification_result"] = {
            "primary_type": "åç»­å’¨è¯¢",
            "specialist_role": "ä¸“ä¸šå¾‹å¸ˆ",
            "confidence": 0.9,
            "urgency": "medium",
            "complexity": "medium",
            "key_entities": [],
            "key_facts": [],
            "relevant_laws": [],
            "preliminary_assessment": "åç»­å’¨è¯¢é—®é¢˜",
            "need_confirmation": False,  # ä¸éœ€è¦ç¡®è®¤
            "basic_summary": "ç”¨æˆ·åŸºäºä¹‹å‰çš„å’¨è¯¢æå‡ºåç»­é—®é¢˜",
            "direct_questions": [question],  # ç›´æ¥ä½¿ç”¨ç”¨æˆ·çš„é—®é¢˜
            "suggested_questions": [],
            "recommended_approach": "ç”±ä¸“ä¸šå¾‹å¸ˆç»§ç»­è§£ç­”"
        }
        state["specialist_role"] = "ä¸“ä¸šå¾‹å¸ˆ"
        state["confidence"] = 0.9
        state["current_step"] = "assistant_node_completed"
        state["user_confirmed"] = True  # è‡ªåŠ¨ç¡®è®¤ï¼Œç»§ç»­åˆ°ä¸“ä¸šå¾‹å¸ˆ
        state["relevant_laws"] = []

        return state

    # æ–°é—®é¢˜æ¨¡å¼ï¼šè¿›è¡Œå®Œæ•´çš„é—®é¢˜åˆ†ç±»
    logger.info("[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] æ–°é—®é¢˜æ¨¡å¼ï¼Œè¿›è¡Œå®Œæ•´åˆ†ç±»")

    # è·å–å¾‹å¸ˆåŠ©ç†ä¸“ç”¨ LLM
    llm = get_assistant_llm()

    # æ„å»ºæ¶ˆæ¯
    # äººç±»æ¶ˆæ¯ï¼šå®¢æˆ·å’¨è¯¢é—®é¢˜
    human_content = f"å®¢æˆ·å’¨è¯¢é—®é¢˜ï¼š\n\n{question}"

    # ã€æ–°å¢ã€‘è¿½åŠ æ–‡ä»¶é¢„è¯»å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    file_preview = context.get("file_preview_text")
    if file_preview and file_preview.strip():
        human_content += f"""

---
**ğŸ“ å®¢æˆ·ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹**ï¼š
{file_preview}
---
"""
        logger.info(f"[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] æ–‡ä»¶é¢„è¯»å†…å®¹é•¿åº¦ï¼š{len(file_preview)} å­—ç¬¦")

    messages = [
        SystemMessage(content=ASSISTANT_SYSTEM_PROMPT),
        HumanMessage(content=f"{human_content}\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚")
    ]

    # å¦‚æœæœ‰ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
    if context.get("has_file_content"):
        messages.append(SystemMessage(content="æ³¨æ„ï¼šå®¢æˆ·å·²ä¸Šä¼ ç›¸å…³æ–‡ä»¶ï¼Œè¯·ä»”ç»†åˆ†ææ–‡ä»¶å†…å®¹ã€‚"))

    try:
        # è°ƒç”¨ LLM
        response: AIMessage = await llm.ainvoke(messages)
        response_text = response.content

        logger.info(f"[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] LLM å®Œæ•´å“åº”ï¼š{response_text}")
        logger.info(f"[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] LLM å“åº”é•¿åº¦ï¼š{len(response_text)} å­—ç¬¦")

        # è§£æ JSON å“åº”
        classification = parse_classification_response(response_text)

        logger.info(f"[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] è§£æåçš„åˆ†ç±»ç»“æœï¼š{classification}")
        logger.info(f"[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] direct_questions æ•°é‡ï¼š{len(classification.get('direct_questions', []))}")
        logger.info(f"[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] suggested_questions æ•°é‡ï¼š{len(classification.get('suggested_questions', []))}")

        # æ›´æ–°çŠ¶æ€
        state["classification_result"] = classification
        state["specialist_role"] = classification.get("specialist_role", "ä¸“ä¸šå¾‹å¸ˆ")
        state["confidence"] = classification.get("confidence", 0.8)
        state["current_step"] = "assistant_node_completed"
        state["relevant_laws"] = classification.get("relevant_laws", [])

        logger.info(f"[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] åˆ†ç±»å®Œæˆï¼š{classification.get('primary_type')} - {classification.get('specialist_role')}")

    except Exception as e:
        logger.error(f"[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] å¤„ç†å¤±è´¥ï¼š{str(e)}")
        logger.error(f"[å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹] å¼‚å¸¸å †æ ˆ: {e.__traceback__}")
        state["error"] = f"é—®é¢˜åˆ†ç±»å¤±è´¥ï¼š{str(e)}"
        # è®¾ç½®é»˜è®¤å€¼ï¼Œç¡®ä¿æµç¨‹å¯ä»¥ç»§ç»­
        state["classification_result"] = {
            "primary_type": "æ³•å¾‹å’¨è¯¢",
            "specialist_role": "ä¸“ä¸šå¾‹å¸ˆ",
            "confidence": 0.5,
            "urgency": "medium",
            "complexity": "medium",
            "key_entities": [],
            "key_facts": [],
            "relevant_laws": ["ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹"],
            "preliminary_assessment": "éœ€è¦è¿›ä¸€æ­¥åˆ†æ",
            "need_confirmation": True,
            "basic_summary": "ç³»ç»Ÿå¤„ç†å¼‚å¸¸ï¼Œå¾…è¿›ä¸€æ­¥åˆ†æ",
            "direct_questions": [],  # é»˜è®¤ä¸ºç©ºæ•°ç»„
            "suggested_questions": [],  # é»˜è®¤ä¸ºç©ºæ•°ç»„
            "recommended_approach": "è½¬äº¤ä¸“ä¸šå¾‹å¸ˆåˆ†æ"
        }
        state["specialist_role"] = "ä¸“ä¸šå¾‹å¸ˆ"
        state["confidence"] = 0.5

    return state


def parse_classification_response(response_text: str) -> Dict[str, Any]:
    """
    è§£æå¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹çš„ JSON å“åº”

    Args:
        response_text: LLM è¿”å›çš„æ–‡æœ¬

    Returns:
        è§£æåçš„åˆ†ç±»ç»“æœå­—å…¸
    """
    # å°è¯•æå– JSON
    import re

    # æŸ¥æ‰¾ JSON ä»£ç å—
    json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
    if json_match:
        json_str = json_match.group(1)
    else:
        # å°è¯•ç›´æ¥æŸ¥æ‰¾ JSON å¯¹è±¡
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
        else:
            # æ— æ³•æ‰¾åˆ° JSONï¼Œè¿”å›é»˜è®¤å€¼
            logger.warning("[parse_classification] æœªæ‰¾åˆ° JSONï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»")
            logger.warning(f"[parse_classification] LLM å“åº”å†…å®¹: {response_text[:500]}...")
            return {
                "primary_type": "æ³•å¾‹å’¨è¯¢",
                "specialist_role": "ä¸“ä¸šå¾‹å¸ˆ",
                "confidence": 0.6,
                "urgency": "medium",
                "complexity": "medium",
                "key_entities": [],
                "key_facts": [],
                "relevant_laws": ["ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹"],
                "preliminary_assessment": "éœ€è¦è¿›ä¸€æ­¥åˆ†æ",
                "need_confirmation": True,
                "basic_summary": "å¾…åˆ†æ",
                "direct_questions": [],  # é»˜è®¤ä¸ºç©ºæ•°ç»„
                "suggested_questions": [],  # é»˜è®¤ä¸ºç©ºæ•°ç»„
                "recommended_approach": "è½¬äº¤ä¸“ä¸šå¾‹å¸ˆåˆ†æ"
            }

    try:
        result = json.loads(json_str)

        # ç¡®ä¿åŒ…å«å¿…éœ€çš„å­—æ®µ
        if "direct_questions" not in result:
            result["direct_questions"] = []
        if "suggested_questions" not in result:
            result["suggested_questions"] = []

        # ã€æ–°å¢ã€‘è§„èŒƒåŒ–æ³•å¾‹é¢†åŸŸåˆ†ç±»
        if "primary_type" in result:
            original_type = result["primary_type"]
            normalized_type, was_normalized = _normalize_legal_domain(original_type)
            result["primary_type"] = normalized_type

            # å¦‚æœ specialist_role ä¹ŸåŒ…å«äº†åŸåˆ†ç±»ï¼Œä¸€å¹¶æ›´æ–°
            if was_normalized and "specialist_role" in result:
                result["specialist_role"] = result["specialist_role"].replace(original_type, normalized_type)

        return result
    except json.JSONDecodeError as e:
        logger.error(f"[parse_classification] JSON è§£æå¤±è´¥ï¼š{e}")
        logger.error(f"[parse_classification] å°è¯•è§£æçš„ JSON å­—ç¬¦ä¸²: {json_str[:500]}...")
        return {
            "primary_type": "æ°‘æ³•",
            "specialist_role": "ä¸“ä¸šå¾‹å¸ˆ",
            "confidence": 0.6,
            "urgency": "medium",
            "complexity": "medium",
            "key_entities": [],
            "key_facts": [],
            "relevant_laws": ["ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹"],
            "preliminary_assessment": "éœ€è¦è¿›ä¸€æ­¥åˆ†æ",
            "need_confirmation": True,
            "basic_summary": "å¾…åˆ†æ",
            "direct_questions": [],  # é»˜è®¤ä¸ºç©ºæ•°ç»„
            "suggested_questions": [],  # é»˜è®¤ä¸ºç©ºæ•°ç»„
            "recommended_approach": "è½¬äº¤ä¸“ä¸šå¾‹å¸ˆåˆ†æ"
        }


# ==================== èŠ‚ç‚¹2ï¼šèµ„æ–™åˆ†æï¼ˆæ–‡æ¡£æ·±åº¦åˆ†æï¼‰====================

async def document_analysis_node(state: ConsultationState) -> ConsultationState:
    """
    èµ„æ–™åˆ†æèŠ‚ç‚¹ï¼šæ·±åº¦åˆ†æä¸Šä¼ çš„æ–‡ä»¶å†…å®¹

    ä½¿ç”¨é€šç”¨æ–‡æ¡£é¢„æ•´ç†æœåŠ¡ + å’¨è¯¢ç‰¹å®šåŠŸèƒ½
    """
    from app.services.consultation.document_analysis import get_consultation_document_analysis_service
    from app.services.unified_document_service import StructuredDocumentResult

    logger.info("[èµ„æ–™åˆ†æèŠ‚ç‚¹] å¼€å§‹åˆ†ææ–‡æ¡£...")

    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶å†…å®¹
    context = state.get("context", {})
    uploaded_file_ids = context.get("uploaded_files", [])
    logger.info(f"[èµ„æ–™åˆ†æèŠ‚ç‚¹] uploaded_file_idsç±»å‹: {type(uploaded_file_ids)}, å†…å®¹: {uploaded_file_ids}")

    # å¦‚æœæ˜¯åˆ—è¡¨(æ–‡ä»¶IDåˆ—è¡¨),éœ€è¦ä»å…¨å±€å­˜å‚¨ä¸­è·å–æ–‡ä»¶ä¿¡æ¯
    # å¦‚æœæ˜¯å­—å…¸(æ–‡ä»¶ä¿¡æ¯å­—å…¸),ç›´æ¥ä½¿ç”¨
    if isinstance(uploaded_file_ids, list):
        # ä»å…¨å±€ uploaded_files å­˜å‚¨ä¸­è·å–æ–‡ä»¶ä¿¡æ¯
        from app.api.consultation_router import uploaded_files as global_uploaded_files
        uploaded_files_dict = {}
        for file_id in uploaded_file_ids:
            logger.info(f"[èµ„æ–™åˆ†æèŠ‚ç‚¹] å¤„ç†æ–‡ä»¶ID: {file_id}, ç±»å‹: {type(file_id)}")
            # ç¡®ä¿file_idæ˜¯å­—ç¬¦ä¸²
            if isinstance(file_id, dict):
                # å¦‚æœæ˜¯å­—å…¸,ç›´æ¥ä½¿ç”¨
                uploaded_files_dict[file_id.get("file_id", str(hash(str(file_id))))] = file_id
            elif isinstance(file_id, str) and file_id in global_uploaded_files:
                uploaded_files_dict[file_id] = global_uploaded_files[file_id]
        uploaded_files = uploaded_files_dict
        logger.info(f"[èµ„æ–™åˆ†æèŠ‚ç‚¹] ä»å…¨å±€å­˜å‚¨åŠ è½½äº† {len(uploaded_files)} ä¸ªæ–‡ä»¶")
    else:
        # å·²ç»æ˜¯å­—å…¸æ ¼å¼
        uploaded_files = uploaded_file_ids

    if not uploaded_files:
        # æ— æ–‡ä»¶,è·³è¿‡åˆ†æ
        logger.info("[èµ„æ–™åˆ†æèŠ‚ç‚¹] æ— æ–‡ä»¶ï¼Œè·³è¿‡åˆ†æ")
        state["document_analysis"] = None
        state["current_step"] = "document_analysis_skipped"
        return state

    try:
        # æ„å»º StructuredDocumentResult åˆ—è¡¨
        documents = []
        for file_id, file_info in uploaded_files.items():
            if file_info.get("content"):
                doc = StructuredDocumentResult(
                    status="success",
                    content=file_info["content"],
                    metadata=file_info.get("metadata", {}),
                    processing_method=file_info.get("processing_method"),
                    warnings=file_info.get("warnings", [])
                )
                documents.append(doc)

        logger.info(f"[èµ„æ–™åˆ†æèŠ‚ç‚¹] å¼€å§‹åˆ†æ {len(documents)} ä¸ªæ–‡æ¡£")

        # è°ƒç”¨å’¨è¯¢æ–‡æ¡£åˆ†ææœåŠ¡
        llm = get_consultation_llm()
        analysis_service = get_consultation_document_analysis_service(llm)

        # æ‰§è¡Œåˆ†æ
        classification_result = state.get("classification_result", {})
        analysis_result = await analysis_service.analyze_for_consultation(
            documents=documents,
            user_question=state["question"],
            classification=classification_result
        )

        # æ›´æ–°çŠ¶æ€
        state["document_analysis"] = analysis_result
        state["current_step"] = "document_analysis_completed"

        logger.info(f"[èµ„æ–™åˆ†æèŠ‚ç‚¹] åˆ†æå®Œæˆ: {len(analysis_result.get('document_summaries', {}))} ä¸ªæ‘˜è¦, "
                   f"{len(analysis_result.get('legal_issues', []))} ä¸ªæ³•å¾‹é—®é¢˜, "
                   f"{len(analysis_result.get('dispute_points', []))} ä¸ªäº‰è®®ç„¦ç‚¹")

    except Exception as e:
        logger.error(f"[èµ„æ–™åˆ†æèŠ‚ç‚¹] åˆ†æå¤±è´¥: {e}", exc_info=True)
        state["document_analysis"] = None
        state["error"] = f"æ–‡æ¡£åˆ†æå¤±è´¥ï¼š{str(e)}"
        state["current_step"] = "document_analysis_failed"

    return state


def should_analyze_documents(state: ConsultationState) -> str:
    """
    å†³å®šæ˜¯å¦éœ€è¦èµ„æ–™åˆ†æ

    æ¡ä»¶è·¯ç”±ï¼šå¦‚æœæœ‰æ–‡ä»¶åˆ™åˆ†æï¼Œå¦åˆ™è·³è¿‡
    """
    context = state.get("context", {})
    uploaded_files = context.get("uploaded_files", [])

    if uploaded_files:
        return "analyze"
    return "skip"


# ==================== èŠ‚ç‚¹3ï¼šä¸“ä¸šå¾‹å¸ˆï¼ˆæ³•å¾‹å’¨è¯¢ï¼‰====================

SPECIALIST_SYSTEM_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½{specialist_role}ï¼Œæ‹¥æœ‰15å¹´æ‰§ä¸šç»éªŒçš„èµ„æ·±å¾‹å¸ˆã€‚

ã€ä¸“ä¸šèƒŒæ™¯ã€‘
- 15å¹´æ‰§ä¸šç»éªŒï¼Œå¤„ç†è¿‡500+æ³•å¾‹æ¡ˆä»¶
- ä¸“æ³¨é¢†åŸŸï¼š{legal_domain}
- å…·å¤‡å¾‹å¸ˆèµ„æ ¼è¯å’Œæ³•å­¦ç¡•å£«å­¦ä½

ã€æ ¸å¿ƒå·¥ä½œåŸåˆ™ã€‘
1. **ç®€æ´æ˜ç¡®**ï¼šç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦ç»•åœˆå­
2. **åŸºäºäº‹å®å’Œæ³•å¾‹**ï¼šæ¯ä¸ªç»“è®ºéƒ½è¦æœ‰æ³•å¾‹ä¾æ®æˆ–äº‹å®æ”¯æ’‘
3. **å¯æ“ä½œå»ºè®®**ï¼šæä¾›å…·ä½“ã€å¯æ‰§è¡Œçš„å»ºè®®
4. **é£é™©æç¤º**ï¼šæ˜ç¡®å‘ŠçŸ¥æ½œåœ¨æ³•å¾‹é£é™©
5. **é€ä¸€å›ç­”åŸåˆ™**ï¼šå¿…é¡»é’ˆå¯¹ç”¨æˆ·æå‡ºçš„æ¯ä¸ªé—®é¢˜é€ä¸€ç»™å‡ºæ˜ç¡®ã€å…·ä½“çš„å›ç­”

ã€è¾“å‡ºè¦æ±‚ã€‘
- ä½¿ç”¨æ¸…æ™°ç®€æ´çš„è¯­è¨€ï¼Œé¿å…æ³•è¨€æ³•è¯­å †ç Œ
- ä¸¥ç¦ç®€å•å¤è¿°æ–‡æ¡£å†…å®¹æˆ–ç”¨æˆ·é—®é¢˜
- å¿…é¡»ä½¿ç”¨ Markdown æ ¼å¼ï¼Œæ‰€æœ‰æ ‡é¢˜ä½¿ç”¨ ## ### æ ‡è®°

**è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„æä¾›ä¸“ä¸šæ³•å¾‹æ„è§**ï¼š

---

## ä¸€ã€æ–‡ä»¶æƒ…å†µ

<<<FILE_DESCRIPTION_PLACEHOLDER>>>

---

## äºŒã€é—®é¢˜è§£ç­”

**ã€å¿…é¡»é€ä¸€å›ç­”ç”¨æˆ·æå‡ºçš„æ‰€æœ‰é—®é¢˜ã€‘**

<<<USER_QUESTIONS_PLACEHOLDER>>>

---

## ä¸‰ã€ç®€è¦åˆ†æ

**åŸºäºäº‹å®æƒ…å†µçš„æ³•å¾‹åˆ†æï¼ˆ2-3æ®µï¼‰**ï¼š

1. **æ ¸å¿ƒæ³•å¾‹å…³ç³»**ï¼šè¯†åˆ«æ¡ˆä»¶çš„æ ¸å¿ƒæ³•å¾‹å…³ç³»å’Œäº‰è®®ç„¦ç‚¹
2. **æ³•å¾‹ä¾æ®**ï¼šå¼•ç”¨ç›¸å…³æ³•å¾‹æ¡æ–‡ï¼ˆæ³¨æ˜æ³•æ¡ç¼–å·å’Œå†…å®¹ï¼‰
3. **äº‹å®ä¸æ³•å¾‹ç»“åˆ**ï¼šå°†å…·ä½“äº‹å®ä¸æ³•å¾‹è§„å®šç»“åˆåˆ†æ

---

## å››ã€ä¸“ä¸šå»ºè®®

**å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®ï¼ˆ3-5æ¡ï¼‰**ï¼š

æ¯æ¡å»ºè®®åŒ…æ‹¬ï¼š
- å…·ä½“è¡ŒåŠ¨å†…å®¹
- æ³•å¾‹ä¾æ®
- é¢„æœŸæ•ˆæœ
- æ³¨æ„äº‹é¡¹

---

## äº”ã€é£é™©æç¤º

**ä¸»è¦æ³•å¾‹é£é™©ï¼ˆæŒ‰ä¸¥é‡ç¨‹åº¦æ’åºï¼‰**ï¼š

1. **é£é™©1**ï¼šé£é™©æè¿° + åº”å¯¹æªæ–½
2. **é£é™©2**ï¼šé£é™©æè¿° + åº”å¯¹æªæ–½
3. **é£é™©3**ï¼šé£é™©æè¿° + åº”å¯¹æªæ–½

---

ã€é‡è¦ç¦å¿Œã€‘
- âŒ ä¸è¦ç®€å•å¤è¿°æ–‡æ¡£å†…å®¹æˆ–ç”¨æˆ·é—®é¢˜
- âŒ ä¸è¦ä½¿ç”¨"å»ºè®®æŸ¥é˜…"ã€"æ”¶é›†è¯æ®"ç­‰ç©ºæ³›è¡¨è¿°
- âŒ ä¸è¦ç»™å‡ºæ¨¡æ£±ä¸¤å¯çš„å»ºè®®
- âŒ **ä¸¥ç¦é—æ¼ç”¨æˆ·æå‡ºçš„ä»»ä½•ä¸€ä¸ªé—®é¢˜**

ã€ä¸“ä¸šæ ‡å‡†ã€‘
- âœ… æ¯ä¸ªç»“è®ºéƒ½è¦æœ‰æ˜ç¡®çš„æ³•å¾‹ä¾æ®
- âœ… å»ºè®®è¦å…·ä½“åˆ°"åšä»€ä¹ˆ"ã€"æ€ä¹ˆåš"
- âœ… ä½“ç°15å¹´æ‰§ä¸šç»éªŒçš„ä¸“ä¸šåˆ¤æ–­
- âœ… **é’ˆå¯¹æ¯ä¸ªé—®é¢˜éƒ½ç»™å‡ºæ˜ç¡®ã€å…·ä½“çš„å›ç­”**

**å…³äºæ³•å¾‹æ£€ç´¢**ï¼š
- ä½ æ‹¥æœ‰ä¸°å¯Œçš„æ³•å¾‹çŸ¥è¯†å’Œå®è·µç»éªŒ
- å¯¹äºå¸¸è§„æ³•å¾‹é—®é¢˜ï¼Œå¯ä»¥ç›´æ¥å‡­ä¸“ä¸šçŸ¥è¯†æä¾›å‡†ç¡®å»ºè®®
- å¯¹äºå¤æ‚æˆ–ç½•è§çš„æ³•å¾‹é—®é¢˜ï¼Œå¦‚æœéœ€è¦æŸ¥æ‰¾å…·ä½“æ³•æ¡æˆ–ç±»ä¼¼æ¡ˆä¾‹ï¼Œè¯·å…ˆè¿›è¡Œåˆ†æå†è¯´æ˜éœ€è¦è¿›ä¸€æ­¥æ£€ç´¢
- ä¸è¦ä¸ºäº†æ£€ç´¢è€Œæ£€ç´¢ï¼Œä»¥ä½ çš„ä¸“ä¸šåˆ¤æ–­ä¸ºå‡†
"""


async def specialist_node(state: ConsultationState) -> ConsultationState:
    """
    ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹ï¼šæ ¹æ®åˆ†ç±»ç»“æœæä¾›ä¸“ä¸šæ³•å¾‹å’¨è¯¢ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰

    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ LLM è‡ªä¸»åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢æ³•å¾‹ä¿¡æ¯
    - å¦‚æœé—®é¢˜å¤æ‚æˆ–éœ€è¦ç²¾ç¡®æ³•æ¡ï¼Œè‡ªåŠ¨è°ƒç”¨æ£€ç´¢å·¥å…·
    - å¦‚æœæ˜¯å¸¸è§„é—®é¢˜ï¼Œç›´æ¥åŸºäºçŸ¥è¯†æä¾›å»ºè®®
    - æ”¯æŒå¤šè½®å¯¹è¯ï¼šæ ¹æ® is_follow_up æ ‡å¿—åˆ¤æ–­æ˜¯å¦ä¸ºåç»­é—®é¢˜

    è¾“å…¥ï¼šstate["question"], state["classification_result"], state["specialist_role"]
    è¾“å‡ºï¼šæ›´æ–° state["legal_analysis"], state["legal_advice"], state["action_steps"], ç­‰
    """
    logger.info("[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] å¼€å§‹æä¾›ä¸“ä¸šå’¨è¯¢...")

    question = state["question"]
    classification = state.get("classification_result") or {}
    specialist_role = state.get("specialist_role") or "ä¸“ä¸šå¾‹å¸ˆ"

    # æ£€æŸ¥æ˜¯å¦ä¸ºåç»­é—®é¢˜ï¼ˆå¤šè½®å¯¹è¯ï¼‰
    is_follow_up = state.get("is_follow_up", False)
    previous_output = state.get("previous_specialist_output")

    # è·å–ä¸“ä¸šå¾‹å¸ˆä¸“ç”¨ LLMï¼ˆä¼˜å…ˆä½¿ç”¨ Qwen3-235B-Thinkingï¼‰
    llm = get_specialist_llm()

    # æ„å»ºä¸“ä¸šå¾‹å¸ˆçš„ç³»ç»Ÿæç¤ºè¯
    # å¦‚æœ classification ä¸ºç©ºï¼ˆå¤šè½®å¯¹è¯æ—¶ç›´æ¥è¿›å…¥ specialistï¼‰ï¼Œä½¿ç”¨é»˜è®¤å€¼
    legal_domain = classification.get("primary_type", "æ³•å¾‹") if classification else "æ³•å¾‹"
    specialist_role = classification.get("specialist_role", "ä¸“ä¸šå¾‹å¸ˆ") if classification else "ä¸“ä¸šå¾‹å¸ˆ"
    system_prompt = SPECIALIST_SYSTEM_PROMPT_TEMPLATE.format(
        specialist_role=specialist_role,
        legal_domain=legal_domain
    )

    # å¤šè½®å¯¹è¯æ¨¡å¼ï¼šæ·»åŠ ä¸Šä¸‹æ–‡è¿ç»­æ€§æç¤ºï¼Œå¹¶åŒ…å«ä¸Šä¸€è½®å¯¹è¯å†…å®¹
    if is_follow_up and previous_output:
        logger.info("[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] å¤šè½®å¯¹è¯æ¨¡å¼ï¼ŒåŒ…å«ä¸Šä¸€è½®å®Œæ•´ä¸Šä¸‹æ–‡")

        # æ£€æµ‹ç”¨æˆ·æ„å›¾
        user_intent = _detect_user_intent(question)

        # æ·»åŠ å¤šè½®å¯¹è¯æŒ‡ä»¤ï¼ˆæ ¹æ®ç”¨æˆ·æ„å›¾è°ƒæ•´ï¼‰
        system_prompt += "\n\nã€å¤šè½®å¯¹è¯æ¨¡å¼ã€‘\n"
        system_prompt += "ç”¨æˆ·æ­£åœ¨åŸºäºä½ ä¹‹å‰çš„å»ºè®®æå‡ºåç»­é—®é¢˜ã€‚\n"

        if user_intent == "concise":
            system_prompt += "**ç”¨æˆ·è¦æ±‚ç®€æ´å›ç­”**ï¼šè¯·ç”¨1-2å¥è¯æ¦‚æ‹¬æ ¸å¿ƒè¦ç‚¹ï¼Œä¸è¦å±•å¼€è®ºè¿°ã€‚\n"
        elif user_intent == "detailed":
            system_prompt += "**ç”¨æˆ·è¦æ±‚è¯¦ç»†è¯´æ˜**ï¼šè¯·æä¾›æ›´æ·±å…¥çš„åˆ†æå’Œæ›´å¤šç»†èŠ‚ã€‚\n"
        elif user_intent == "specific":
            system_prompt += "**ç”¨æˆ·æå‡ºå…·ä½“é—®é¢˜**ï¼šè¯·ç›´æ¥å›ç­”è¯¥é—®é¢˜ï¼Œä¸è¦é‡å¤ä¹‹å‰çš„å†…å®¹ã€‚\n"
        else:
            system_prompt += "è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜è°ƒæ•´å›ç­”é£æ ¼ï¼Œä¿æŒæ¡ˆä»¶åˆ†æçš„è¿è´¯æ€§ã€‚\n"

        system_prompt += "\nã€å¤šè½®å¯¹è¯æ ¸å¿ƒåŸåˆ™ã€‘\n"
        system_prompt += "1. **ç¦æ­¢é‡å¤**ï¼šä¸è¦å¤è¿°æˆ–é‡æ–°è¯´æ˜ä¹‹å‰å·²ç»æä¾›çš„å†…å®¹\n"
        system_prompt += "2. **å¼•ç”¨ä¸ºä¸»**ï¼šå¦‚éœ€æåŠå‰æ–‡å†…å®¹ï¼Œä½¿ç”¨'å¦‚å‰æ‰€è¿°'ã€'æ­£å¦‚ä¹‹å‰åˆ†æ'ç­‰å¼•ç”¨æ–¹å¼\n"
        system_prompt += "3. **èšç„¦æ–°é—®é¢˜**ï¼šç›´æ¥å›ç­”ç”¨æˆ·æ–°æå‡ºçš„é—®é¢˜ï¼Œä¸éœ€è¦é‡æ–°é“ºå«èƒŒæ™¯\n"
        system_prompt += "4. **ç®€æ´ä¼˜å…ˆ**ï¼šé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚è¯¦ç»†è¯´æ˜ï¼Œå¦åˆ™ä¿æŒå›ç­”ç®€æ´\n"
        system_prompt += "5. **ä¸Šä¸‹æ–‡è¿ç»­**ï¼šç¡®ä¿æ–°å›ç­”ä¸ä¹‹å‰çš„å»ºè®®ä¿æŒä¸€è‡´æ€§å’Œè¿è´¯æ€§\n"

    # æ·»åŠ é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç²¾ç®€ç‰ˆï¼Œé¿å…LLMè¿‡åº¦å¤è¿°ï¼‰
    additional_context_parts = []

    # ã€å¤šè½®å¯¹è¯ã€‘åªä¼ é€’å¼•ç”¨ï¼Œä¸ä¼ é€’å®Œæ•´å†…å®¹
    if is_follow_up and previous_output:
        previous_steps = previous_output.get("action_steps", [])

        # ä¸ä¼ é€’å®Œæ•´å†…å®¹ï¼Œåªä¼ é€’ç®€çŸ­å¼•ç”¨
        additional_context_parts.append("\n\nã€ä¸Šä¸‹æ–‡æç¤ºã€‘")
        additional_context_parts.append("ç”¨æˆ·æ­£åœ¨åŸºäºä½ ä¹‹å‰çš„æ³•å¾‹å»ºè®®æå‡ºåç»­é—®é¢˜ã€‚")
        additional_context_parts.append("ä½ ä¹‹å‰å·²ç»æä¾›äº†ï¼š")
        additional_context_parts.append(f"- é—®é¢˜åˆ†æï¼ˆå·²æä¾›ï¼‰")
        additional_context_parts.append(f"- ä¸“ä¸šå»ºè®®ï¼ˆå·²æä¾›ï¼‰")
        additional_context_parts.append(f"- é£é™©æé†’ï¼ˆå·²æä¾›ï¼‰")
        additional_context_parts.append(f"- è¡ŒåŠ¨æ­¥éª¤ï¼ˆ{len(previous_steps)}é¡¹ï¼Œå·²æä¾›ï¼‰")
        additional_context_parts.append("\nè¯·å‚è€ƒä¹‹å‰çš„åˆ†æï¼Œç›´æ¥å›ç­”ç”¨æˆ·çš„æ–°é—®é¢˜ã€‚")
        additional_context_parts.append("**é‡è¦**ï¼šä¸è¦é‡å¤ä¹‹å‰å·²ç»è¯´æ˜çš„å†…å®¹ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚é‡è¿°ã€‚")

        logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] å·²æ·»åŠ ä¸Šä¸€è½®å¯¹è¯å¼•ç”¨ï¼ˆä½¿ç”¨å¼•ç”¨æ¨¡å¼ï¼‰")

    # 1. æ¡ˆä»¶åŸºæœ¬æƒ…å†µ (æ¥è‡ªå¾‹å¸ˆåŠ©ç†çš„æ‘˜è¦ï¼Œéå®Œæ•´æ–‡æ¡£)
    if classification.get("basic_summary"):
        additional_context_parts.append(f"\n\n**æ¡ˆä»¶åŸºæœ¬æƒ…å†µ**ï¼š\n{classification['basic_summary']}")

    # 2. å¾…å’¨è¯¢é—®é¢˜æ¸…å•ï¼ˆå¢å¼ºå¯è§æ€§ï¼‰
    question_list_parts = []
    if classification.get("direct_questions"):
        question_list_parts.extend(classification["direct_questions"])
    selected_questions = state.get("selected_suggested_questions")
    if selected_questions:
        question_list_parts.extend(selected_questions)

    if question_list_parts:
        # ä½¿ç”¨æ›´çªå‡ºçš„æ ¼å¼
        questions_summary = "\n\n".join(f"### é—®é¢˜ {i+1}ï¼š{q}\nã€è¯·é’ˆå¯¹æ­¤é—®é¢˜ç»™å‡ºæ˜ç¡®å›ç­”ã€‘" for i, q in enumerate(question_list_parts))
        additional_context_parts.append(f"\n\n**å®¢æˆ·è¦æ±‚é€ä¸€å›ç­”çš„é—®é¢˜æ¸…å•**ï¼ˆå…± {len(question_list_parts)} ä¸ªé—®é¢˜ï¼‰")
        additional_context_parts.append("**é‡è¦ï¼šæ‚¨å¿…é¡»é’ˆå¯¹ä»¥ä¸‹æ¯ä¸ªé—®é¢˜é€ä¸€ç»™å‡ºæ˜ç¡®ã€å…·ä½“çš„å›ç­”ï¼Œä¸å¯é—æ¼**\n")
        additional_context_parts.append(questions_summary)
        logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] å°†å›ç­” {len(question_list_parts)} ä¸ªé—®é¢˜")

    # 3. å…³é”®äº‹å®ï¼ˆé™åˆ¶æ•°é‡ï¼Œé¿å…è¿‡å¤šï¼‰
    if classification.get("key_facts"):
        additional_context_parts.append("\n\n**å…³é”®äº‹å®**ï¼š\n" + "\n".join(f"- {fact}" for fact in classification["key_facts"][:5]))

    # 4. æ–‡æ¡£åˆ†æç»“æœï¼ˆç²¾ç®€ç‰ˆ - åªæå–å…³é”®ä¿¡æ¯ï¼Œä¸åŒ…å«å®Œæ•´æ‘˜è¦ï¼‰
    document_analysis = state.get("document_analysis")
    if document_analysis:
        # åªæ·»åŠ è¯†åˆ«çš„æ³•å¾‹é—®é¢˜å’Œäº‰è®®ç„¦ç‚¹ï¼ˆä¸æ·»åŠ å®Œæ•´æ–‡ä»¶æ‘˜è¦å’Œæ—¶é—´çº¿ï¼‰
        if document_analysis.get("legal_issues"):
            additional_context_parts.append("\n\n**è¯†åˆ«çš„æ³•å¾‹é—®é¢˜**ï¼š")
            for issue in document_analysis["legal_issues"][:5]:  # æœ€å¤š5ä¸ª
                additional_context_parts.append(f"- {issue}")

        if document_analysis.get("dispute_points"):
            additional_context_parts.append("\n\n**äº‰è®®ç„¦ç‚¹**ï¼š")
            for dispute in document_analysis["dispute_points"][:3]:  # æœ€å¤š3ä¸ª
                additional_context_parts.append(f"- {dispute}")

    additional_context = "\n".join(additional_context_parts)

    # æ„å»ºç²¾ç®€çš„æ£€ç´¢æŸ¥è¯¢ï¼ˆä¸åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œé¿å…æœç´¢æŸ¥è¯¢è¿‡é•¿ï¼‰
    search_query = question
    if classification.get("direct_questions"):
        search_query = classification["direct_questions"][0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜

    # åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢ï¼ˆå¯¹äºæ°‘æ³•å…¸ç›¸å…³é¢†åŸŸï¼Œå¼ºåˆ¶æ£€ç´¢ï¼‰
    force_search = any(domain in legal_domain for domain in LEGAL_DOMAINS_REQUIRING_SEARCH)

    search_formatted = ""
    if force_search:
        logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] æ³•å¾‹é¢†åŸŸ'{legal_domain}'éœ€è¦å¼ºåˆ¶æ£€ç´¢ï¼Œä»¥ç¡®ä¿å¼•ç”¨ç°è¡Œæ³•å¾‹")

        # ç­–ç•¥1ï¼šå…ˆä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“
        try:
            from app.services.legal_knowledge_base import get_legal_knowledge_base
            kb = get_legal_knowledge_base()
            kb_articles = kb.search(search_query, legal_domain)
            if kb_articles:
                logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] ä»çŸ¥è¯†åº“æ‰¾åˆ° {len(kb_articles)} æ¡ç›¸å…³æ¡æ–‡")
                search_formatted = kb.format_for_llm(kb_articles)
        except Exception as e:
            logger.warning(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")

        # ç­–ç•¥2ï¼šå¦‚æœçŸ¥è¯†åº“æ²¡æœ‰ç»“æœï¼Œå°è¯•åœ¨çº¿æœç´¢
        if not search_formatted:
            logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] çŸ¥è¯†åº“æœªæ‰¾åˆ°ï¼Œå°è¯•åœ¨çº¿æœç´¢")
            try:
                search_data = await get_legal_search_results(search_query, legal_domain)
                search_formatted = search_data["formatted"]
                logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] åœ¨çº¿æ£€ç´¢å®Œæˆ")
            except Exception as e:
                logger.warning(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] åœ¨çº¿æœç´¢å¤±è´¥: {e}")
                # ä½¿ç”¨çŸ¥è¯†åº“çš„é»˜è®¤æ¡æ–‡
                try:
                    from app.services.legal_knowledge_base import get_legal_knowledge_base
                    kb = get_legal_knowledge_base()
                    kb_articles = kb.get_default_articles(legal_domain)
                    if kb_articles:
                        logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] ä½¿ç”¨çŸ¥è¯†åº“é»˜è®¤æ¡æ–‡ï¼ˆ{len(kb_articles)}æ¡ï¼‰")
                        search_formatted = kb.format_for_llm(kb_articles)
                except Exception as e2:
                    logger.warning(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] è·å–é»˜è®¤æ¡æ–‡å¤±è´¥: {e2}")
    else:
        # å…¶ä»–é¢†åŸŸç”± LLM åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢
        need_search = await _decide_if_search_needed(llm, system_prompt, search_query, "", legal_domain)
        if need_search:
            logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] åˆ¤æ–­éœ€è¦æ£€ç´¢ï¼Œä½¿ç”¨ç²¾ç®€æŸ¥è¯¢: {search_query[:50]}...")
            try:
                search_data = await get_legal_search_results(search_query, legal_domain)
                search_formatted = search_data["formatted"]
                logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] æ£€ç´¢å®Œæˆ")
            except Exception as e:
                logger.warning(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] æ£€ç´¢å¤±è´¥: {str(e)}")
        else:
            logger.info("[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] åˆ¤æ–­æ— éœ€æ£€ç´¢ï¼Œç›´æ¥åŸºäºä¸“ä¸šçŸ¥è¯†æä¾›å’¨è¯¢")

    # æ·»åŠ æ£€ç´¢ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
    if search_formatted:
        additional_context += f"\n\n**æ³•å¾‹æ£€ç´¢ç»“æœï¼ˆç°è¡Œæœ‰æ•ˆæ³•å¾‹ï¼‰**ï¼š\n{search_formatted}"

        # å¼ºè°ƒä½¿ç”¨æ£€ç´¢ç»“æœ
        system_prompt += "\n\nã€âš ï¸ é‡è¦æç¤ºã€‘\n"
        system_prompt += "ä½ åˆšåˆšé€šè¿‡æ³•å¾‹æ£€ç´¢è·å¾—äº†ç°è¡Œæœ‰æ•ˆçš„æ³•å¾‹æ³•è§„ã€‚\n"
        system_prompt += "**å¿…é¡»ä¼˜å…ˆå¼•ç”¨æ£€ç´¢ç»“æœä¸­çš„æ³•å¾‹æ¡æ–‡**ï¼Œè¿™äº›æ˜¯ç°è¡Œæœ‰æ•ˆçš„æ³•å¾‹ä¾æ®ã€‚\n"
        system_prompt += "åˆ‡å‹¿å¼•ç”¨å·²åºŸæ­¢çš„æ³•å¾‹ï¼ˆå¦‚ã€ŠåˆåŒæ³•ã€‹ã€ã€Šç‰©æƒæ³•ã€‹ç­‰ï¼‰ã€‚\n"

    # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆå…ˆå¡«å……ç”¨æˆ·é—®é¢˜ï¼‰
    # å‡†å¤‡ç”¨æˆ·é—®é¢˜åˆ—è¡¨
    questions_to_answer = []

    # ã€è°ƒè¯•ã€‘æ‰“å°åˆ†ç±»ç»“æœå’ŒçŠ¶æ€
    logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] classification.direct_questions = {classification.get('direct_questions')}")
    logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] state.selected_suggested_questions = {state.get('selected_suggested_questions')}")
    logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] state keys = {list(state.keys())}")

    # ã€ä¿®å¤é—®é¢˜1ã€‘å§‹ç»ˆåŒ…å«åŸå§‹é—®é¢˜ + (ç”¨æˆ·é€‰æ‹© OR æ¨èé—®é¢˜)
    # æ„å»ºé—®é¢˜åˆ—è¡¨ï¼šåŸå§‹é—®é¢˜ + (ç”¨æˆ·é€‰æ‹© OR æ¨èé—®é¢˜)
    questions_to_answer = []

    # 1. å§‹ç»ˆåŒ…å«ç”¨æˆ·çš„åŸå§‹è¾“å…¥ï¼ˆé™¤éæ˜¯"ç»§ç»­"ç­‰æ— å…³è¾“å…¥ï¼‰
    original_question = state.get("question")
    if original_question and "ç»§ç»­" not in original_question and original_question.strip():
        questions_to_answer.append(original_question)
        logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] æ·»åŠ åŸå§‹é—®é¢˜: {original_question[:50]}...")

    # 2. è¿½åŠ è¡¥å……é—®é¢˜ï¼ˆä¼˜å…ˆç”¨æˆ·é€‰æ‹©ï¼Œå…¶æ¬¡ AI æ¨èï¼‰
    selected_questions = state.get("selected_suggested_questions")
    if selected_questions and len(selected_questions) > 0:
        # Aæ–¹æ¡ˆï¼šç”¨æˆ·æœ‰æ˜ç¡®é€‰æ‹©
        for q in selected_questions:
            if q not in questions_to_answer:  # é¿å…é‡å¤
                questions_to_answer.append(q)
        logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] è¿½åŠ ç”¨æˆ·é€‰æ‹©çš„ {len(selected_questions)} ä¸ªè¡¥å……é—®é¢˜")
    elif classification.get("direct_questions"):
        # Bæ–¹æ¡ˆï¼šç”¨æˆ·æ— é€‰æ‹©ï¼Œè¿½åŠ  AI æ¨èçš„æ ¸å¿ƒé—®é¢˜
        for q in classification["direct_questions"]:
            if q not in questions_to_answer:  # é¿å…é‡å¤
                questions_to_answer.append(q)
        logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] è¿½åŠ  AI æ¨èçš„ {len(classification['direct_questions'])} ä¸ªé—®é¢˜")

    logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] æœ€ç»ˆé—®é¢˜åˆ—è¡¨: {len(questions_to_answer)} ä¸ªé—®é¢˜")

    # å¦‚æœæœ‰ç”¨æˆ·é€‰æ‹©çš„é—®é¢˜ï¼Œå¡«å……åˆ°ç³»ç»Ÿæç¤ºè¯ä¸­
    if questions_to_answer:
        # ã€ä¿®å¤é—®é¢˜2ã€‘å¢å¼ºé—®é¢˜æ ¼å¼ï¼Œä½¿å…¶æ›´æ˜¾çœ¼ï¼Œç¡®ä¿ LLM é€ä¸€å›ç­”
        questions_formatted = "\n\n" + "="*60 + "\n"
        questions_formatted += f"ã€å¿…é¡»é€ä¸€å›ç­”çš„é—®é¢˜æ¸…å•ã€‘ï¼ˆå…± {len(questions_to_answer)} ä¸ªé—®é¢˜ï¼‰\n"
        questions_formatted += "="*60 + "\n\n"
        for i, q in enumerate(questions_to_answer):
            questions_formatted += f"### ğŸ”· é—®é¢˜ {i+1}ï¼š{q}\n"
            questions_formatted += "**ã€å¿…é¡»é’ˆå¯¹æ­¤é—®é¢˜ç»™å‡ºæ˜ç¡®ã€å…·ä½“çš„å›ç­”ï¼Œä¸å¯é—æ¼ã€‘**\n\n"
        questions_formatted += "="*60 + "\n"

        # å¡«å……é—®é¢˜å ä½ç¬¦ï¼ˆä½¿ç”¨è‡ªå®šä¹‰å ä½ç¬¦é¿å…ä¸ format() å†²çªï¼‰
        system_prompt = system_prompt.replace("<<<USER_QUESTIONS_PLACEHOLDER>>>", questions_formatted)
        logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] å·²å¡«å…… {len(questions_to_answer)} ä¸ªç”¨æˆ·é—®é¢˜åˆ°ç³»ç»Ÿæç¤ºè¯ï¼ˆå¢å¼ºæ ¼å¼ï¼‰")
    else:
        # ç†è®ºä¸Šä¸ä¼šè¿›å…¥è¿™é‡Œï¼ˆå› ä¸ºè‡³å°‘æœ‰åŸå§‹é—®é¢˜ï¼‰
        logger.warning("[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] æ— é—®é¢˜åˆ—è¡¨ï¼Œä½¿ç”¨å…œåº•é€»è¾‘")

    # ==================== æ„å»ºæ–‡ä»¶æè¿° ====================
    file_description = ""
    document_analysis = state.get("document_analysis")
    if document_analysis and document_analysis.get("document_summaries"):
        # æœ‰æ–‡ä»¶ï¼šæ„å»ºæ–‡ä»¶æè¿°
        file_description = "å®¢æˆ·å·²æä¾›ä»¥ä¸‹æ–‡ä»¶ï¼š\n\n"

        # ä» context è·å–æ–‡ä»¶ä¿¡æ¯ï¼ˆç”¨äºè·å–æ–‡ä»¶åå’Œç±»å‹ï¼‰
        context = state.get("context", {})
        uploaded_file_ids = context.get("uploaded_files", [])

        # ä»å…¨å±€å­˜å‚¨è·å–æ–‡ä»¶ä¿¡æ¯
        from app.api.consultation_router import uploaded_files as global_uploaded_files
        file_info_map = {}
        if isinstance(uploaded_file_ids, list):
            for file_id in uploaded_file_ids:
                if isinstance(file_id, str) and file_id in global_uploaded_files:
                    file_info_map[file_id] = global_uploaded_files[file_id]

        summaries = document_analysis.get("document_summaries", {})

        # éå†æ‰€æœ‰æ‘˜è¦ï¼Œç”Ÿæˆæ–‡ä»¶æè¿°
        for file_path, summary in summaries.items():
            # è·å–æ–‡ä»¶åå’Œç±»å‹
            filename = "æœªçŸ¥æ–‡ä»¶"
            file_type = "æœªçŸ¥ç±»å‹"

            # ä» file_info_map ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶ä¿¡æ¯
            for file_id, info in file_info_map.items():
                if info.get("file_path") == file_path or file_path.endswith(info.get("filename", "")):
                    filename = info.get("filename", "æœªçŸ¥æ–‡ä»¶")
                    file_type = info.get("file_type", "æœªçŸ¥ç±»å‹")
                    break

            # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼ˆ300å­—ï¼‰
            summary_text = summary.summary
            if len(summary_text) > 300:
                summary_text = summary_text[:300] + "..."

            file_description += f"**æ–‡ä»¶åç§°**: {filename}\n"
            file_description += f"**æ–‡ä»¶ç±»å‹**: {file_type}\n"
            file_description += f"**æ–‡ä»¶æ‘˜è¦**: {summary_text}\n\n"

        logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] å·²æ·»åŠ  {len(summaries)} ä¸ªæ–‡ä»¶çš„æè¿°")
    else:
        # æ— æ–‡ä»¶ï¼šä¸æ·»åŠ æ–‡ä»¶æè¿°éƒ¨åˆ†
        file_description = ""
        logger.info("[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] æ— æ–‡ä»¶ï¼Œè·³è¿‡æ–‡ä»¶æè¿°")

    # å¡«å……æ–‡ä»¶æè¿°å ä½ç¬¦
    if file_description:
        system_prompt = system_prompt.replace("<<<FILE_DESCRIPTION_PLACEHOLDER>>>", file_description)
    else:
        # ç§»é™¤æ•´ä¸ª"ä¸€ã€æ–‡ä»¶æƒ…å†µ"éƒ¨åˆ†
        system_prompt = system_prompt.replace("## ä¸€ã€æ–‡ä»¶æƒ…å†µ\n\n<<<FILE_DESCRIPTION_PLACEHOLDER>>>\n\n---\n\n", "")
        logger.info("[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] æ— æ–‡ä»¶æè¿°ï¼Œå·²ç§»é™¤æ–‡ä»¶æƒ…å†µç« èŠ‚")

    # æ„å»ºäººç±»æ¶ˆæ¯ - ç®€æ´ç‰ˆæœ¬ï¼Œå› ä¸ºé—®é¢˜å·²åœ¨ system prompt ä¸­
    if questions_to_answer:
        # æœ‰æ˜ç¡®çš„é—®é¢˜åˆ—è¡¨ - ä½¿ç”¨ç®€æ´æ ¼å¼ï¼ˆé—®é¢˜è¯¦æƒ…å·²åœ¨ system prompt ä¸­ï¼‰
        human_content = f"""ã€å®¢æˆ·å’¨è¯¢ã€‘{original_question}

{additional_context}

---
**ğŸš¨ è¾“å‡ºè¦æ±‚ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰**ï¼š
1. **å¿…é¡»é€ä¸€å›ç­”ä¸Šè¿°ç³»ç»Ÿæç¤ºè¯ä¸­çš„æ‰€æœ‰ {len(questions_to_answer)} ä¸ªé—®é¢˜ï¼Œä¸å¯é—æ¼ä»»ä½•ä¸€ä¸ª**
2. æ¯ä¸ªé—®é¢˜çš„å›ç­”åº”åŒ…å«ï¼šç›´æ¥å›ç­” + æ³•å¾‹ä¾æ® + å…·ä½“å»ºè®®
3. **å»ºè®®ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜æ ¼å¼ï¼Œå¦‚ï¼š"ğŸ”· é—®é¢˜1ï¼š[é—®é¢˜æ ‡é¢˜]"**
"""
    else:
        # æ²¡æœ‰å…·ä½“é—®é¢˜åˆ—è¡¨ï¼ˆå…œåº•é€»è¾‘ï¼‰
        human_content = f"å®¢æˆ·å’¨è¯¢é—®é¢˜ï¼š{original_question}{additional_context}"

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=human_content)
    ]

    try:
        # è°ƒç”¨ LLM
        response: AIMessage = await llm.ainvoke(messages)
        response_text = response.content

        # ã€æ–°å¢ã€‘æ£€æŸ¥å¹¶ä¿®æ­£è¿‡æ—¶çš„æ³•å¾‹å¼•ç”¨
        response_text, was_fixed = _check_and_fix_legal_references(response_text)
        if was_fixed:
            logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] å·²è‡ªåŠ¨ä¿®æ­£è¿‡æ—¶çš„æ³•å¾‹å¼•ç”¨ï¼Œç¡®ä¿è¾“å‡ºç°è¡Œæœ‰æ•ˆæ³•å¾‹")

        logger.info(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] LLM å“åº”é•¿åº¦ï¼š{len(response_text)} å­—ç¬¦")

        # è§£æä¸“ä¸šå¾‹å¸ˆçš„å›å¤
        parsed_result = parse_specialist_response(response_text)

        # ä½¿ç”¨æ¸²æŸ“å™¨æ¸…ç† Markdown ç¬¦å·ï¼ˆè½¬ä¸ºçº¯æ–‡æœ¬ï¼Œç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
        from app.services.markdown_renderer import get_markdown_renderer
        renderer = get_markdown_renderer()

        # æ¸…ç†å„ä¸ªéƒ¨åˆ†çš„ Markdown ç¬¦å·
        clean_analysis = renderer.render_to_clean_text(parsed_result.get("analysis", response_text))
        clean_advice = renderer.render_to_clean_text(parsed_result.get("advice", ""))
        clean_risk_warning = renderer.render_to_clean_text(parsed_result.get("risk_warning", ""))

        # æ›´æ–°çŠ¶æ€ï¼ˆå­˜å‚¨çº¯æ–‡æœ¬ï¼‰
        state["legal_analysis"] = clean_analysis
        state["legal_advice"] = clean_advice
        state["risk_warning"] = clean_risk_warning
        state["action_steps"] = parsed_result.get("action_steps", [])
        state["relevant_laws"] = parsed_result.get("relevant_laws", state.get("relevant_laws", []))
        state["current_step"] = "specialist_node_completed"

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼Œç§»é™¤ Markdown ç¬¦å·ï¼‰
        state["final_report"] = generate_final_report(state, renderer)

        logger.info("[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] å’¨è¯¢å®Œæˆ")

    except Exception as e:
        logger.error(f"[ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹] å¤„ç†å¤±è´¥ï¼š{str(e)}")
        import traceback
        traceback.print_exc()
        state["error"] = f"ä¸“ä¸šå’¨è¯¢å¤±è´¥ï¼š{str(e)}"
        state["legal_analysis"] = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„å’¨è¯¢æ—¶é‡åˆ°é—®é¢˜ã€‚"
        state["legal_advice"] = "è¯·ç¨åé‡è¯•æˆ–è”ç³»ä¸“ä¸šå¾‹å¸ˆã€‚"
        state["action_steps"] = ["è¯·é‡æ–°æäº¤å’¨è¯¢", "æˆ–è”ç³»çº¿ä¸‹ä¸“ä¸šå¾‹å¸ˆ"]

    return state


async def _decide_if_search_needed(
    llm: ChatOpenAI,
    system_prompt: str,
    question: str,
    additional_context: str,
    legal_domain: str
) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢æ³•å¾‹ä¿¡æ¯

    ä½¿ç”¨ LLM è¯„ä¼°é—®é¢˜çš„å¤æ‚åº¦å’Œæ˜¯å¦éœ€è¦ç²¾ç¡®æ³•æ¡

    Returns:
        True è¡¨ç¤ºéœ€è¦æ£€ç´¢ï¼ŒFalse è¡¨ç¤ºä¸éœ€è¦
    """
    decision_prompt = f"""{system_prompt}

**å½“å‰ä»»åŠ¡**ï¼š
åˆ¤æ–­ä»¥ä¸‹æ³•å¾‹å’¨è¯¢é—®é¢˜æ˜¯å¦éœ€è¦æ£€ç´¢æœ€æ–°çš„æ³•å¾‹æ³•è§„æˆ–ç±»ä¼¼æ¡ˆä¾‹ã€‚

**åˆ¤æ–­æ ‡å‡†**ï¼š
1. éœ€è¦æ£€ç´¢çš„æƒ…å†µï¼š
   - é—®é¢˜æ¶‰åŠå…·ä½“æ³•æ¡æ¡æ–‡ç¼–å·
   - é—®é¢˜æ¶‰åŠæœ€æ–°çš„æ³•å¾‹ä¿®æ”¹æˆ–å¸æ³•è§£é‡Š
   - é—®é¢˜å±äºç½•è§æˆ–å¤æ‚çš„æ³•å¾‹æƒ…å½¢
   - ç”¨æˆ·æ˜ç¡®è¦æ±‚æŸ¥æ‰¾ç›¸å…³æ¡ˆä¾‹
   - é—®é¢˜æ—¶æ•ˆæ€§è¾ƒå¼ºï¼ˆå¦‚æ–°é¢å¸ƒçš„æ³•è§„ï¼‰

2. ä¸éœ€è¦æ£€ç´¢çš„æƒ…å†µï¼š
   - å¸¸è§„æ³•å¾‹é—®é¢˜ï¼ˆå¦‚åŠ³åŠ¨çº çº·ã€åˆåŒè¿çº¦çš„ä¸€èˆ¬å¤„ç†ï¼‰
   - åŸºç¡€æ³•å¾‹æ¦‚å¿µè§£é‡Š
   - å¸¸è§æ³•å¾‹æµç¨‹å’¨è¯¢
   - å¯ä»¥å‡­ä¸“ä¸šçŸ¥è¯†ç›´æ¥å›ç­”çš„é—®é¢˜

**å’¨è¯¢é—®é¢˜**ï¼š
{question}{additional_context}

**æ³•å¾‹é¢†åŸŸ**ï¼š{legal_domain}

è¯·åªå›ç­” "éœ€è¦æ£€ç´¢" æˆ– "ä¸éœ€è¦æ£€ç´¢"ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""

    try:
        messages = [
            SystemMessage(content=decision_prompt),
            HumanMessage(content="è¯·åˆ¤æ–­ï¼š")
        ]

        response: AIMessage = await llm.ainvoke(messages)
        response_text = response.content.strip()

        logger.info(f"[æ£€ç´¢åˆ¤æ–­] LLM åˆ¤æ–­ç»“æœ: {response_text}")

        # åˆ¤æ–­å“åº”
        if "éœ€è¦æ£€ç´¢" in response_text or "éœ€è¦" in response_text:
            return True
        else:
            return False

    except Exception as e:
        logger.warning(f"[æ£€ç´¢åˆ¤æ–­] åˆ¤æ–­å¤±è´¥: {e}ï¼Œé»˜è®¤ä¸æ£€ç´¢")
        return False


def parse_specialist_response(response_text: str) -> Dict[str, Any]:
    """
    è§£æä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹çš„ç»“æ„åŒ–å›å¤

    Args:
        response_text: LLM è¿”å›çš„æ–‡æœ¬

    Returns:
        è§£æåçš„ç»“æ„åŒ–å­—å…¸
    """
    result = {
        "analysis": "",
        "advice": "",
        "risk_warning": "",
        "action_steps": [],
        "relevant_laws": []
    }

    # æå–å„ä¸ªéƒ¨åˆ†ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ”¯æŒå¤šçº§æ ‡é¢˜ï¼‰
    import re

    # æå–æ³•å¾‹ä¾æ®ï¼ˆæ”¯æŒ # æˆ– ## æˆ– ###ï¼‰
    legal_basis_match = re.search(r'#+\s*æ³•å¾‹ä¾æ®\s*\n(.*?)(?=#+\s*(?:é—®é¢˜åˆ†æ|ä¸“ä¸šå»ºè®®|é£é™©æé†’|è¡ŒåŠ¨æ­¥éª¤)|\Z)', response_text, re.DOTALL)
    if legal_basis_match:
        result["relevant_laws"] = [line.strip() for line in legal_basis_match.group(1).strip().split('\n') if line.strip() and not line.strip().startswith('#')]

    # æå–é—®é¢˜åˆ†æ
    analysis_match = re.search(r'#+\s*é—®é¢˜åˆ†æ\s*\n(.*?)(?=#+\s*(?:ä¸“ä¸šå»ºè®®|é£é™©æé†’|è¡ŒåŠ¨æ­¥éª¤)|\Z)', response_text, re.DOTALL)
    if analysis_match:
        result["analysis"] = analysis_match.group(1).strip()

    # æå–ä¸“ä¸šå»ºè®®
    advice_match = re.search(r'#+\s*ä¸“ä¸šå»ºè®®\s*\n(.*?)(?=#+\s*(?:é£é™©æé†’|è¡ŒåŠ¨æ­¥éª¤)|\Z)', response_text, re.DOTALL)
    if advice_match:
        result["advice"] = advice_match.group(1).strip()

    # æå–é£é™©æé†’
    risk_match = re.search(r'#+\s*é£é™©æé†’\s*\n(.*?)(?=#+\s*è¡ŒåŠ¨æ­¥éª¤|\Z)', response_text, re.DOTALL)
    if risk_match:
        result["risk_warning"] = risk_match.group(1).strip()

    # æå–è¡ŒåŠ¨æ­¥éª¤
    steps_match = re.search(r'#+\s*è¡ŒåŠ¨æ­¥éª¤\s*\n(.*?)(?=\Z)', response_text, re.DOTALL)
    if steps_match:
        steps_text = steps_match.group(1).strip()
        # è§£ææ­¥éª¤åˆ—è¡¨ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        steps = []
        for line in steps_text.split('\n'):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            # ç§»é™¤åºå·å‰ç¼€ï¼ˆå¦‚ "1." æˆ– "[ç´§æ€¥]"ï¼‰
            step = re.sub(r'^[\d\[\]]+\.\s*', '', line)  # ç§»é™¤ "1. "
            step = re.sub(r'^\[.*?\]\s*', '', step)  # ç§»é™¤ "[ç´§æ€¥] "
            step = re.sub(r'^\*\*.*?\*\*\s*', '', step)  # ç§»é™¤ "**ç´§æ€¥**"
            step = step.strip('-*â€¢ ')  # ç§»é™¤åˆ—è¡¨ç¬¦å·
            if step:
                steps.append(step)
        result["action_steps"] = steps

    # å¦‚æœåˆ†æä¸ºç©ºï¼Œä½¿ç”¨æ•´ä¸ªæ–‡æœ¬
    if not result["analysis"]:
        result["analysis"] = response_text

    return result


def generate_final_report(state: ConsultationState, renderer=None) -> str:
    """
    ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š

    Args:
        state: å’¨è¯¢çŠ¶æ€
        renderer: Markdown æ¸²æŸ“å™¨ï¼ˆå¯é€‰ï¼‰

    Returns:
        æœ€ç»ˆæŠ¥å‘Šï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼Œå·²ç§»é™¤ Markdown ç¬¦å·ï¼‰
    """
    question = state["question"]
    classification = state.get("classification_result") or {}
    relevant_laws = state.get("relevant_laws") or []
    analysis = state.get("legal_analysis") or ""
    advice = state.get("legal_advice") or ""
    risk_warning = state.get("risk_warning") or ""
    action_steps = state.get("action_steps") or []

    # ä½¿ç”¨æ¸²æŸ“å™¨æ¸…ç† Markdown ç¬¦å·ï¼Œè½¬ä¸ºçº¯æ–‡æœ¬
    if renderer:
        clean_question = renderer.render_to_clean_text(question)
        clean_analysis = renderer.render_to_clean_text(analysis)
        clean_advice = renderer.render_to_clean_text(advice)
        clean_risk_warning = renderer.render_to_clean_text(risk_warning)
    else:
        clean_question = question
        clean_analysis = analysis
        clean_advice = advice
        clean_risk_warning = risk_warning

    # æ„å»ºçº¯æ–‡æœ¬æŠ¥å‘Š
    report = f"""â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                    æ³•å¾‹å’¨è¯¢æŠ¥å‘Š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ã€é—®é¢˜æè¿°ã€‘
{clean_question}

ã€åˆ†ç±»ç»“æœã€‘
  ä¸“ä¸šé¢†åŸŸï¼š{classification.get('primary_type') or 'æœªçŸ¥'}
  ä¸“ä¸šå¾‹å¸ˆï¼š{state.get('specialist_role') or 'ä¸“ä¸šå¾‹å¸ˆ'}
  ç½®ä¿¡åº¦ï¼š{(classification.get('confidence') or 0.8) * 100:.1f}%
  å¤æ‚ç¨‹åº¦ï¼š{classification.get('complexity') or 'medium'}
  ç´§æ€¥ç¨‹åº¦ï¼š{classification.get('urgency') or 'medium'}
"""

    if relevant_laws:
        report += f"""
ã€æ³•å¾‹ä¾æ®ã€‘
"""
        for law in relevant_laws:
            report += f"  â€¢ {law}\n"

    report += f"""
ã€é—®é¢˜åˆ†æã€‘
{clean_analysis}
"""

    # åªåœ¨æœ‰å†…å®¹æ—¶æ·»åŠ ä¸“ä¸šå»ºè®®
    if clean_advice and clean_advice.strip():
        report += f"""

ã€ä¸“ä¸šå»ºè®®ã€‘
{clean_advice}"""

    # åªåœ¨æœ‰å†…å®¹æ—¶æ·»åŠ é£é™©æé†’
    if clean_risk_warning and clean_risk_warning.strip():
        report += f"""

ã€é£é™©æé†’ã€‘
{clean_risk_warning}"""

    # åªåœ¨æœ‰æ­¥éª¤æ—¶æ·»åŠ è¡ŒåŠ¨æ­¥éª¤
    if action_steps:
        report += """

ã€è¡ŒåŠ¨æ­¥éª¤ã€‘
"""
        for i, step in enumerate(action_steps, 1):
            report += f"  {i}. {step}\n"

    report += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä»¥ä¸Šå»ºè®®ä»…ä¾›å‚è€ƒï¼Œé‡è¦æ³•å¾‹äº‹åŠ¡å»ºè®®çº¿ä¸‹å’¨è¯¢ä¸“ä¸šæ‰§ä¸šå¾‹å¸ˆã€‚
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

    return report


# ==================== æ¡ä»¶è·¯ç”±å‡½æ•° ====================

def should_start_with_assistant(state: ConsultationState) -> str:
    """
    æ™ºèƒ½å…¥å£è·¯ç”±ï¼šå†³å®šä»å“ªä¸ªèŠ‚ç‚¹å¼€å§‹ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰

    è·¯ç”±é€»è¾‘ï¼š
    - å¦‚æœæ˜¯åç»­é—®é¢˜ (is_follow_up=True) â†’ ç›´æ¥è¿›å…¥ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹
    - å¦‚æœæ˜¯æ–°é—®é¢˜ â†’ ä»å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹å¼€å§‹
    """
    if state.get("is_follow_up"):
        # åç»­é—®é¢˜ï¼Œç›´æ¥è¿›å…¥ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹
        logger.info("[è·¯ç”±] æ£€æµ‹åˆ°åç»­é—®é¢˜ï¼Œç›´æ¥è¿›å…¥ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹")
        return "specialist"
    else:
        # æ–°é—®é¢˜ï¼Œä»å¾‹å¸ˆåŠ©ç†å¼€å§‹
        logger.info("[è·¯ç”±] æ–°é—®é¢˜ï¼Œä»å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹å¼€å§‹")
        return "assistant"


def should_continue_after_assistant(state: ConsultationState) -> str:
    """
    å¾‹å¸ˆåŠ©ç†èŠ‚ç‚¹åçš„è·¯ç”±å†³ç­–

    ä¸¤é˜¶æ®µæ‰§è¡Œæ¨¡å¼ï¼š
    - å¦‚æœç”¨æˆ·å·²ç¡®è®¤ (user_confirmed=True) â†’ ç»§ç»­æ‰§è¡Œæ–‡æ¡£åˆ†æå’Œä¸“ä¸šå¾‹å¸ˆ
    - å¦‚æœç”¨æˆ·æœªç¡®è®¤ (user_confirmed=False) â†’ è¿”å› ENDï¼Œç­‰å¾…å‰ç«¯ç¡®è®¤

    è·¯ç”±é€»è¾‘ï¼š
    - å¦‚æœæœ‰é”™è¯¯ï¼Œç»“æŸ
    - å¦‚æœç”¨æˆ·å·²ç¡®è®¤ä¸”æœ‰æ–‡ä»¶ï¼Œæ‰§è¡Œæ–‡æ¡£åˆ†æ
    - å¦‚æœç”¨æˆ·å·²ç¡®è®¤ä¸”æ— æ–‡ä»¶ï¼Œç›´æ¥åˆ°ä¸“ä¸šå¾‹å¸ˆ
    - å¦‚æœç”¨æˆ·æœªç¡®è®¤ï¼Œç»“æŸï¼ˆè¿”å›ç¡®è®¤æ¶ˆæ¯ï¼‰
    """
    if state.get("error"):
        logger.error("[è·¯ç”±] æ£€æµ‹åˆ°é”™è¯¯ï¼Œç»“æŸæµç¨‹")
        return "end"

    if state.get("user_confirmed"):
        # ç”¨æˆ·å·²ç¡®è®¤ï¼Œç»§ç»­æ‰§è¡Œ
        context = state.get("context", {})
        uploaded_files = context.get("uploaded_files", [])
        if uploaded_files:
            logger.info("[è·¯ç”±] ç”¨æˆ·å·²ç¡®è®¤ä¸”æœ‰æ–‡ä»¶ï¼Œæ‰§è¡Œæ–‡æ¡£åˆ†æ")
            return "analyze"
        else:
            logger.info("[è·¯ç”±] ç”¨æˆ·å·²ç¡®è®¤ä¸”æ— æ–‡ä»¶ï¼Œç›´æ¥åˆ°ä¸“ä¸šå¾‹å¸ˆ")
            return "specialist"
    else:
        # ç”¨æˆ·æœªç¡®è®¤ï¼Œè¿”å›ç¡®è®¤æ¶ˆæ¯
        logger.info("[è·¯ç”±] ç”¨æˆ·æœªç¡®è®¤ï¼Œè¿”å›ç¡®è®¤æ¶ˆæ¯")
        return "end"


def should_analyze_documents(state: ConsultationState) -> str:
    """
    å†³å®šæ˜¯å¦éœ€è¦æ–‡æ¡£åˆ†æ

    ç”¨äºæ–‡æ¡£åˆ†æèŠ‚ç‚¹åçš„è·¯ç”±
    """
    return "specialist"  # æ€»æ˜¯ç»§ç»­åˆ°ä¸“ä¸šå¾‹å¸ˆ


def should_continue_to_specialist(state: ConsultationState) -> str:
    """
    å†³å®šæ˜¯å¦ç»§ç»­åˆ°ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹ï¼ˆå·²å¼ƒç”¨ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
    """
    if state.get("error"):
        logger.error("[è·¯ç”±] æ£€æµ‹åˆ°é”™è¯¯ï¼Œç»“æŸæµç¨‹")
        return "end"

    if state.get("classification_result"):
        logger.info("[è·¯ç”±] åˆ†ç±»å®Œæˆï¼Œç»§ç»­åˆ°ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹")
        return "specialist"

    logger.warning("[è·¯ç”±] æœªå®Œæˆåˆ†ç±»ï¼Œç»“æŸæµç¨‹")
    return "end"


# ==================== æ„å»º LangGraph å·¥ä½œæµ ====================

def create_legal_consultation_graph():
    """
    åˆ›å»ºæ³•å¾‹å’¨è¯¢çš„ LangGraph å·¥ä½œæµï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰

    å¤šè½®å¯¹è¯æ¨¡å¼ï¼š
    - æ–°é—®é¢˜ï¼šé€šè¿‡è·¯ç”±å‡½æ•°å†³å®šä» assistant å¼€å§‹
    - åç»­é—®é¢˜ï¼šé€šè¿‡è·¯ç”±å‡½æ•°å†³å®šç›´æ¥è¿›å…¥ specialist

    1. assistant_node: é—®é¢˜åˆ†ç±»å’Œæ„å›¾è¯†åˆ«,ç”Ÿæˆ basic_summary å’Œ question_list
    2. document_analysis_node: æ–‡æ¡£æ·±åº¦åˆ†æï¼ˆä»…åœ¨æœ‰æ–‡ä»¶æ—¶æ‰§è¡Œï¼‰
    3. specialist_node: ç”Ÿæˆä¸“ä¸šæ³•å¾‹å»ºè®®ï¼ˆå†…éƒ¨è‡ªä¸»å†³å®šæ˜¯å¦æ£€ç´¢ï¼‰
    """
    logger.info("[å·¥ä½œæµ] æ„å»ºæ³•å¾‹å’¨è¯¢ LangGraphï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰...")

    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(ConsultationState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("assistant", assistant_node)
    workflow.add_node("doc_analysis", document_analysis_node)
    workflow.add_node("specialist", specialist_node)

    # ã€å…³é”®ä¿®æ”¹ã€‘ä½¿ç”¨æ¡ä»¶å…¥å£ç‚¹ï¼Œæ”¯æŒå¤šè½®å¯¹è¯ç›´æ¥è¿›å…¥ä¸“ä¸šå¾‹å¸ˆèŠ‚ç‚¹
    workflow.set_entry_point("assistant")  # è®¾ç½®é»˜è®¤å…¥å£ç‚¹

    # æ·»åŠ è¾¹ï¼šassistant â†’ [æ¡ä»¶è·¯ç”±: END(ç­‰å¾…ç¡®è®¤) æˆ– doc_analysis æˆ– specialist]
    workflow.add_conditional_edges(
        "assistant",
        should_continue_after_assistant,
        {
            "analyze": "doc_analysis",
            "specialist": "specialist",
            "end": END
        }
    )

    # æ·»åŠ è¾¹ï¼šdoc_analysis â†’ specialist
    workflow.add_edge("doc_analysis", "specialist")

    # æ·»åŠ è¾¹ï¼šspecialist â†’ END
    workflow.add_edge("specialist", END)

    # ç¼–è¯‘å·¥ä½œæµ
    app = workflow.compile()

    logger.info("[å·¥ä½œæµ] LangGraph æ„å»ºå®Œæˆï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ - é€šè¿‡assistantèŠ‚ç‚¹å†…éƒ¨åˆ¤æ–­ï¼‰")

    return app


def create_legal_consultation_graph_v2():
    """
    åˆ›å»ºæ³•å¾‹å’¨è¯¢çš„ LangGraph å·¥ä½œæµï¼ˆV2ç‰ˆæœ¬ - çœŸæ­£çš„æ¡ä»¶å…¥å£ç‚¹ï¼‰

    ã€æ¶æ„æ”¹è¿›ã€‘ä½¿ç”¨æ¡ä»¶å…¥å£ç‚¹ï¼Œå¤šè½®å¯¹è¯å®Œå…¨ç»•è¿‡assistantèŠ‚ç‚¹

    æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬éœ€è¦ LangGraph 0.2.0+ æ”¯æŒï¼Œå¦‚æœæŠ¥é”™è¯·ä½¿ç”¨ create_legal_consultation_graph
    """
    logger.info("[å·¥ä½œæµ V2] æ„å»ºæ³•å¾‹å’¨è¯¢ LangGraphï¼ˆçœŸæ­£çš„æ¡ä»¶å…¥å£ç‚¹ï¼‰...")

    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(ConsultationState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("assistant", assistant_node)
    workflow.add_node("doc_analysis", document_analysis_node)
    workflow.add_node("specialist", specialist_node)

    # ã€å…³é”®ä¿®æ”¹ã€‘çœŸæ­£çš„æ¡ä»¶å…¥å£ç‚¹
    # å¤šè½®å¯¹è¯æ—¶ï¼Œåç»­é—®é¢˜å®Œå…¨è·³è¿‡ assistant_node
    try:
        # å°è¯•ä½¿ç”¨ START èŠ‚ç‚¹ä½œä¸ºæ¡ä»¶å…¥å£
        from langgraph.graph import START

        workflow.add_conditional_edges(
            START,
            should_start_with_assistant,
            {
                "assistant": "assistant",
                "specialist": "specialist"
            }
        )

        # assistant èŠ‚ç‚¹åçš„è·¯ç”±
        workflow.add_conditional_edges(
            "assistant",
            should_continue_after_assistant,
            {
                "analyze": "doc_analysis",
                "specialist": "specialist",
                "end": END
            }
        )

        # æ·»åŠ è¾¹ï¼šdoc_analysis â†’ specialist
        workflow.add_edge("doc_analysis", "specialist")

        # æ·»åŠ è¾¹ï¼šspecialist â†’ END
        workflow.add_edge("specialist", END)

        logger.info("[å·¥ä½œæµ V2] ä½¿ç”¨ START æ¡ä»¶å…¥å£ç‚¹æˆåŠŸ")

    except Exception as e:
        logger.warning(f"[å·¥ä½œæµ V2] START æ¡ä»¶å…¥å£ç‚¹ä¸æ”¯æŒ: {e}")
        logger.info("[å·¥ä½œæµ V2] å›é€€åˆ°ä¼ ç»Ÿå›ºå®šå…¥å£ç‚¹æ¨¡å¼")
        # å›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼
        workflow.set_entry_point("assistant")

        workflow.add_conditional_edges(
            "assistant",
            should_continue_after_assistant,
            {
                "analyze": "doc_analysis",
                "specialist": "specialist",
                "end": END
            }
        )

        workflow.add_edge("doc_analysis", "specialist")
        workflow.add_edge("specialist", END)

    # ç¼–è¯‘å·¥ä½œæµ
    app = workflow.compile()

    logger.info("[å·¥ä½œæµ V2] LangGraph æ„å»ºå®Œæˆ")

    return app


# ==================== ä¸»è¦æ¥å£å‡½æ•° ====================

# åˆ›å»ºå…¨å±€å·¥ä½œæµå®ä¾‹
_legal_consultation_graph = None


def get_consultation_graph():
    """è·å–å’¨è¯¢å·¥ä½œæµå•ä¾‹"""
    global _legal_consultation_graph
    if _legal_consultation_graph is None:
        # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨ V2 ç‰ˆæœ¬ï¼Œæ”¯æŒçœŸæ­£çš„æ¡ä»¶å…¥å£ç‚¹
        _legal_consultation_graph = create_legal_consultation_graph_v2()
        logger.info("[å·¥ä½œæµ] ä½¿ç”¨ V2 ç‰ˆæœ¬ï¼ˆæ”¯æŒæ¡ä»¶å…¥å£ç‚¹ï¼‰")
    return _legal_consultation_graph


async def run_legal_consultation(
    question: str,
    context: Optional[Dict[str, Any]] = None,
    conversation_history: Optional[List[BaseMessage]] = None,
    user_confirmed: bool = False,
    selected_suggested_questions: Optional[List[str]] = None,
    is_follow_up: bool = False,  # æ–°å¢ï¼šå¤šè½®å¯¹è¯æ ‡å¿—
    session_id: Optional[str] = None,  # æ–°å¢ï¼šä¼šè¯ID
    previous_specialist_output: Optional[Dict[str, Any]] = None,  # æ–°å¢ï¼šä¸Šä¸€è½®è¾“å‡º
    saved_classification: Optional[Dict[str, Any]] = None  # ã€æ–°å¢ã€‘æ¢å¤çš„åˆ†ç±»ç»“æœ
) -> Tuple[Optional[ConsultationOutput], Optional[str]]:
    """
    è¿è¡Œæ³•å¾‹å’¨è¯¢å·¥ä½œæµï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰

    Args:
        question: ç”¨æˆ·é—®é¢˜
        context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
        conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
        user_confirmed: ç”¨æˆ·æ˜¯å¦å·²ç¡®è®¤ï¼ˆç”¨äºä¸¤é˜¶æ®µæ‰§è¡Œï¼‰
        selected_suggested_questions: ç”¨æˆ·é€‰æ‹©çš„å»ºè®®é—®é¢˜ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰
        is_follow_up: æ˜¯å¦ä¸ºåç»­é—®é¢˜ï¼ˆå¤šè½®å¯¹è¯æ ‡å¿—ï¼‰
        session_id: ä¼šè¯IDï¼ˆç”¨äºæŒä¹…åŒ–ï¼‰
        previous_specialist_output: ä¸Šä¸€è½®ä¸“ä¸šå¾‹å¸ˆè¾“å‡ºï¼ˆç”¨äºå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
        saved_classification: æ¢å¤çš„åˆ†ç±»ç»“æœï¼ˆç”¨æˆ·ç¡®è®¤é˜¶æ®µä½¿ç”¨ï¼‰

    Returns:
        (ConsultationOutput, final_report)
    """
    logger.info(f"[å’¨è¯¢æµç¨‹] ===== å¼€å§‹å¤„ç† =====")
    logger.info(f"[å’¨è¯¢æµç¨‹] é—®é¢˜ï¼š{question[:50]}... (is_follow_up={is_follow_up}, user_confirmed={user_confirmed})")
    logger.info(f"[å’¨è¯¢æµç¨‹] selected_suggested_questions ç±»å‹: {type(selected_suggested_questions)}")
    logger.info(f"[å’¨è¯¢æµç¨‹] selected_suggested_questions å€¼: {selected_suggested_questions}")
    if selected_suggested_questions:
        logger.info(f"[å’¨è¯¢æµç¨‹] selected_suggested_questions é•¿åº¦: {len(selected_suggested_questions)}")
        for i, q in enumerate(selected_suggested_questions):
            logger.info(f"[å’¨è¯¢æµç¨‹]   é—®é¢˜ {i+1}: {q}")
    else:
        logger.warning(f"[å’¨è¯¢æµç¨‹] selected_suggested_questions ä¸º None æˆ–ç©ºï¼")

    # ã€æ–°å¢ã€‘å¦‚æœæœ‰æ¢å¤çš„åˆ†ç±»ç»“æœï¼Œæ‰“å°æ—¥å¿—
    if saved_classification:
        logger.info(f"[å’¨è¯¢æµç¨‹] ä½¿ç”¨æ¢å¤çš„åˆ†ç±»ç»“æœ: primary_type={saved_classification.get('primary_type')}")
        logger.info(f"[å’¨è¯¢æµç¨‹]   direct_questions: {saved_classification.get('direct_questions')}")
        logger.info(f"[å’¨è¯¢æµç¨‹]   suggested_questions: {saved_classification.get('suggested_questions')}")

    # åˆå§‹åŒ–çŠ¶æ€
    initial_state: ConsultationState = {
        "question": question,
        "context": context or {},
        "conversation_history": conversation_history or [],
        "user_confirmed": user_confirmed,
        "selected_suggested_questions": selected_suggested_questions,
        "is_follow_up": is_follow_up,  # æ–°å¢ï¼šå¤šè½®å¯¹è¯æ ‡å¿—
        "session_id": session_id,  # æ–°å¢ï¼šä¼šè¯ID
        "previous_specialist_output": previous_specialist_output,  # æ–°å¢ï¼šä¸Šä¸€è½®è¾“å‡º
        "classification_result": saved_classification,  # ã€ä¿®æ”¹ã€‘ä½¿ç”¨æ¢å¤çš„åˆ†ç±»ç»“æœ
        "specialist_role": None,
        "confidence": None,
        "document_analysis": None,
        "legal_analysis": None,
        "legal_advice": None,
        "risk_warning": None,
        "action_steps": None,
        "relevant_laws": None,
        "final_report": None,
        "need_follow_up": False,
        "follow_up_questions": [],
        "error": None,
        "current_step": "start"
    }

    try:
        # è·å–å·¥ä½œæµ
        graph = get_consultation_graph()

        # æ‰§è¡Œå·¥ä½œæµ
        result_state = await graph.ainvoke(initial_state)

        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if result_state.get("error"):
            logger.error(f"[å’¨è¯¢æµç¨‹] æ‰§è¡Œå¤±è´¥ï¼š{result_state['error']}")
            return None, result_state["error"]

        # æ„å»ºè¾“å‡º
        output = ConsultationOutput(
            question=question,
            legal_basis="ã€".join(result_state.get("relevant_laws", [])),
            analysis=result_state.get("legal_analysis", ""),
            advice=result_state.get("legal_advice", ""),
            risk_warning=result_state.get("risk_warning", ""),
            action_steps=result_state.get("action_steps", []),
            classification_result=result_state.get("classification_result"),
            need_follow_up=result_state.get("need_follow_up", False),
            follow_up_questions=result_state.get("follow_up_questions", [])
        )

        final_report = result_state.get("final_report", "")

        logger.info("[å’¨è¯¢æµç¨‹] å¤„ç†å®Œæˆ")
        return output, final_report

    except Exception as e:
        logger.error(f"[å’¨è¯¢æµç¨‹] æ‰§è¡Œå¼‚å¸¸ï¼š{str(e)}")
        import traceback
        traceback.print_exc()
        return None, f"å¤„ç†å’¨è¯¢æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"


# ==================== æµ‹è¯•ä»£ç  ====================

if __name__ == "__main__":
    import asyncio

    async def test_consultation():
        """æµ‹è¯•æ³•å¾‹å’¨è¯¢åŠŸèƒ½"""
        # æµ‹è¯•é—®é¢˜1ï¼šç®€å•é—®é¢˜
        test_question_1 = "å…¬å¸è®¾ç«‹æ³•å¾‹è¦æ±‚"

        # æµ‹è¯•é—®é¢˜2ï¼šå¤æ‚å»ºå·¥çº çº·
        test_question_2 = """æˆéƒ½å…´ä¸šå»ºç­‘å·¥ç¨‹æœ‰é™å…¬å¸ä»¥å››å·é‘«ç»µå…´å»ºç­‘å·¥ç¨‹æœ‰é™å…¬å¸åä¹‰å–å¾—äº†ä¸€ä¸ªæ–½å·¥æ€»æ‰¿åŒ…é¡¹ç›®...
        ï¼ˆè¯¦ç»†æ¡ˆæƒ…çœç•¥ï¼Œè¯·æŸ¥çœ‹æµ‹è¯•ç”¨ä¾‹ï¼‰
        """

        print("=" * 80)
        print("æµ‹è¯•é—®é¢˜1ï¼š", test_question_1)
        print("=" * 80)

        result1, report1 = await run_legal_consultation(test_question_1)

        if result1:
            print("\nâœ… å’¨è¯¢æˆåŠŸï¼")
            print(f"åˆ†ç±»ï¼š{result1.classification_result}")
            print(f"åˆ†æï¼š{result1.analysis[:200]}...")
            print("\nå®Œæ•´æŠ¥å‘Šï¼š")
            print(report1)
        else:
            print(f"âŒ å’¨è¯¢å¤±è´¥ï¼š{report1}")

    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_consultation())
